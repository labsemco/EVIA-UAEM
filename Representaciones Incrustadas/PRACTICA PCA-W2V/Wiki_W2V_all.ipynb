{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WKlKAi7muXLj"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jhermosillo/Escuela_CD_IMATE_2019/blob/master/Wiki_W2V_all.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DvvxyRi1uXLl"
      },
      "source": [
        "<h3><center>\n",
        "    \n",
        "### **Modelado de texto usando redes neuronales: algoritmo Word2Vec.**\n",
        "#### Aplicación en WikiPedia para medir semejanza entre documentos.\n",
        "    \n",
        "</center></h3>\n",
        "<h5><center>\n",
        "    Dr. Jorge Hermosillo Valadez<br>\n",
        "    Centro de Investigación en Ciencias<br>\n",
        "    Universidad Autónoma del Estado de Morelos<br>\n",
        "</center></h5>\n",
        "</center>\n",
        "<img src=\"https://github.com/labsemco/EVIA-UAEM/blob/main/Representaciones%20Incrustadas/PRACTICA%20PCA-W2V/img/logoCInC.jpg?raw=1\" width=\"100\"/>\n",
        "<img src=\"https://github.com/labsemco/EVIA-UAEM/blob/main/Representaciones%20Incrustadas/PRACTICA%20PCA-W2V/img/uaem.jpg?raw=1\" width=\"100\"/>\n",
        "</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPVd6aqLuXLm"
      },
      "source": [
        "En este curso veremos cómo:\n",
        "* Los principios básicos de W2V\n",
        "* Cómo construir una matriz de vectores de palabras usando W2V\n",
        "* Cómo modelar documentos\n",
        "* Calcular la semejanza entre dos documentos usando W2V y comparar contra PCA y LSI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxMEfXsYuXLm"
      },
      "source": [
        "# Módulos necesarios"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTTyuriIuXLm"
      },
      "source": [
        "## **Sólo para COLAB**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "XGG9oayvuXLp"
      },
      "outputs": [],
      "source": [
        "import wiki as wi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "dwqYPzDJuXLq",
        "outputId": "8ae5bd2e-9237-4a43-f0ff-9d0b897c8bab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import glob\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "VMhDbK89uXLq",
        "outputId": "901cc646-48fe-4130-db89-f14ad9b0a9e8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n"
          ]
        }
      ],
      "source": [
        "archivos = glob.glob('/content/datos/textosWiki_1')\n",
        "print(archivos)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0eBtvoUmuXLq"
      },
      "source": [
        "Leemos los archivos descargados y sus nombres."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "aBMzluiguXLq",
        "outputId": "f60b9805-2d84-4982-86b0-8804db522c5a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "leyendo...\n",
            "tamaño del contenido de archivos cargados:             0 MB\n"
          ]
        }
      ],
      "source": [
        "file,nombres = wi.carga_datos(archivos)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LiHp_o1PuXLq"
      },
      "source": [
        "# **Extracción de documentos**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "rBqf4--YuXLq",
        "outputId": "d7c17fcd-7f49-4bf1-daeb-69ce0ca92c0b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Se leyeron 0 archivos\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-c09a36046391>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlee_documentos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnombres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Se leyeron {} archivos'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ],
      "source": [
        "docs = wi.lee_documentos(file,nombres)\n",
        "print('Se leyeron {} archivos'.format(len(docs)))\n",
        "print(docs[0][0][0],docs[0][0][1][:100])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1s_pJLFuXLr"
      },
      "source": [
        "# **Data Frame de documentos**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hNvsi84yuXLr"
      },
      "outputs": [],
      "source": [
        "# df,documentos=wi.get_dataFrame_WiDocs(docs)\n",
        "# print(df.shape)\n",
        "# df.head()\n",
        "\n",
        "df_0 = pd.DataFrame(docs[0],columns = ['doc_id','Texto','clase'])\n",
        "#df_1 = pd.DataFrame(docs[1],columns = ['doc_id','Texto','clase'])\n",
        "print(len(df_0.index),'documentos clase 0')\n",
        "#print(len(df_1.index),'documentos clase 1')\n",
        "df = df_0\n",
        "#df = pd.concat([df_0, df_1], ignore_index=True, sort=False)\n",
        "print(df.shape)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fpKjhSruXLr"
      },
      "source": [
        "# **Modelo Word2Vec**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stWJAbnquXLr"
      },
      "source": [
        "**Introducción** <br>\n",
        "\n",
        "El modelo Word2Vec (Mikolov et al., 2013) es un algoritmo de representación latente (o embebida) de palabras, que se calcula utilizando una red neuronal.\n",
        "\n",
        "Su origen epistémico está en los modelos estadísticos del lenguaje.\n",
        "\n",
        "$P(w_1,w_2,\\cdots,w_n)=P(w_1)P(w_2|w_1)P(w_3|w_1^2)\\cdots P(w_n|w_1^{n-1})$.\n",
        "\n",
        "Estos modelos, buscan calcular la probabilidad de _n-gramas_: $P(w_1)$ unigramas, $P(w_2|w_1)$ bigramas, $P(w_3|w_1^2)$ trigramas, etc.\n",
        "\n",
        "Los unigramas, son modelos tipo Bolsa-de-Palabras, ya que todas las palabras se consideran _independientes_; los bigramas son modelos donde se busca la probabilidad de una palabra, dado un _contexto_ de una palabra; en los trigramas el contexto es de dos palabras, y así sucesivamente.\n",
        "\n",
        "El uso pionero de redes neuronales para calcular estas probabilidades se debe a Bengio y colegas (Bengio et al., 2003). La hipótesis es que _**los términos que co-ocurren en contextos similares tendrán representaciones similares**_, ya que la red neuronal busca maximizar el valor de probabilidad de co-ocurrencia y ajusta los pesos (representación embebida) de la red para este fin.\n",
        "\n",
        "Sin embargo, la red de Bengio era profunda y muy ineficiente. La aportación de Mikolov y colegas fue optimizar la arquitectura, haciéndola superficial y utilizando trucos de aceleración del cómputo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11J1zaSHuXLr"
      },
      "source": [
        "**Modelos en word2vec** <br>\n",
        "\n",
        "Word2vec implementa dos tipos de modelos: CBOW (Continuous Bag-of-Words: predicción de una palabra dado un contexto de n palabras) y SKIP-gram (predicción de un contexto de n palabras, dada una palabra).\n",
        "\n",
        "</center>\n",
        "<img src=\"https://github.com/labsemco/EVIA-UAEM/blob/main/Representaciones%20Incrustadas/PRACTICA%20PCA-W2V/img/CBOW.png?raw=1\" width=\"300\"/>\n",
        "</center><em><center>Modelo CBOW de un bigrama</em></center>\n",
        "\n",
        "</center>\n",
        "<img src=\"https://github.com/labsemco/EVIA-UAEM/blob/main/Representaciones%20Incrustadas/PRACTICA%20PCA-W2V/img/SKIP-gram.png?raw=1\" width=\"300\"/>\n",
        "</center><em><center>Modelo SKIP-gram para un contexto de 3 palabras</em></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GlwgZq26uXLr"
      },
      "source": [
        "**Paso hacia adelante (forward propagation) en word2vec** <br>\n",
        "\n",
        "Las palabras del vocabulario se modelan como un vector _one-hot_, donde sola hay un $1$ en la unidad correspondiente a la palabra de entrada.\n",
        "\n",
        "Si el vocabulario es de tamaño $V$, y si la capa oculta ($\\mathbf{h}$) tiene $N$ neuronas, entonces la matriz de pesos que une la entrada a $\\mathbf{h}$, $\\mathbf{W}$, es de tamaño $V\\times N$.\n",
        "\n",
        "Cada fila de $\\mathbf{W}$ es la representación vectorial $\\mathbf{v}_w$ de dimensión $N$ de la palabra $w$. Formalmente, la fila $i$ de $\\mathbf{W}$ es $\\mathbf{v}^{^\\textrm{T}}_w$.\n",
        "\n",
        "Dado un contexto (de una palabra para el modelo CBOW del bigrama), suponiendo $x_k=1$ y $x_k'=0$ para $k'\\neq k$ tenemos:\n",
        "<center>\n",
        "$\\mathbf{h}=\\mathbf{W}^{^\\textrm{T}}\\mathbf{x}=\\mathbf{W}^{^\\textrm{T}}_{(k,\\cdot)}:=\\mathbf{v}^{^\\textrm{T}}_{w_I}$\n",
        "</center>\n",
        "\n",
        "Hacia la salida, hay otra matriz $\\mathbf{W}'$ de tamaño $N\\times V$, por lo que una unidad de salida $j$ (palabra del vocabulario) tendrá un puntaje (score):\n",
        "<center>\n",
        "$u_j=\\mathbf{v}_{w_j}'^{^\\textrm{T}}\\mathbf{h}$\n",
        "</center>\n",
        "donde $\\mathbf{v}_{w_j}'$ es la columna $j$ de la matriz $\\mathbf{W}'$.\n",
        "\n",
        "Para obtener la distribución a posteriori de las palabras del vocabulario, que es una distribución multinomial, podemos usar un modelo de clasificación multiclase log-lineal llamado _softmax_\n",
        "<center>\n",
        "$p(w_j|w_I)=y_j=\\frac{\\exp(u_j)}{\\sum_{j'=1}^{V}\\exp(u_{j'})}$\n",
        "</center>\n",
        "\n",
        "El objetivo es entonces optimizar la expresión anterior mediante el algoritmo de descenso de gradiente, utilizando una función de costo (loss function) de entropía cruzada."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0pRmg4VuXLs"
      },
      "source": [
        "**Entropía cruzada y cálculo de pesos en word2vec** <br>\n",
        "\n",
        "**Entropía:**<br>\n",
        "\n",
        "Recordemos que podemos medir la cantidad de información de un evento estocástico dada su probabilidad:\n",
        "<center>\n",
        "$I(E)=-\\log[Pr(E)]=−\\log(p)$\n",
        "</center>\n",
        "\n",
        "La entropía es el valor esperado o promedio de la información de un conjunto de eventos estocásticos.\n",
        "\n",
        "El valor esperado de una variable aleatoria se escribe:\n",
        "<center>\n",
        "$E[X]=\\sum_i^nx_ip_i$\n",
        "</center>\n",
        "Por lo que la entropía es:\n",
        "<center>\n",
        "$E[I(X)]=E[-\\log[Pr(I(X))]]=-\\sum_i^np(x_i)\\log p(x_i)$\n",
        "</center>\n",
        "\n",
        "\n",
        "**Entropía cruzada:**<br>\n",
        "\n",
        "Una forma de interpretar la entropía cruzada es verla como (menos) una función de verosimilitud log (log-likelyhood) para datos $y_i'$, bajo un modelo $y_i$.\n",
        "\n",
        "Es decir, supongamos que tenemos algún modelo fijo (también conocido como \"hipótesis\"), que predice para $n$ clases $\\{1,2, \\cdots,n\\}$ sus probabilidades de ocurrencia hipotéticas $y_1, y_2, \\cdots, y_n$. Supongamos que ahora observamos (en realidad) $k_1$ instancias de la clase 1, $k_2$ instancias de la clase 2, $k_n$ instancias de la clase $n$, etc.\n",
        "\n",
        "Según el modelo, la probabilidad de que esto ocurra es (distribución multinomial):\n",
        "<center>\n",
        "$P[datos|modelo]:= y_1^{k_1}\\,y_2^{k_2}\\,\\cdots,y_n^{k_n}$.\n",
        "</center>\n",
        "\n",
        "Tomando el logaritmo y cambiando el signo:\n",
        "<center>\n",
        "$-\\log\\,P[datos|modelo]= -k_1\\log\\,y_1-k_2\\log\\,y_2\\,\\cdots,-k_n\\log\\,y_n=-\\sum_i k_i\\log\\,y_i$.\n",
        "</center>\n",
        "\n",
        "Si ahora dividimos por el número de observaciones $N=k_1+k_2+\\cdots+k_n$, y escribimos las probabilidades empíricas $y_i'=k_i/N$, tenemos la _entropía cruzada_:\n",
        "<center>\n",
        "$-\\frac{1}{N}\\log\\,P[datos|modelo]= -\\frac{1}{N}\\sum_i k_i\\log\\,y_i = \\sum_i y_i'\\log\\,y_i $.\n",
        "</center>\n",
        "\n",
        "En el caso de las redes neuronales, $y_i'$ corresponde con el valor _verdadero_ de la instancia ($\\{0,1\\}$) y $y_i$ es el valor que predice el modelo.\n",
        "\n",
        "\n",
        "</center>\n",
        "<img src=\"https://github.com/labsemco/EVIA-UAEM/blob/main/Representaciones%20Incrustadas/PRACTICA%20PCA-W2V/img/softmax.png?raw=1\" width=\"450\"/>\n",
        "</center><em><center>Resumen del modelo CBOW - Forward pass</em></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYjbVsoTuXLs"
      },
      "source": [
        "**Funciones de costo en word2vec**(Rong, 2014)<br>\n",
        "\n",
        "En word2vec queremos maximizar:\n",
        "<center>\n",
        "$p(w_O|w_I)=y_{j*}=\\frac{\\exp(u_{j*})}{\\sum_{j'=1}^{V}\\exp(u_{j'})}$\n",
        "</center>\n",
        "donde $j*$ es el índice de la palabra que debe estar a la salida. Este es el índice que se compara contra la salida de la red cuando se entrena.\n",
        "\n",
        "**CBOW:**<br>\n",
        "<center>\n",
        "$E=-\\log p(w_O|w_I)=-u_{j*}+\\log\\sum_{j'=1}^{V}\\exp(u_{j'})$\n",
        "</center>\n",
        "<center>\n",
        "$E=-\\mathbf{v}_{w_O}'^{^\\textrm{T}}\\cdot\\mathbf{h}+\\log\\sum_{j'=1}^{V}\\exp(\\mathbf{v}_{w_j}'^{^\\textrm{T}}\\cdot\\mathbf{h})$\n",
        "</center>\n",
        "\n",
        "\n",
        "En el caso en que haya $C$ palabras de entrada (e.g. trigramas o más) la expresión de arriba es la misma, solo cambia $\\mathbf{h}$ que en este caso es:\n",
        "<center>\n",
        "$\\mathbf{h}=\\frac{1}{C}\\mathbf{W}^{^\\textrm{T}}(\\mathbf{x}_1+\\mathbf{x}_2+\\cdots+\\mathbf{x}_C)$\n",
        "</center>\n",
        "\n",
        "**SKIP-gram**<br>\n",
        "Para este caso, en lugar de tener una sola distribución multinomial, tenemos $C$ distribuciones, donde $C$ es el número de palabras del contexto.\n",
        "<center>\n",
        "$E=-\\log p(w_{O,1},w_{O,2},\\cdots,w_{O,C}|w_I)=-\\log\\,\\prod_{c=1}^C\\frac{\\exp(u_{c,j*})}{\\sum_{j'=1}^{V}\\exp(u_{j'})}=\n",
        "-\\sum_{c=1}^{C}u_{j*_c}+C\\cdot\\log\\sum_{j'=1}^{V}\\exp(u_{j'})$\n",
        "</center>\n",
        "\n",
        "\n",
        "De esta forma, se puede aplicar el algoritmo de Back-Propagation, donde se calcula el gradiente de las funciones de costo con respecto a las entradas, según el caso y según la capa correspondiente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "325gebwKuXLs"
      },
      "source": [
        "## **Gensim**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTbZ-e-buXLs"
      },
      "source": [
        "Vamos a utilizar las librerías y módulos de [gensim](https://radimrehurek.com/gensim/) para [word2vec](https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec), que tienen una amplia gama de soluciones en Python para procesamiento de la [Wikipedia](https://radimrehurek.com/gensim/scripts/segment_wiki.html), y el Procesamiento de Lenguaje Natural en general."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TkEoNkECuXLs"
      },
      "outputs": [],
      "source": [
        "types=df['Texto'].str.split(' ',expand=True).stack().unique()\n",
        "Textos=df.Texto.values\n",
        "\n",
        "#Creamos las oraciones, este será la entrada del modelo W2V\n",
        "frases = [s.split() for s in Textos]\n",
        "\n",
        "documentos= []\n",
        "\n",
        "#Concatenamos todas las oraciones en una sola lista\n",
        "for f in frases:\n",
        "    documentos.append(f)\n",
        "\n",
        "print('Hay {} documentos y {} palabras únicas'.\\\n",
        "      format(len(documentos),len(types)))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "types"
      ],
      "metadata": {
        "id": "vYv8Gjby3AyL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "863t10wXuXLs"
      },
      "outputs": [],
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "vec_dim= 300\n",
        "W2V = Word2Vec(documentos, min_count=1, workers=4, window=5,vector_size=vec_dim)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "W2V.wv.most_similar(positive=['king'])"
      ],
      "metadata": {
        "id": "rk6_ibZi1UZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'woman' in W2V.wv"
      ],
      "metadata": {
        "id": "YJdFifFS28qm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6hfrMSgruXLs"
      },
      "outputs": [],
      "source": [
        "print(W2V.wv.get_vector(\"comenzó\").shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(W2V.wv.get_vector(\"comenzó\"))"
      ],
      "metadata": {
        "id": "BP7tkDOI3JM7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "W2V.wv.similarity('comenzó','king')"
      ],
      "metadata": {
        "id": "X87UIgEb3UvG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "W2V.wv.n_similarity('cell','blood')"
      ],
      "metadata": {
        "id": "tHhOnpt13Q52"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "W2V.wv.similarity('france', 'spain')"
      ],
      "metadata": {
        "id": "sTJbjtI1070O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JX2PK6lkuXLt"
      },
      "source": [
        "Podemos guardar el modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HPxQG4A5uXLt"
      },
      "outputs": [],
      "source": [
        "W2V.save('/content/datos/word2vec.model')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_-5ipaAuXLt"
      },
      "source": [
        "o leer el modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SnaAWubcuXLt"
      },
      "outputs": [],
      "source": [
        "W2V= Word2Vec.load('datos/word2vec.model')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OzaeGwsMuXLt"
      },
      "outputs": [],
      "source": [
        "print(W2V.wv.get_vector(\"comenzó\")[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLmmFGEquXLt"
      },
      "source": [
        "### Modelación de documentos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v_cMEKakuXLt"
      },
      "outputs": [],
      "source": [
        "from collections import Counter, OrderedDict\n",
        "\n",
        "def modela_documentos_rep(df):\n",
        "    id_=df.doc_id.values\n",
        "    datos=df.drop(columns=['doc_id'])\n",
        "    datos=datos.values\n",
        "    dx=[]\n",
        "    for i,doc_id in enumerate(id_):\n",
        "        dx.append((doc_id,datos[i]))\n",
        "    do=pd.DataFrame(dx,columns=['doc_id','Vectores'])\n",
        "    return do\n",
        "\n",
        "def modela_documentos(df,w2v):\n",
        "    docs=df.doc_id.values\n",
        "    textos=df.Texto.str.split(' ').values.tolist()\n",
        "    d=[]\n",
        "    Dx=[]\n",
        "    for i,texto in enumerate(textos):\n",
        "        for w in texto:\n",
        "            e=w2v.wv.get_vector(w)\n",
        "            d.append(e)\n",
        "        d=np.array(d)\n",
        "        dx=np.sum(d,axis=0)/len(d)\n",
        "        Dx.append([docs[i],dx])\n",
        "        d=[]\n",
        "    do=pd.DataFrame(Dx,columns=['doc_id','W2V'])\n",
        "    return do\n",
        "\n",
        "def k_vecinos_mas_cercanos(docus,df,k=1):\n",
        "    l=docus.doc_id.values\n",
        "    vec=OrderedDict()\n",
        "    for id_ in l:\n",
        "        d=dist_vecinos(id_,df)\n",
        "        for i in range(k):\n",
        "            if i==0:\n",
        "                vec[id_]=[[d[i][1],d[i][2]]]\n",
        "            else:\n",
        "                vec[id_].append([d[i][1],d[i][2]])\n",
        "    return vec\n",
        "\n",
        "def vecinos_mas_cercanos(df,distancias):\n",
        "    l=df.doc_id.values\n",
        "    vec=OrderedDict()\n",
        "    for id_ in l:\n",
        "        for i,d in enumerate(distancias):\n",
        "            if id_ == d[0]:\n",
        "                vecino=d[1]\n",
        "                if id_ not in vec.keys():\n",
        "                    vec[id_]=[(vecino,d[2])]\n",
        "                else:\n",
        "                    vec[id_].append((vecino,d[2]))\n",
        "            elif id_== d[1]:\n",
        "                vecino=d[0]\n",
        "                if id_ not in vec.keys():\n",
        "                    vec[id_]=[(vecino,d[2])]\n",
        "                else:\n",
        "                    vec[id_].append((vecino,d[2]))\n",
        "    return vec\n",
        "\n",
        "def dist_vecinos(id_docu,df):\n",
        "    dist=[]\n",
        "    candidato = df[df['doc_id']==id_docu]\n",
        "    candidato = candidato.iloc[:,1].values[0]\n",
        "    fila=df.index[df['doc_id'] == id_docu].tolist()\n",
        "    pts=df.drop(df.index[fila])\n",
        "    id_=pts.doc_id.values\n",
        "    pts=pts.iloc[:,1].values\n",
        "\n",
        "    for i in range(len(pts)):\n",
        "        d = np.sqrt(np.sum(np.square(candidato-pts[i])))\n",
        "        dist.append((id_docu,id_[i],d))\n",
        "    dist=sorted(dist,key=lambda x: x[2])\n",
        "    return dist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FVOuwXzcuXLt"
      },
      "outputs": [],
      "source": [
        "edf=modela_documentos(df,W2V)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "edf"
      ],
      "metadata": {
        "id": "mRI7jGlQ02t3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEIUk7UquXLu"
      },
      "source": [
        "## **Ejercicio 1**\n",
        "\n",
        "Queremos saber que tan bien podemos modelar documentos utilizando Word2Vec.\n",
        "\n",
        "Compara estos resultados con los obtenidos por les métodos PCA y LSA."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Flgb3YpzuXLx"
      },
      "source": [
        "### Documentos de análisis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kD6fAPfquXLx"
      },
      "outputs": [],
      "source": [
        "# docus = df[(df['doc_id']=='1023628') |\\\n",
        "#            (df['doc_id']=='1024447') |\\\n",
        "#            (df['doc_id']=='1035967') |\\\n",
        "#            (df['doc_id']=='1891029') |\\\n",
        "#            (df['doc_id']=='1894599') ]\n",
        "docus = df.sample(n=5)\n",
        "docus.index=range(len(docus.index))\n",
        "\n",
        "docus.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "omwKt3bmuXLx"
      },
      "outputs": [],
      "source": [
        "k=1\n",
        "vecinos=k_vecinos_mas_cercanos(docus,edf,k)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZQgk4OvIuXLx"
      },
      "outputs": [],
      "source": [
        "print(vecinos)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z8J00q0JuXLx"
      },
      "outputs": [],
      "source": [
        "lista = []\n",
        "for item in vecinos:\n",
        "    vecino = vecinos[item][0][0]\n",
        "    lista.append((item,vecino))\n",
        "for docs in lista:\n",
        "    print(df.loc[df['doc_id']==docs[0]].Texto.values)\n",
        "    print('  ',df.loc[df['doc_id']==docs[1]].Texto.values)\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GuGb4aYuXLx"
      },
      "source": [
        "## **Ejercicio 2**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVszWpZFuXLy"
      },
      "source": [
        "Modelo SKIP-gram... Ojo! Es tardado..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QPEHmF_luXLy"
      },
      "outputs": [],
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "vec_dim= 300\n",
        "w2v_sg = Word2Vec(documentos, min_count=1, vector_size=vec_dim, workers=4, window=5,sg=1)\n",
        "#print(W2V[\"comenzó\"][:10])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w2v_sg.save('/content/datos/word2vec_sg.model')"
      ],
      "metadata": {
        "id": "09lQgz1BzZ5N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i6PUFYrKuXLy"
      },
      "outputs": [],
      "source": [
        "w2v_sg=Word2Vec.load('/content/datos/word2vec_sg.model')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w2v_sg"
      ],
      "metadata": {
        "id": "zsYonc1b5EyW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FHcTr3jSuXLy"
      },
      "outputs": [],
      "source": [
        "edf_sg=wi.modela_documentos(df,w2v_sg.wv)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ah2PjwwIuXLy"
      },
      "outputs": [],
      "source": [
        "k=1\n",
        "vecinos_sg=wi.k_vecinos_mas_cercanos(docus,edf_sg,k)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vecinos_sg"
      ],
      "metadata": {
        "id": "4I6Q-soc5XiK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vLRxGG_kuXLy"
      },
      "outputs": [],
      "source": [
        "print(vecinos_sg['1879679'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CmGmyLOGuXLy"
      },
      "outputs": [],
      "source": [
        "test1=df[df['doc_id']=='1891029'].values\n",
        "test2=df[df['doc_id']=='1896707'].values\n",
        "print(test1)\n",
        "print(test2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RIZQZ7kMuXLz"
      },
      "source": [
        "<hr>\n",
        "</hr>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VGU_5LeauXLz"
      },
      "outputs": [],
      "source": [
        "df_lsa=pd.read_pickle('/content/datos/data_frame_4K.pkl')\n",
        "df_lsa.index = range(len(df_lsa.index))\n",
        "print(df_lsa.shape)\n",
        "df_lsa.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dG_d4EfruXLz"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from collections import Counter, OrderedDict\n",
        "\n",
        "def bow_(docs):\n",
        "    v = DictVectorizer(sparse=False)\n",
        "    X = v.fit_transform(docs)\n",
        "    return X,v\n",
        "\n",
        "docs = df_lsa.Conteos.tolist()\n",
        "X,vocab_ = bow_(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ffv5D6C9uXLz"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "q=300  #Elegimos usar q componentes\n",
        "svd = TruncatedSVD(n_components=q, n_iter=7, random_state=42)\n",
        "\n",
        "vectorizer = TfidfVectorizer(vocabulary=vocab_.vocabulary_)\n",
        "\n",
        "corpus = df_lsa.Texto.tolist()\n",
        "D_tfidf = vectorizer.fit_transform(corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8mg1471wuXLz"
      },
      "outputs": [],
      "source": [
        "dlsa=svd.fit_transform(D_tfidf)\n",
        "dlsa=wi.get_dataFrame(dlsa,df_lsa)\n",
        "print(dlsa.shape)\n",
        "dlsa.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pTLMgC30uXLz"
      },
      "outputs": [],
      "source": [
        "svd_vr=svd.explained_variance_ratio_\n",
        "wi.distribucion_vr(svd_vr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zW8M2VPOuXLz"
      },
      "outputs": [],
      "source": [
        "q=300  #debe ser <= 300 o debes correr de nuevo el algoritmo más arriba\n",
        "lsa_rep=wi.get_representativos(dlsa,q)\n",
        "print(lsa_rep.shape)\n",
        "lsa_rep.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PQHrQzgduXL0"
      },
      "outputs": [],
      "source": [
        "edf_lsa=wi.modela_documentos_rep(lsa_rep)\n",
        "print(edf_lsa.shape)\n",
        "edf_lsa.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docus"
      ],
      "metadata": {
        "id": "Fb9uNnb08MOu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'1876977'\tin edf_lsa[\"doc_id\"].to_list()"
      ],
      "metadata": {
        "id": "lF4O3xqH9Qqv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docus=docus[docus['doc_id'].isin(edf_lsa[\"doc_id\"].to_list())]\n",
        "docus"
      ],
      "metadata": {
        "id": "vcg73sAgORYW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UlanJ7cGuXL0"
      },
      "outputs": [],
      "source": [
        "k=1\n",
        "vecinos_lsa=wi.k_vecinos_mas_cercanos(docus,edf_lsa,k)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TqcF0oXGuXL0"
      },
      "outputs": [],
      "source": [
        "print(vecinos_lsa['1896519'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6FpsxW07uXL0"
      },
      "outputs": [],
      "source": [
        "test1=df[df['doc_id']=='1896519'].Texto.values[0][:400]\n",
        "test2=df[df['doc_id']=='1876977'].Texto.values[0][:400]\n",
        "print(test1)\n",
        "print(test2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tgxcDVHluXL1"
      },
      "outputs": [],
      "source": [
        "print(vecinos_lsa['1876977'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cb5lYjBXuXL2"
      },
      "source": [
        "**Referencias** <br>\n",
        "\n",
        "Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and Christian Janvin. 2003. A neural probabilistic language model. J. Mach. Learn. Res. 3 (March 2003), 1137-1155.\n",
        "\n",
        "Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Distributed representations of words and phrases and their compositionality. In _Proceedings of the 26th International Conference on Neural Information Processing Systems - Volume 2 (NIPS'13)_, C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger (Eds.), Vol. 2. Curran Associates Inc., USA, 3111-3119.\n",
        "\n",
        "Xin Rong. 2014. Word2vec Parameter Learning Explained.arXiv 1411.2738. disponible en linea {http://arxiv.org/abs/1411.2738}"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}