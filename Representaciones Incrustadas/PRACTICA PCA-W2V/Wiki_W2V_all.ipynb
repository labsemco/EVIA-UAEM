{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/jhermosillo/Escuela_CD_IMATE_2019/blob/master/Wiki_W2V_all.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><center>\n",
    "    \n",
    "### **Modelado de texto usando redes neuronales: algoritmo Word2Vec.**\n",
    "#### Aplicación en WikiPedia para medir semejanza entre documentos.\n",
    "    \n",
    "</center></h3>\n",
    "<h5><center>\n",
    "    Dr. Jorge Hermosillo Valadez<br>\n",
    "    Centro de Investigación en Ciencias<br>\n",
    "    Universidad Autónoma del Estado de Morelos<br>\n",
    "</center></h5>\n",
    "</center>\n",
    "<img src=\"img/logoCInC.jpg\" width=\"100\"/>\n",
    "<img src=\"img/uaem.jpg\" width=\"100\"/>\n",
    "</center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este curso veremos cómo:\n",
    "* Los principios básicos de W2V\n",
    "* Cómo construir una matriz de vectores de palabras usando W2V\n",
    "* Cómo modelar documentos\n",
    "* Calcular la semejanza entre dos documentos usando W2V y comparar contra PCA y LSI"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Módulos necesarios"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Sólo para COLAB**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\"apt-get\" no se reconoce como un comando interno o externo,\n",
      "programa o archivo por lotes ejecutable.\n",
      "\"svn\" no se reconoce como un comando interno o externo,\n",
      "programa o archivo por lotes ejecutable.\n",
      "\"svn\" no se reconoce como un comando interno o externo,\n",
      "programa o archivo por lotes ejecutable.\n"
     ]
    }
   ],
   "source": [
    "#\"\"\"\n",
    "!apt-get install subversion \n",
    "!svn checkout \"https://github.com/jhermosillo/Escuela_CD_IMATE_2019/trunk/datos/\"\n",
    "!svn checkout \"https://github.com/jhermosillo/Escuela_CD_IMATE_2019/trunk/modelos/\"\n",
    "#\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ApplicationDefaultCredentialsError",
     "evalue": "The Application Default Credentials are not available. They are available if running in Google Compute Engine. Otherwise, the environment variable GOOGLE_APPLICATION_CREDENTIALS must be defined pointing to a file defining the credentials. See https://developers.google.com/accounts/docs/application-default-credentials for more information.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mApplicationDefaultCredentialsError\u001b[0m        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[39m#auth.authenticate_user()\u001b[39;00m\n\u001b[0;32m      8\u001b[0m gauth \u001b[39m=\u001b[39m GoogleAuth()\n\u001b[1;32m----> 9\u001b[0m gauth\u001b[39m.\u001b[39mcredentials \u001b[39m=\u001b[39m GoogleCredentials\u001b[39m.\u001b[39;49mget_application_default()\n\u001b[0;32m     10\u001b[0m file_drive \u001b[39m=\u001b[39m GoogleDrive(gauth)\n\u001b[0;32m     11\u001b[0m \u001b[39m#\"\"\"\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\oauth2client\\client.py:1271\u001b[0m, in \u001b[0;36mGoogleCredentials.get_application_default\u001b[1;34m()\u001b[0m\n\u001b[0;32m   1263\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[0;32m   1264\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_application_default\u001b[39m():\n\u001b[0;32m   1265\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Get the Application Default Credentials for the current environment.\u001b[39;00m\n\u001b[0;32m   1266\u001b[0m \n\u001b[0;32m   1267\u001b[0m \u001b[39m    Raises:\u001b[39;00m\n\u001b[0;32m   1268\u001b[0m \u001b[39m        ApplicationDefaultCredentialsError: raised when the credentials\u001b[39;00m\n\u001b[0;32m   1269\u001b[0m \u001b[39m                                            fail to be retrieved.\u001b[39;00m\n\u001b[0;32m   1270\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1271\u001b[0m     \u001b[39mreturn\u001b[39;00m GoogleCredentials\u001b[39m.\u001b[39;49m_get_implicit_credentials()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\oauth2client\\client.py:1261\u001b[0m, in \u001b[0;36mGoogleCredentials._get_implicit_credentials\u001b[1;34m(cls)\u001b[0m\n\u001b[0;32m   1258\u001b[0m         \u001b[39mreturn\u001b[39;00m credentials\n\u001b[0;32m   1260\u001b[0m \u001b[39m# If no credentials, fail.\u001b[39;00m\n\u001b[1;32m-> 1261\u001b[0m \u001b[39mraise\u001b[39;00m ApplicationDefaultCredentialsError(ADC_HELP_MSG)\n",
      "\u001b[1;31mApplicationDefaultCredentialsError\u001b[0m: The Application Default Credentials are not available. They are available if running in Google Compute Engine. Otherwise, the environment variable GOOGLE_APPLICATION_CREDENTIALS must be defined pointing to a file defining the credentials. See https://developers.google.com/accounts/docs/application-default-credentials for more information."
     ]
    }
   ],
   "source": [
    "#\"\"\"\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "#from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials\n",
    "\n",
    "#auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "file_drive = GoogleDrive(gauth)\n",
    "#\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La siguiente instrucción requiere el vínculo al archivo desde DRIVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'file_drive' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m wi \u001b[39m=\u001b[39m file_drive\u001b[39m.\u001b[39mCreateFile({\u001b[39m'\u001b[39m\u001b[39mid\u001b[39m\u001b[39m'\u001b[39m:\u001b[39m'\u001b[39m\u001b[39m1sV6vK0CLUXpH1VInmtBUBnVACMm5fPFB\u001b[39m\u001b[39m'\u001b[39m})\n",
      "\u001b[1;31mNameError\u001b[0m: name 'file_drive' is not defined"
     ]
    }
   ],
   "source": [
    "wi = file_drive.CreateFile({'id':'1sV6vK0CLUXpH1VInmtBUBnVACMm5fPFB'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wi' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m wi\u001b[39m.\u001b[39mGetContentFile(\u001b[39m'\u001b[39m\u001b[39mwiki.py\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'wi' is not defined"
     ]
    }
   ],
   "source": [
    "wi.GetContentFile('wiki.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\LABSEMCO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import wiki as wi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\LABSEMCO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./datos/textosWiki_1']\n"
     ]
    }
   ],
   "source": [
    "archivos = glob.glob('./datos/textosWiki_1')\n",
    "print(archivos)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leemos los archivos descargados y sus nombres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "leyendo...\n",
      "./datos/textosWiki_1\n",
      "tamaño del contenido de archivos cargados:             12 MB\n"
     ]
    }
   ],
   "source": [
    "file,nombres = wi.carga_datos(archivos)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Extracción de documentos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "missing ), unterminated subpattern at position 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m docs \u001b[39m=\u001b[39m wi\u001b[39m.\u001b[39;49mlee_documentos(file,nombres)\n\u001b[0;32m      2\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mSe leyeron \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m archivos\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mlen\u001b[39m(docs)))\n\u001b[0;32m      3\u001b[0m \u001b[39mprint\u001b[39m(docs[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m],docs[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m][\u001b[39m1\u001b[39m][:\u001b[39m100\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\LABSEMCO\\Documents\\github\\EVIA-UAEM\\Representaciones Incrustadas\\PRACTICA PCA-W2V\\wiki.py:74\u001b[0m, in \u001b[0;36mlee_documentos\u001b[1;34m(file, nombres)\u001b[0m\n\u001b[0;32m     67\u001b[0m x \u001b[39m=\u001b[39m archivo[inicio:fin]\n\u001b[0;32m     68\u001b[0m r\u001b[39m=\u001b[39mpd\u001b[39m.\u001b[39mDataFrame(x,columns\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mcadena\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m     69\u001b[0m r\u001b[39m=\u001b[39mr\u001b[39m.\u001b[39;49mcadena\u001b[39m.\u001b[39;49mstr\u001b[39m.\u001b[39;49mtranslate(\\\n\u001b[0;32m     70\u001b[0m         \u001b[39mstr\u001b[39;49m\u001b[39m.\u001b[39;49mmaketrans(\u001b[39m'\u001b[39;49m\u001b[39m'\u001b[39;49m,\u001b[39m'\u001b[39;49m\u001b[39m'\u001b[39;49m,string\u001b[39m.\u001b[39;49mdigits))\\\n\u001b[0;32m     71\u001b[0m         \u001b[39m.\u001b[39;49mstr\u001b[39m.\u001b[39;49mtranslate(\\\n\u001b[0;32m     72\u001b[0m        \u001b[39mstr\u001b[39;49m\u001b[39m.\u001b[39;49mmaketrans(\u001b[39m'\u001b[39;49m\u001b[39m'\u001b[39;49m,\u001b[39m'\u001b[39;49m\u001b[39m'\u001b[39;49m,string\u001b[39m.\u001b[39;49mpunctuation))\\\n\u001b[0;32m     73\u001b[0m \u001b[39m.\u001b[39;49mstr\u001b[39m.\u001b[39;49mreplace(\u001b[39m'\u001b[39;49m\u001b[39m«\u001b[39;49m\u001b[39m'\u001b[39;49m,\u001b[39m'\u001b[39;49m\u001b[39m'\u001b[39;49m,regex\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\\\n\u001b[1;32m---> 74\u001b[0m \u001b[39m.\u001b[39;49mstr\u001b[39m.\u001b[39;49mreplace(\u001b[39m'\u001b[39;49m\u001b[39m»\u001b[39;49m\u001b[39m'\u001b[39;49m,\u001b[39m'\u001b[39;49m\u001b[39m'\u001b[39;49m,regex\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\u001b[39m.\u001b[39;49mstr\u001b[39m.\u001b[39;49mreplace(\u001b[39m'\u001b[39;49m\u001b[39m(\u001b[39;49m\u001b[39m'\u001b[39;49m,\u001b[39m'\u001b[39;49m\u001b[39m'\u001b[39;49m,regex\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\\\n\u001b[0;32m     75\u001b[0m \u001b[39m.\u001b[39mstr\u001b[39m.\u001b[39mreplace(\u001b[39m'\u001b[39m\u001b[39m)\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m,regex\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\u001b[39m.\u001b[39mstr\u001b[39m.\u001b[39mstrip()\n\u001b[0;32m     76\u001b[0m palabras \u001b[39m=\u001b[39m r\u001b[39m.\u001b[39mtolist()\n\u001b[0;32m     77\u001b[0m \u001b[39m#elimino las stopwords\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\strings\\accessor.py:129\u001b[0m, in \u001b[0;36mforbid_nonstring_types.<locals>._forbid_nonstring_types.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    124\u001b[0m     msg \u001b[39m=\u001b[39m (\n\u001b[0;32m    125\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCannot use .str.\u001b[39m\u001b[39m{\u001b[39;00mfunc_name\u001b[39m}\u001b[39;00m\u001b[39m with values of \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    126\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39minferred dtype \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inferred_dtype\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    127\u001b[0m     )\n\u001b[0;32m    128\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(msg)\n\u001b[1;32m--> 129\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\strings\\accessor.py:1505\u001b[0m, in \u001b[0;36mStringMethods.replace\u001b[1;34m(self, pat, repl, n, case, flags, regex)\u001b[0m\n\u001b[0;32m   1502\u001b[0m \u001b[39mif\u001b[39;00m case \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1503\u001b[0m     case \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m-> 1505\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data\u001b[39m.\u001b[39;49marray\u001b[39m.\u001b[39;49m_str_replace(\n\u001b[0;32m   1506\u001b[0m     pat, repl, n\u001b[39m=\u001b[39;49mn, case\u001b[39m=\u001b[39;49mcase, flags\u001b[39m=\u001b[39;49mflags, regex\u001b[39m=\u001b[39;49mregex\n\u001b[0;32m   1507\u001b[0m )\n\u001b[0;32m   1508\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_wrap_result(result)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\strings\\object_array.py:170\u001b[0m, in \u001b[0;36mObjectStringArrayMixin._str_replace\u001b[1;34m(self, pat, repl, n, case, flags, regex)\u001b[0m\n\u001b[0;32m    168\u001b[0m     \u001b[39mif\u001b[39;00m regex \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[0;32m    169\u001b[0m         pat \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39mescape(pat)\n\u001b[1;32m--> 170\u001b[0m     pat \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39;49mcompile(pat, flags\u001b[39m=\u001b[39;49mflags)\n\u001b[0;32m    172\u001b[0m n \u001b[39m=\u001b[39m n \u001b[39mif\u001b[39;00m n \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39m0\u001b[39m\n\u001b[0;32m    173\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m x: pat\u001b[39m.\u001b[39msub(repl\u001b[39m=\u001b[39mrepl, string\u001b[39m=\u001b[39mx, count\u001b[39m=\u001b[39mn)\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.1264.0_x64__qbz5n2kfra8p0\\Lib\\re\\__init__.py:227\u001b[0m, in \u001b[0;36mcompile\u001b[1;34m(pattern, flags)\u001b[0m\n\u001b[0;32m    225\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompile\u001b[39m(pattern, flags\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m):\n\u001b[0;32m    226\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mCompile a regular expression pattern, returning a Pattern object.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 227\u001b[0m     \u001b[39mreturn\u001b[39;00m _compile(pattern, flags)\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.1264.0_x64__qbz5n2kfra8p0\\Lib\\re\\__init__.py:294\u001b[0m, in \u001b[0;36m_compile\u001b[1;34m(pattern, flags)\u001b[0m\n\u001b[0;32m    288\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mwarnings\u001b[39;00m\n\u001b[0;32m    289\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\u001b[39m\"\u001b[39m\u001b[39mThe re.TEMPLATE/re.T flag is deprecated \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    290\u001b[0m               \u001b[39m\"\u001b[39m\u001b[39mas it is an undocumented flag \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    291\u001b[0m               \u001b[39m\"\u001b[39m\u001b[39mwithout an obvious purpose. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    292\u001b[0m               \u001b[39m\"\u001b[39m\u001b[39mDon\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt use it.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    293\u001b[0m               \u001b[39mDeprecationWarning\u001b[39;00m)\n\u001b[1;32m--> 294\u001b[0m p \u001b[39m=\u001b[39m _compiler\u001b[39m.\u001b[39;49mcompile(pattern, flags)\n\u001b[0;32m    295\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (flags \u001b[39m&\u001b[39m DEBUG):\n\u001b[0;32m    296\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(_cache) \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m _MAXCACHE:\n\u001b[0;32m    297\u001b[0m         \u001b[39m# Drop the oldest item\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.1264.0_x64__qbz5n2kfra8p0\\Lib\\re\\_compiler.py:743\u001b[0m, in \u001b[0;36mcompile\u001b[1;34m(p, flags)\u001b[0m\n\u001b[0;32m    741\u001b[0m \u001b[39mif\u001b[39;00m isstring(p):\n\u001b[0;32m    742\u001b[0m     pattern \u001b[39m=\u001b[39m p\n\u001b[1;32m--> 743\u001b[0m     p \u001b[39m=\u001b[39m _parser\u001b[39m.\u001b[39;49mparse(p, flags)\n\u001b[0;32m    744\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    745\u001b[0m     pattern \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.1264.0_x64__qbz5n2kfra8p0\\Lib\\re\\_parser.py:980\u001b[0m, in \u001b[0;36mparse\u001b[1;34m(str, flags, state)\u001b[0m\n\u001b[0;32m    977\u001b[0m state\u001b[39m.\u001b[39mflags \u001b[39m=\u001b[39m flags\n\u001b[0;32m    978\u001b[0m state\u001b[39m.\u001b[39mstr \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m\n\u001b[1;32m--> 980\u001b[0m p \u001b[39m=\u001b[39m _parse_sub(source, state, flags \u001b[39m&\u001b[39;49m SRE_FLAG_VERBOSE, \u001b[39m0\u001b[39;49m)\n\u001b[0;32m    981\u001b[0m p\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mflags \u001b[39m=\u001b[39m fix_flags(\u001b[39mstr\u001b[39m, p\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mflags)\n\u001b[0;32m    983\u001b[0m \u001b[39mif\u001b[39;00m source\u001b[39m.\u001b[39mnext \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.1264.0_x64__qbz5n2kfra8p0\\Lib\\re\\_parser.py:455\u001b[0m, in \u001b[0;36m_parse_sub\u001b[1;34m(source, state, verbose, nested)\u001b[0m\n\u001b[0;32m    453\u001b[0m start \u001b[39m=\u001b[39m source\u001b[39m.\u001b[39mtell()\n\u001b[0;32m    454\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m--> 455\u001b[0m     itemsappend(_parse(source, state, verbose, nested \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m,\n\u001b[0;32m    456\u001b[0m                        \u001b[39mnot\u001b[39;49;00m nested \u001b[39mand\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m items))\n\u001b[0;32m    457\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m sourcematch(\u001b[39m\"\u001b[39m\u001b[39m|\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    458\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.1264.0_x64__qbz5n2kfra8p0\\Lib\\re\\_parser.py:865\u001b[0m, in \u001b[0;36m_parse\u001b[1;34m(source, state, verbose, nested, first)\u001b[0m\n\u001b[0;32m    863\u001b[0m p \u001b[39m=\u001b[39m _parse_sub(source, state, sub_verbose, nested \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[0;32m    864\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m source\u001b[39m.\u001b[39mmatch(\u001b[39m\"\u001b[39m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m--> 865\u001b[0m     \u001b[39mraise\u001b[39;00m source\u001b[39m.\u001b[39merror(\u001b[39m\"\u001b[39m\u001b[39mmissing ), unterminated subpattern\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    866\u001b[0m                        source\u001b[39m.\u001b[39mtell() \u001b[39m-\u001b[39m start)\n\u001b[0;32m    867\u001b[0m \u001b[39mif\u001b[39;00m group \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    868\u001b[0m     state\u001b[39m.\u001b[39mclosegroup(group, p)\n",
      "\u001b[1;31merror\u001b[0m: missing ), unterminated subpattern at position 0"
     ]
    }
   ],
   "source": [
    "docs = wi.lee_documentos(file,nombres)\n",
    "print('Se leyeron {} archivos'.format(len(docs)))\n",
    "print(docs[0][0][0],docs[0][0][1][:100])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data Frame de documentos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'docs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# df,documentos=wi.get_dataFrame_WiDocs(docs)\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m# print(df.shape)\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m# df.head()\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m df_0 \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(docs[\u001b[39m0\u001b[39m],columns \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mdoc_id\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mTexto\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mclase\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m      6\u001b[0m \u001b[39m#df_1 = pd.DataFrame(docs[1],columns = ['doc_id','Texto','clase'])\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mlen\u001b[39m(df_0\u001b[39m.\u001b[39mindex),\u001b[39m'\u001b[39m\u001b[39mdocumentos clase 0\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'docs' is not defined"
     ]
    }
   ],
   "source": [
    "# df,documentos=wi.get_dataFrame_WiDocs(docs)\n",
    "# print(df.shape)\n",
    "# df.head()\n",
    "\n",
    "df_0 = pd.DataFrame(docs[0],columns = ['doc_id','Texto','clase'])\n",
    "#df_1 = pd.DataFrame(docs[1],columns = ['doc_id','Texto','clase'])\n",
    "print(len(df_0.index),'documentos clase 0')\n",
    "#print(len(df_1.index),'documentos clase 1')\n",
    "df = df_0\n",
    "#df = pd.concat([df_0, df_1], ignore_index=True, sort=False)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Modelo Word2Vec**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Introducción** <br>\n",
    "\n",
    "El modelo Word2Vec (Mikolov et al., 2013) es un algoritmo de representación latente (o embebida) de palabras, que se calcula utilizando una red neuronal.\n",
    "\n",
    "Su origen epistémico está en los modelos estadísticos del lenguaje.\n",
    "\n",
    "$P(w_1,w_2,\\cdots,w_n)=P(w_1)P(w_2|w_1)P(w_3|w_1^2)\\cdots P(w_n|w_1^{n-1})$.\n",
    "\n",
    "Estos modelos, buscan calcular la probabilidad de _n-gramas_: $P(w_1)$ unigramas, $P(w_2|w_1)$ bigramas, $P(w_3|w_1^2)$ trigramas, etc.\n",
    "\n",
    "Los unigramas, son modelos tipo Bolsa-de-Palabras, ya que todas las palabras se consideran _independientes_; los bigramas son modelos donde se busca la probabilidad de una palabra, dado un _contexto_ de una palabra; en los trigramas el contexto es de dos palabras, y así sucesivamente.\n",
    "\n",
    "El uso pionero de redes neuronales para calcular estas probabilidades se debe a Bengio y colegas (Bengio et al., 2003). La hipótesis es que _**los términos que co-ocurren en contextos similares tendrán representaciones similares**_, ya que la red neuronal busca maximizar el valor de probabilidad de co-ocurrencia y ajusta los pesos (representación embebida) de la red para este fin. \n",
    "\n",
    "Sin embargo, la red de Bengio era profunda y muy ineficiente. La aportación de Mikolov y colegas fue optimizar la arquitectura, haciéndola superficial y utilizando trucos de aceleración del cómputo."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modelos en word2vec** <br>\n",
    "\n",
    "Word2vec implementa dos tipos de modelos: CBOW (Continuous Bag-of-Words: predicción de una palabra dado un contexto de n palabras) y SKIP-gram (predicción de un contexto de n palabras, dada una palabra).\n",
    "\n",
    "</center>\n",
    "<img src=\"img/CBOW.png\" width=\"300\"/>\n",
    "</center><em><center>Modelo CBOW de un bigrama</em></center>\n",
    "\n",
    "</center>\n",
    "<img src=\"img/SKIP-gram.png\" width=\"300\"/>\n",
    "</center><em><center>Modelo SKIP-gram para un contexto de 3 palabras</em></center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Paso hacia adelante (forward propagation) en word2vec** <br>\n",
    "\n",
    "Las palabras del vocabulario se modelan como un vector _one-hot_, donde sola hay un $1$ en la unidad correspondiente a la palabra de entrada.\n",
    "\n",
    "Si el vocabulario es de tamaño $V$, y si la capa oculta ($\\mathbf{h}$) tiene $N$ neuronas, entonces la matriz de pesos que une la entrada a $\\mathbf{h}$, $\\mathbf{W}$, es de tamaño $V\\times N$. \n",
    "\n",
    "Cada fila de $\\mathbf{W}$ es la representación vectorial $\\mathbf{v}_w$ de dimensión $N$ de la palabra $w$. Formalmente, la fila $i$ de $\\mathbf{W}$ es $\\mathbf{v}^{^\\textrm{T}}_w$.\n",
    "\n",
    "Dado un contexto (de una palabra para el modelo CBOW del bigrama), suponiendo $x_k=1$ y $x_k'=0$ para $k'\\neq k$ tenemos:\n",
    "<center>\n",
    "$\\mathbf{h}=\\mathbf{W}^{^\\textrm{T}}\\mathbf{x}=\\mathbf{W}^{^\\textrm{T}}_{(k,\\cdot)}:=\\mathbf{v}^{^\\textrm{T}}_{w_I}$\n",
    "</center>\n",
    "\n",
    "Hacia la salida, hay otra matriz $\\mathbf{W}'$ de tamaño $N\\times V$, por lo que una unidad de salida $j$ (palabra del vocabulario) tendrá un puntaje (score):\n",
    "<center>\n",
    "$u_j=\\mathbf{v}_{w_j}'^{^\\textrm{T}}\\mathbf{h}$\n",
    "</center>\n",
    "donde $\\mathbf{v}_{w_j}'$ es la columna $j$ de la matriz $\\mathbf{W}'$.\n",
    "\n",
    "Para obtener la distribución a posteriori de las palabras del vocabulario, que es una distribución multinomial, podemos usar un modelo de clasificación multiclase log-lineal llamado _softmax_ \n",
    "<center>\n",
    "$p(w_j|w_I)=y_j=\\frac{\\exp(u_j)}{\\sum_{j'=1}^{V}\\exp(u_{j'})}$\n",
    "</center>\n",
    "\n",
    "El objetivo es entonces optimizar la expresión anterior mediante el algoritmo de descenso de gradiente, utilizando una función de costo (loss function) de entropía cruzada. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Entropía cruzada y cálculo de pesos en word2vec** <br>\n",
    "\n",
    "**Entropía:**<br>\n",
    "\n",
    "Recordemos que podemos medir la cantidad de información de un evento estocástico dada su probabilidad:\n",
    "<center>\n",
    "$I(E)=-\\log[Pr(E)]=−\\log(p)$\n",
    "</center>\n",
    "\n",
    "La entropía es el valor esperado o promedio de la información de un conjunto de eventos estocásticos.\n",
    "\n",
    "El valor esperado de una variable aleatoria se escribe:\n",
    "<center>\n",
    "$E[X]=\\sum_i^nx_ip_i$\n",
    "</center>\n",
    "Por lo que la entropía es:\n",
    "<center>\n",
    "$E[I(X)]=E[-\\log[Pr(I(X))]]=-\\sum_i^np(x_i)\\log p(x_i)$\n",
    "</center>\n",
    "\n",
    "\n",
    "**Entropía cruzada:**<br>\n",
    "\n",
    "Una forma de interpretar la entropía cruzada es verla como (menos) una función de verosimilitud log (log-likelyhood) para datos $y_i'$, bajo un modelo $y_i$.\n",
    "\n",
    "Es decir, supongamos que tenemos algún modelo fijo (también conocido como \"hipótesis\"), que predice para $n$ clases $\\{1,2, \\cdots,n\\}$ sus probabilidades de ocurrencia hipotéticas $y_1, y_2, \\cdots, y_n$. Supongamos que ahora observamos (en realidad) $k_1$ instancias de la clase 1, $k_2$ instancias de la clase 2, $k_n$ instancias de la clase $n$, etc. \n",
    "\n",
    "Según el modelo, la probabilidad de que esto ocurra es (distribución multinomial):\n",
    "<center>\n",
    "$P[datos|modelo]:= y_1^{k_1}\\,y_2^{k_2}\\,\\cdots,y_n^{k_n}$.\n",
    "</center>\n",
    "\n",
    "Tomando el logaritmo y cambiando el signo:\n",
    "<center>\n",
    "$-\\log\\,P[datos|modelo]= -k_1\\log\\,y_1-k_2\\log\\,y_2\\,\\cdots,-k_n\\log\\,y_n=-\\sum_i k_i\\log\\,y_i$.\n",
    "</center>\n",
    "\n",
    "Si ahora dividimos por el número de observaciones $N=k_1+k_2+\\cdots+k_n$, y escribimos las probabilidades empíricas $y_i'=k_i/N$, tenemos la _entropía cruzada_:\n",
    "<center>\n",
    "$-\\frac{1}{N}\\log\\,P[datos|modelo]= -\\frac{1}{N}\\sum_i k_i\\log\\,y_i = \\sum_i y_i'\\log\\,y_i $.\n",
    "</center>\n",
    "\n",
    "En el caso de las redes neuronales, $y_i'$ corresponde con el valor _verdadero_ de la instancia ($\\{0,1\\}$) y $y_i$ es el valor que predice el modelo. \n",
    "\n",
    "\n",
    "</center>\n",
    "<img src=\"img/softmax.png\" width=\"450\"/>\n",
    "</center><em><center>Resumen del modelo CBOW - Forward pass</em></center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Funciones de costo en word2vec**(Rong, 2014)<br>\n",
    "\n",
    "En word2vec queremos maximizar:\n",
    "<center>\n",
    "$p(w_O|w_I)=y_{j*}=\\frac{\\exp(u_{j*})}{\\sum_{j'=1}^{V}\\exp(u_{j'})}$\n",
    "</center>\n",
    "donde $j*$ es el índice de la palabra que debe estar a la salida. Este es el índice que se compara contra la salida de la red cuando se entrena.\n",
    "\n",
    "**CBOW:**<br>\n",
    "<center>\n",
    "$E=-\\log p(w_O|w_I)=-u_{j*}+\\log\\sum_{j'=1}^{V}\\exp(u_{j'})$\n",
    "</center>\n",
    "<center>\n",
    "$E=-\\mathbf{v}_{w_O}'^{^\\textrm{T}}\\cdot\\mathbf{h}+\\log\\sum_{j'=1}^{V}\\exp(\\mathbf{v}_{w_j}'^{^\\textrm{T}}\\cdot\\mathbf{h})$\n",
    "</center>\n",
    "\n",
    "\n",
    "En el caso en que haya $C$ palabras de entrada (e.g. trigramas o más) la expresión de arriba es la misma, solo cambia $\\mathbf{h}$ que en este caso es:\n",
    "<center>\n",
    "$\\mathbf{h}=\\frac{1}{C}\\mathbf{W}^{^\\textrm{T}}(\\mathbf{x}_1+\\mathbf{x}_2+\\cdots+\\mathbf{x}_C)$\n",
    "</center>\n",
    "\n",
    "**SKIP-gram**<br>\n",
    "Para este caso, en lugar de tener una sola distribución multinomial, tenemos $C$ distribuciones, donde $C$ es el número de palabras del contexto.\n",
    "<center>\n",
    "$E=-\\log p(w_{O,1},w_{O,2},\\cdots,w_{O,C}|w_I)=-\\log\\,\\prod_{c=1}^C\\frac{\\exp(u_{c,j*})}{\\sum_{j'=1}^{V}\\exp(u_{j'})}=\n",
    "-\\sum_{c=1}^{C}u_{j*_c}+C\\cdot\\log\\sum_{j'=1}^{V}\\exp(u_{j'})$\n",
    "</center>\n",
    "\n",
    "\n",
    "De esta forma, se puede aplicar el algoritmo de Back-Propagation, donde se calcula el gradiente de las funciones de costo con respecto a las entradas, según el caso y según la capa correspondiente."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Gensim**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a utilizar las librerías y módulos de [gensim](https://radimrehurek.com/gensim/) para [word2vec](https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec), que tienen una amplia gama de soluciones en Python para procesamiento de la [Wikipedia](https://radimrehurek.com/gensim/scripts/segment_wiki.html), y el Procesamiento de Lenguaje Natural en general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hay 4753 documentos y 103132 palabras únicas\n"
     ]
    }
   ],
   "source": [
    "types=df['Texto'].str.split(' ',expand=True).stack().unique()\n",
    "Textos=df.Texto.values\n",
    "\n",
    "#Creamos las oraciones, este será la entrada del modelo W2V\n",
    "frases = [s.split() for s in Textos]\n",
    "\n",
    "documentos= []\n",
    "\n",
    "#Concatenamos todas las oraciones en una sola lista\n",
    "for f in frases:\n",
    "    documentos.append(f)\n",
    "\n",
    "print('Hay {} documentos y {} palabras únicas'.\\\n",
    "      format(len(documentos),len(types)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "vec_dim= 300\n",
    "W2V = Word2Vec(documentos, min_count=1, workers=4, window=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.95365494  1.1842033   0.95986784 -0.20210184  0.06581045 -1.5050063\n",
      "  0.3009835   3.0103698  -0.39941457 -0.67336667  0.25975367 -1.7849637\n",
      " -0.6012992   1.0028936   0.78561294  0.07730709  0.4696674  -0.48145908\n",
      " -0.00316865 -1.9760407  -0.08393261  0.11784887  1.2656853  -0.5249868\n",
      "  0.3182674   0.45680985 -0.4472887  -0.32462436 -0.92281055  0.28334874\n",
      "  1.1180803   0.06162243 -0.08688541 -0.9917644  -0.26961812  0.40924034\n",
      "  0.4008505  -1.2847699  -0.56720024 -1.4302909   0.28427872 -0.7403414\n",
      " -0.71390074  0.02349045  0.98195964 -0.06941931 -1.1503823   0.03580956\n",
      " -0.20438397  0.8060666   0.34001172 -1.420007   -1.3811796  -0.25946775\n",
      " -0.6504957   0.26506352  1.0957309  -0.03950319 -0.9075906   0.29268694\n",
      " -0.4622718   0.53714085 -0.68410915  0.21584918 -0.528052    1.075222\n",
      "  1.117179    0.7701692  -0.75786674  1.1643965  -0.40643275  0.71804243\n",
      "  1.0749515  -0.47401437  0.57457703  0.57578593  0.10439728 -0.39164957\n",
      " -1.1636435  -0.05190044 -0.24959731 -0.837277   -0.6171033   1.1416694\n",
      " -0.17987466 -0.03038325  0.05810107  0.32290325  1.3564587   0.20147242\n",
      "  0.41625667  0.13705021  0.32561877 -0.33966637  2.1703248   1.0634131\n",
      "  0.6928267  -1.3955834  -0.05833076  0.405967  ]\n"
     ]
    }
   ],
   "source": [
    "print(W2V.wv.get_vector(\"comenzó\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos guardar el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2V.save('datos/word2vec.model')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "o leer el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2V= Word2Vec.load('datos/word2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.95365494  1.1842033   0.95986784 -0.20210184  0.06581045 -1.5050063\n",
      "  0.3009835   3.0103698  -0.39941457 -0.67336667]\n"
     ]
    }
   ],
   "source": [
    "print(W2V.wv.get_vector(\"comenzó\")[:10])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelación de documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, OrderedDict\n",
    "\n",
    "def modela_documentos_rep(df):\n",
    "    id_=df.doc_id.values\n",
    "    datos=df.drop(columns=['doc_id'])\n",
    "    datos=datos.values\n",
    "    dx=[]\n",
    "    for i,doc_id in enumerate(id_):\n",
    "        dx.append((doc_id,datos[i]))\n",
    "    do=pd.DataFrame(dx,columns=['doc_id','Vectores'])\n",
    "    return do\n",
    "\n",
    "def modela_documentos(df,w2v):\n",
    "    docs=df.doc_id.values\n",
    "    textos=df.Texto.str.split(' ').values.tolist()\n",
    "    d=[]\n",
    "    Dx=[]\n",
    "    for i,texto in enumerate(textos):\n",
    "        for w in texto:\n",
    "            e=w2v.wv.get_vector(w)\n",
    "            d.append(e)\n",
    "        d=np.array(d)\n",
    "        dx=np.sum(d,axis=0)/len(d)\n",
    "        Dx.append([docs[i],dx])\n",
    "        d=[]\n",
    "    do=pd.DataFrame(Dx,columns=['doc_id','W2V'])\n",
    "    return do\n",
    "\n",
    "def k_vecinos_mas_cercanos(docus,df,k=1):\n",
    "    l=docus.doc_id.values\n",
    "    vec=OrderedDict()\n",
    "    for id_ in l:\n",
    "        d=dist_vecinos(id_,df)\n",
    "        for i in range(k):\n",
    "            if i==0:\n",
    "                vec[id_]=[[d[i][1],d[i][2]]]\n",
    "            else:\n",
    "                vec[id_].append([d[i][1],d[i][2]])\n",
    "    return vec\n",
    "\n",
    "def vecinos_mas_cercanos(df,distancias):\n",
    "    l=df.doc_id.values\n",
    "    vec=OrderedDict()\n",
    "    for id_ in l:\n",
    "        for i,d in enumerate(distancias):\n",
    "            if id_ == d[0]:\n",
    "                vecino=d[1]\n",
    "                if id_ not in vec.keys():\n",
    "                    vec[id_]=[(vecino,d[2])]\n",
    "                else:\n",
    "                    vec[id_].append((vecino,d[2]))\n",
    "            elif id_== d[1]:\n",
    "                vecino=d[0]\n",
    "                if id_ not in vec.keys():\n",
    "                    vec[id_]=[(vecino,d[2])]\n",
    "                else:\n",
    "                    vec[id_].append((vecino,d[2]))\n",
    "    return vec\n",
    "\n",
    "def dist_vecinos(id_docu,df):\n",
    "    dist=[]\n",
    "    candidato = df[df['doc_id']==id_docu]\n",
    "    candidato = candidato.iloc[:,1].values[0]\n",
    "    fila=df.index[df['doc_id'] == id_docu].tolist()\n",
    "    pts=df.drop(df.index[fila])\n",
    "    id_=pts.doc_id.values\n",
    "    pts=pts.iloc[:,1].values\n",
    "\n",
    "    for i in range(len(pts)):\n",
    "        d = np.sqrt(np.sum(np.square(candidato-pts[i])))\n",
    "        dist.append((id_docu,id_[i],d))\n",
    "    dist=sorted(dist,key=lambda x: x[2])\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edf=modela_documentos(df,W2V)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Ejercicio 1**\n",
    "\n",
    "Queremos saber que tan bien podemos modelar documentos utilizando Word2Vec.\n",
    "\n",
    "Compara estos resultados con los obtenidos por les métodos PCA y LSA."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Documentos de análisis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>Texto</th>\n",
       "      <th>clase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1885635</td>\n",
       "      <td>ornithopus perpusillus planta familia fabáceas...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1886681</td>\n",
       "      <td>frivolité variedad curiosa encaje pasamanería ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1890585</td>\n",
       "      <td>dekker apellido puede referirse siguientes per...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1877195</td>\n",
       "      <td>vida suficiente segundo álbum banda mexicali i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1874126</td>\n",
       "      <td>guyencourtsurnoye población comuna francesa re...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    doc_id                                              Texto  clase\n",
       "0  1885635  ornithopus perpusillus planta familia fabáceas...      0\n",
       "1  1886681  frivolité variedad curiosa encaje pasamanería ...      0\n",
       "2  1890585  dekker apellido puede referirse siguientes per...      0\n",
       "3  1877195  vida suficiente segundo álbum banda mexicali i...      0\n",
       "4  1874126  guyencourtsurnoye población comuna francesa re...      0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# docus = df[(df['doc_id']=='1023628') |\\\n",
    "#            (df['doc_id']=='1024447') |\\\n",
    "#            (df['doc_id']=='1035967') |\\\n",
    "#            (df['doc_id']=='1891029') |\\\n",
    "#            (df['doc_id']=='1894599') ]  \n",
    "docus = df.sample(n=5)\n",
    "docus.index=range(len(docus.index))\n",
    "\n",
    "docus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=1\n",
    "vecinos=k_vecinos_mas_cercanos(docus,edf,k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('1885635', [['1885583', 0.23433523]]), ('1886681', [['1889858', 0.11623342]]), ('1890585', [['1894945', 0.2674673]]), ('1877195', [['1885993', 0.20900859]]), ('1874126', [['1874168', 0.006189191]])])\n"
     ]
    }
   ],
   "source": [
    "print(vecinos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ornithopus perpusillus planta familia fabáceas descripción pelosa anual tallos extendidos hojas pares folíolos pequeños elípticos oblongos flores blancas rosas cabezuelas flores brácteas bajo cabezuela largas flores lóbulos dientes cáliz mitad largo tubo vaina constreñida segmentos pico recto ganchudo florece primavera otoño hábitat habita lugares arenisca arena distribución bélgica gran bretaña dinamarca francia alemania irlanda holanda suecia suiza españa italia portugal polonia rumanía enlaces externos']\n",
      "   ['cuernecillo grande lotus uliginosus especie botánica erecta ascendente pelosa perenne tallos huecos hojas folíolos obovados verdiazules debajo flores amarillas menudo teñidas rojo cabezuelas largo cabillo normalmente flores pétalos dientes cáliz extendidos brote aproximadamente largo tuco vaina florece mayo agosto habita pantanos prados húmedos gran parte europa introducida noruega finlandia hungría centro españa habita comunidades quercofraxinetum enlaces externos']\n",
      "\n",
      "['frivolité variedad curiosa encaje pasamanería llama encaje lanzadera españa tatting gran bretaña occhi italia incluso makuk oriente consiste montar sucesión nudos baguillas único hilo ayuda dos lanzaderas materiales lanzaderas lanzadera llamada naveta países hispanoamericanos útil realiza preciada labor consiste especie aguja alargada guarda suelta hilo gracias extremos apuntados mencionan tres grandes tipos lanzaderas sencillas lanzaderas frecuentes encuentran mercado actualmente fabriacadas pasta plástica madera hace económicas clover deben nombre casa comercial creó deformación anteriores puntas pronunciada permite trabajar baguillas picots mayor precisión victorianas datan inglaterra finales siglo xix ahí nombre éstas constan canilla parecida máquinas coser estrecha cubierta especie ganchillo lanzadera propiamente dicha mencionar variedad atípica frivolité hace aguja coser ahora desuso hilo recomienda vivamente uso hilos resistentes tensión llega soportar hilo lanzadera enorme generalmente suele pasar límite números ganchillo mayoría dibujos deben llevar cabo serie uniones baguillas ello necesario sacar hebra hora llevar cabo operación preferible utilizar ganchillo apropiado alfiler último pica hilo decir quita rizo habitual pierde calidad punto básico doble nudo nudo aparece siempre labores punto doble nudo compone nudo izquierdo nudo derecho nudo doble bien elaborado permite corra hilo lanzadera principal través éste principio fundamental frivolité baguillas tipos baguillas picots características asas dan verdadera belleza tipo pasamanería resultado dejar hilo dos nudos dobles luego jutan distinguen dos tipos según función picots decoración aquellos sirven enbellecer dibujo tamaño varía función artesano canon estilo extendido cuanta mayor gracia quiera dar labor grandes picots unión aquellos sirven enlazar sección generalmente tamaño mínimo embargo dibujos puntillas demás trabajos surgen picots ambas funciones elementos decorativos distinguir dos motivos fundamentales encaje frivolité anillos arcos anillos círculos cerrados nudos dobles trabajan circunferencia traza mano izquierda hilo lanzadera hilo realiza nudos hacen proceden lanzadera arcos líneas curvas ejecutan dos lanzaderas hilo auxiliar casi siempre utiliza ovillo siempre van dar anillo usos patrones frecuentes frivolité suelen mostrar puntillas variados anchos aplicaciones plan central circulares pueden unir formar tapetes paños etc']\n",
      "   ['dracomon personaje ficción criatura digimon anime manga digimon momento única aparición sido juegos nintendo dracomon digimon dragón sangre pura pues primero clase siendo primer dragón historia mundo digital ancestro digimons cuyos nombres acaban dramon nombre significa dragón latin fuerza agilidad dracomon combierten mejores nivel principiante posee gekirin escama invertida dragon algún casual escama tocada dracomon perderá juicio entrando modo rabia furiosa atacara vea bolas fuego amigo enemigo apariencia dracomon digimon pequeño corpulento parece dragón mide pies alto piel verdosa menos parte mandibula cuello estomago base cola tono marrón claro ojos grandes forma disco rojos antebrazos pequeñas resto cuerpo tres dedos dando imprecion típico dragon antiguo tierra cortos brazos delanteros cuerpo grandes piernas cola alas tripa haci pecho desciende cuello base cola semicirculo cola vastante grande tener nivel tan bajo corriente digimons mayor nivel cabeza hocico compensados cuerpo cabeza provista cuernos forma cuernos dragon chino dicho dando imprecion antiguo dragon tierra aparte posee pequeñas alitas color rojo permitiéndole volar cortos periodos tiempo evoluciones digievoluciones nivel cuerpo nombre bebé bebé petitmon bgcolorefefef entrenamiento babydmon principiante infantil dracomon bgcolorefefef campeón maduroadulto coredramon puede ser azul verde megaultimo perfecto groundramon coredramon verde wingdramon coredramon azul bgcolorefefef hipermega supremo breakdramon verde slayerdramon azul adn digievolución fusion examon slayerdramon breakdramon ataques dracomon arsenal ataques francamente demoledores baby breath dispara aliento ardiende lanzallamas tail smash golpea cola shururen tocan gekirin activa técnica dispara bolas fuego indiscriminadamente vea referencias dma digidex wikimon']\n",
      "\n",
      "['dekker apellido puede referirse siguientes personas desmond dekker cantante ska erik dekker ciclista retirado ganador copa mundo ciclismo jeremias dekker poeta thomas dekker ciclista equipo rabobank thomas dekker escritor además puede referirse algoritmo dekker programación concurrente exclusión mutua']\n",
      "   ['término luttenberger puede referirse peter luttenberger ciclista profesional luttenbergerklug dúo musical poprock michelle luttenberger cantante femenina dúo anterior']\n",
      "\n",
      "['vida suficiente segundo álbum banda mexicali insite lanzado temática canciones habla acerca oportunidad vivir suficiente alcanza necesita vida poder expresar quieres sientes explica tema contigo muerte llévame contigo separes minuto hoy sol regrese vez disco cambia profundamente género musical entonces álbum historia primer sencillo disco tema cielos lloran tour vida suficiente tour insite toda latinoamérica comenzó abril comenzado méxico primeramente ciudad natal mexicali luego monterrey siendo sede auditorio cocacola parque fundidora luego guadalajara último ciudad méxico aclamados recibieron buenas críticas fans personas presentaron espectáculo fines junio comenzó gira centroamérica sudámerica último españa participación insite mucha participación año mediados meses enero junio constantes participanes vive latino espera insite participe mtv video music awards sencillos cielos lloran sola siempre dejas lista canciones títuloduración destrózame sola día discúlpame rindo pregunta amé contigo muerte siempre dejas continuación quisiera lejos luces cielos lloran soñar extras títuloduración rifan the making vida suficiente día agradecimientos']\n",
      "   ['carlos enrique estremera colón conocido cano estremera nació septiembre santurce puerto rico excelente estudiante escuela primaria motivado dios padres estudiar música empezó cantando himnos religiosos comenzó mundo profesional percusionista grupo músicos formó barrio obrero santurce sección dentro año vocalista grupo plena grupo folklórico pleneros quinto olivo mayor oportunidad llegó unirse orquesta mulenze contrato exclusivo fania records reunió bobby valentín invitó unirse banda primera canción grabó grupo valentín boda ganó aclamación generalizada mundo música popular grabó seis discos valentín incluyendo dos solista finales decidió abandonar valentín formar propia banda nueva etapa cano conocido dedicó cantar géneros música popular ganó premio paoli vocalista salsa año premios estremera debutó productor propios registros salvaje álbum año tarde dio conocer dueño soneo nueva serie grabaciones cano nació albino conocido industria música mejores soneros cantante afrocaribe música siglo habilidad demostró ahora famoso concierto guánica arena estrofas consecutivos repetir sola frase logro mismo año habilidades improvisación pone prueba vez produjo única soneos concierto fans yabucoa mientras unas pocas semanas tarde espectadores concierto juana díaz número aumentó afilado destreza improvisar par estremera ganado epíteto titular soneo título prácticamente sustituido pibe oro inicialmente conocido utilizó título dos grabaciones álbum hit dueño soneo vol incluyen salsa pistas acaso dueño soneo vol exitos dueño soneo éxitos musicales estremera mejor conocido canciones ingratitudes manuel garcía caso mejoro mujer primavera quedé ganas novia automática awilda compromiso nací así aprovecha viernes social discografía puerto rican maestra historia salsa opera ecuajey vol sonora ponceña años diferente encuentro histórico punto aparte cambio éxitos dueño soneo dueño soneo vol dueño soneo vol salvaje niño oro cano estremera orquesta bobby valentín acción brujería bobby valentín bobby valentín presenta cano estremera siempre forma bobby valentín bobby valentín gato bobby valentín presenta cano estremera boda']\n",
      "\n",
      "['guyencourtsurnoye población comuna francesa región picardía departamento somme distrito amiens cantón boves demografía enlaces externos insee elecciones municipales']\n",
      "   ['grattepanche población comuna francesa región picardía departamento somme distrito amiens cantón boves demografía enlaces externos insee elecciones municipales']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lista = []\n",
    "for item in vecinos:\n",
    "    vecino = vecinos[item][0][0]\n",
    "    lista.append((item,vecino))\n",
    "for docs in lista:\n",
    "    print(df.loc[df['doc_id']==docs[0]].Texto.values)\n",
    "    print('  ',df.loc[df['doc_id']==docs[1]].Texto.values)\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Ejercicio 2**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelo SKIP-gram... Ojo! Es tardado..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "vec_dim= 300\n",
    "w2v_sg = Word2Vec(documentos, min_count=1, size=vec_dim, workers=4, window=5, iter=30,sg=1)\n",
    "print(W2V[\"comenzó\"][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_sg=Word2Vec.load('/content/gdrive/My Drive/MisCursos/word2vec_sg.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edf_sg=wi.modela_documentos(df,w2v_sg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=1\n",
    "vecinos_sg=wi.k_vecinos_mas_cercanos(docus,edf_sg,k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vecinos_sg['1891029'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1=df[df['doc_id']=='1891029'].values\n",
    "test2=df[df['doc_id']=='1896707'].values\n",
    "print(test1)\n",
    "print(test2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "</hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lsa=pd.read_pickle('datos/data_frame_4K.pkl')\n",
    "df_lsa.index = range(len(df_lsa.index))\n",
    "print(df_lsa.shape)\n",
    "df_lsa.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from collections import Counter, OrderedDict\n",
    "\n",
    "def bow_(docs):\n",
    "    v = DictVectorizer(sparse=False)\n",
    "    X = v.fit_transform(docs)\n",
    "    return X,v\n",
    "\n",
    "docs = df_lsa.Conteos.tolist()\n",
    "X,vocab_ = bow_(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "q=300  #Elegimos usar q componentes\n",
    "svd = TruncatedSVD(n_components=q, n_iter=7, random_state=42)\n",
    "\n",
    "vectorizer = TfidfVectorizer(vocabulary=vocab_.vocabulary_)\n",
    "\n",
    "corpus = df_lsa.Texto.tolist()\n",
    "D_tfidf = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dlsa=svd.fit_transform(D_tfidf)\n",
    "dlsa=wi.get_dataFrame(dlsa,df_lsa)\n",
    "print(dlsa.shape)\n",
    "dlsa.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_vr=svd.explained_variance_ratio_\n",
    "wi.distribucion_vr(svd_vr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q=300  #debe ser <= 300 o debes correr de nuevo el algoritmo más arriba\n",
    "lsa_rep=wi.get_representativos(dlsa,q)\n",
    "print(lsa_rep.shape)\n",
    "lsa_rep.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edf_lsa=wi.modela_documentos_rep(lsa_rep)\n",
    "print(edf_lsa.shape)\n",
    "edf_lsa.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=1\n",
    "vecinos_lsa=wi.k_vecinos_mas_cercanos(docus,edf_lsa,k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vecinos_lsa['1891029'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1=df[df['doc_id']=='1891029'].Texto.values[0][:400]\n",
    "test2=df[df['doc_id']=='1035967'].Texto.values[0][:400]\n",
    "print(test1)\n",
    "print(test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vecinos_lsa['1023628'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1=df[df['doc_id']=='1023628'].Texto.values[0][:400]\n",
    "test2=df[df['doc_id']=='1885462'].Texto.values[0][:400]\n",
    "print(test1)\n",
    "print(test2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Referencias** <br>\n",
    "\n",
    "Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and Christian Janvin. 2003. A neural probabilistic language model. J. Mach. Learn. Res. 3 (March 2003), 1137-1155. \n",
    "\n",
    "Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Distributed representations of words and phrases and their compositionality. In _Proceedings of the 26th International Conference on Neural Information Processing Systems - Volume 2 (NIPS'13)_, C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger (Eds.), Vol. 2. Curran Associates Inc., USA, 3111-3119. \n",
    "\n",
    "Xin Rong. 2014. Word2vec Parameter Learning Explained.arXiv 1411.2738. disponible en linea {http://arxiv.org/abs/1411.2738}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
