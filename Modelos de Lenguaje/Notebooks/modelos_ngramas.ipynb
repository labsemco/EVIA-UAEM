{"cells":[{"cell_type":"markdown","source":["# Modelos de N-gramas\n","\n","En un modelo de lenguaje la probabilidad condicional de una palabra $w_m$ dado un contexto $w_1w_2...w_{m-1}$, se puede calcular como:\n","\n","$$P(w_m|w_1^{m-1}) = \\frac{C(w_1w_2...w_m)}{‚àë_{w\\in V} C(w_1w_2...w_{m-1}w)}$$\n","\n","donde $V$ es el vocabulario del modelo.\n","\n","Pero un modelo de N-gramas aproxima esa $P(w_m|w_1^{m-1})$ como:\n","\n","$$P(w_m|w_1^{m-1}) ‚âà P(w_m|w_{m-n+1}^{m-1}) $$\n","\n","donde el N-grama es $(w_{m-n+1}w_{m-n+2}...w_{m-1}w_m)$\n","\n","La idea intituiva de esta aproximaci√≥n es que en vez de considerar todo el contexto anterior a una palabra, se considera solo las $N-1$ palabras anteriores.\n","\n","Para calcular estas probabilidades se necesita de una colecci√≥n de textos (corpus) de donde se recompilen los conteos de N-gramas y el vocabulario de palabras que usara el modelo, posteriormente se calcula una tabla de probabilidades por cada N-grama que aparezca en el corpus, una vez teniendo estas probabilidades, se pueden usar para estimar la probabilidad de una oraci√≥n de la siguiente forma:\n","\n","$$P(w_1^m) = P(w_1)P(w_2|w_1)P(w3|w_1^2)...P(w_m|w_1^{m-1})$$\n","$$ \\approx ùõ±_{k=1}^m P(w_k|w_{k-n+1}^{k-1}) \\qquad \\qquad\\qquad\\,$$\n","\n","Para que todas las probabilidades tengan contexto suficiente se agregan a la oraci√≥n $N-1$ tokens especiales \"\\<s\\>\" que indican el inicio de oraci√≥n, y un token especial \"\\<\\s\\>\" que indica el final de la oraci√≥n. Entonces para $N = 3$ la oraci√≥n \"*nunca fui popular*\" se transforma en:\n","\n","$$<s> \\;\\; <s> \\;\\; nunca \\;\\; fui \\;\\; popular \\;\\; <\\backslash s>$$"],"metadata":{"id":"US6RJZ5ejnsz"}},{"cell_type":"markdown","source":["#### Clases de Vocabulario y N-gramas"],"metadata":{"id":"4MPsdzv-zCGk"}},{"cell_type":"code","source":["from IPython.display import Markdown, display\n","import ipywidgets as widgets\n","\n","import numpy as np\n","import pandas as pd\n","import random\n","from collections import Counter, OrderedDict\n","import matplotlib.pyplot as plt\n","import seaborn as sns"],"metadata":{"id":"jy5P9aQ8tPXh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def laplace(conteoGrande,conteoPeque,tamVocabulario):\n","    return (conteoGrande+1)/(conteoPeque+tamVocabulario)\n","\n","class Vocabulario():\n","  def __init__(self,corpus):\n","    conteoVocabulario = {}\n","    for oracion in corpus:\n","      for i in range(0,len(oracion)):\n","        conteoVocabulario[oracion[i]] = conteoVocabulario.setdefault(oracion[i], 0) + 1\n","\n","    self.palabras = pd.DataFrame.from_dict(conteoVocabulario,orient=\"index\",columns=[\"Frecuencia\"])\n","    self.palabras = self.palabras.sort_values(\"Frecuencia\",ascending=False)\n","    self.palabras_unk = None\n","\n","  def __len__(self):\n","    return len(self.palabras)\n","\n","  def reducir_vocabulario(self,criterio,parametro):\n","    if criterio == \"N palabras\":\n","      self.palabras_unk = self.palabras.tail(len(self.palabras)-parametro).copy()\n","      self.palabras = self.palabras.head(parametro)\n","      self.palabras.loc[\"<UNK>\"] = {\"Frecuencia\":self.palabras_unk[\"Frecuencia\"].sum()}\n","      self.palabras.loc[\"<\\s>\"] = {\"Frecuencia\":0}\n","      self.palabras = self.palabras.sort_values(\"Frecuencia\",ascending=False)\n","\n","    else: # \"FrecMin\"\n","      self.palabras_unk = self.palabras[self.palabras.apply(lambda x: x[\"Frecuencia\"] < parametro,axis=1)].copy()\n","      self.palabras = self.palabras[self.palabras.apply(lambda x: x[\"Frecuencia\"] >= parametro,axis=1)].copy()\n","      self.palabras.loc[\"<UNK>\"] = {\"Frecuencia\":self.palabras_unk[\"Frecuencia\"].sum()}\n","      self.palabras.loc[\"<\\s>\"] = {\"Frecuencia\":0}\n","      self.palabras = self.palabras.sort_values(\"Frecuencia\",ascending=False)\n","\n","  def imprimir_datos_vocabulario(self):\n","    display(Markdown(f'**N√∫mero total de palabras diferentes en el vocabulario reducido:**'))\n","    print(len(self.palabras))\n","    display(Markdown(f'**10 palabras con mayor frecuencia:**'))\n","    print(self.palabras.iloc[:100])\n","    display(Markdown(f'**Mediana de las frecuencias de palabras**'))\n","    print(self.palabras.median()[\"Frecuencia\"])\n","    display(Markdown(f'**10 palabras con menor frecuencia:**'))\n","    print(self.palabras.iloc[-11:-1])\n","\n","    display(Markdown(f'**Grafica Rango y Frecuencia:**'))\n","    self.palabras['Rango'] = range(1,len(self.palabras)+1)\n","    plt.plot(self.palabras['Rango'],self.palabras['Frecuencia'],'bo',markersize=3)\n","    plt.yscale('log')\n","    plt.xscale('log')\n","    plt.xlabel(\"Rango\")\n","    plt.ylabel(\"Frecuencia\")\n","    plt.show()\n","\n","class Ngrama():\n","  def __init__(self,n,vocabulario,suavizado = \"Laplace\"): # Suavizado = [\"Laplace\",\"Backoff\"]\n","    self.n = n\n","    self.vocabulario = vocabulario\n","    self.Nmenos1Gramas = None #nMenos1Grama\n","    self.suavizado = suavizado\n","\n","    self.numTokens = 0\n","\n","    self.conteos = None\n","    self.probabilidades = None\n","\n","  def transformar_oracion(self,oracion):\n","    # Transforma una oracion dependiendo del vocabulario administrado\n","    nueva_oracion = []\n","    for palabra in oracion:\n","      if palabra in self.vocabulario.palabras.index or palabra == \"<s>\" or palabra == \"<\\s>\":\n","        nueva_oracion.append(palabra)\n","      else:\n","        nueva_oracion.append(\"<UNK>\")\n","    return nueva_oracion\n","\n","  def transformar_corpus(self,corpus):\n","    # Transforma todo un corpus dependiendo del vocabulario administrado\n","    corpus_transformado = []\n","    for oracion in corpus:\n","      oracion_transformada = self.transformar_oracion(oracion)\n","      corpus_transformado.append(oracion_transformada)\n","    return corpus_transformado\n","\n","  def contar_ngramas(self,corpus):\n","    # Realiza los conteos de ngramas que aparezcan en el corpus\n","    self.conteos = {}\n","    if self.n == 1:\n","      self.conteos[(\"<s>\",)] = len(corpus)\n","      self.conteos[(\"<\\s>\",)] = len(corpus)\n","    else:\n","      ngrama_inicio = [\"<s>\"]*self.n\n","      self.conteos[tuple(ngrama_inicio)] = len(corpus)\n","\n","    inicios = []\n","    for i in range(self.n-1):\n","      inicios.append(\"<s>\")\n","\n","    for oracion_normal in corpus:\n","      oracion = inicios + oracion_normal + [\"<\\s>\"]\n","\n","      for i in range(len(oracion)-self.n+1):\n","        ngrama = []\n","        for j in range(self.n):\n","          ngrama.append(oracion[i+j])\n","        ngrama = tuple(ngrama)\n","        self.conteos[ngrama] = self.conteos.setdefault(ngrama, 0) + 1\n","\n","    return self.conteos\n","\n","  def entrenar(self,corpus_orginal):\n","    # Transforma el corpus dependiendo el vocabulario que se tiene\n","    corpus = self.transformar_corpus(corpus_orginal)\n","\n","    # Hace el calculo de probabilidades en base al corpus y el suavizado\n","    self.numTokens = 0\n","    for oracion in corpus:\n","      self.numTokens += len(oracion) + 1\n","\n","    # Realiza los conteos de nGramas en el corpus transformado\n","    self.contar_ngramas(corpus)\n","\n","    # Calcula las probabilidades de los nGramas encontrados, usando el suavizado de Laplace\n","    self.probabilidades = pd.DataFrame.from_dict(self.conteos,orient=\"index\",columns=[\"conteos\"])\n","    if self.n == 1:\n","      self.probabilidades[\"probabilidad\"] = self.probabilidades[\"conteos\"]/self.numTokens\n","    else:\n","      self.Nmenos1Gramas = Ngrama(self.n-1,self.vocabulario,self.suavizado)\n","      self.Nmenos1Gramas.contar_ngramas(corpus)\n","      if self.suavizado == \"Laplace\":\n","        self.probabilidades[\"probabilidad\"] = self.probabilidades.apply(lambda x: laplace(x[\"conteos\"],self.Nmenos1Gramas.conteos[tuple(x.name[:self.n-1])],len(self.vocabulario)),axis = 1)\n","      elif self.suavizado == \"Backoff\":\n","        self.probabilidades[\"probabilidad\"] = self.probabilidades.apply(lambda x: x[\"conteos\"]/self.Nmenos1Gramas.conteos[tuple(x.name[:self.n-1])],axis = 1)\n","        self.Nmenos1Gramas.entrenar(corpus)\n","      else:\n","        print(\"ERROR: (\"+self.suavizado+\") No es un tipo de suavizado valido\")\n","\n","    self.probabilidades = self.probabilidades.drop(columns=[\"conteos\"])\n","    self.probabilidades.drop([tuple([\"<s>\"]*self.n)],inplace=True) # Se elimina de las probabilidades el nGrama de puros inicios de oraci√≥n \"<s>\" (Solo servia para calcular las probabilidades)\n","\n","  def siguiente_palabra(self,contexto_original):\n","    # Regresa una palabra al azar dado un contexto y su probabilidad\n","    # Contexto es una lista de palabras, (limpia pero con posibles palabras que se deban cambiar a <UNK>)\n","    # Return (Palabra siguiente, prob_palabra_sig, contexto_usado)\n","    if(len(contexto_original) != self.n-1):\n","      print(\"Error: Contexto muy grande\")\n","      return 0,0,None\n","\n","    contexto = self.transformar_oracion(contexto_original)\n","\n","    if self.n == 1:\n","      busqueda = self.probabilidades.sample(1,weights=\"probabilidad\")\n","      return busqueda.index[0][0], busqueda[\"probabilidad\"][0],None\n","\n","    # self.n >= 2\n","    if self.suavizado == \"Laplace\":\n","      busqueda = self.probabilidades[self.probabilidades.apply(lambda x: x.name[0:self.n-1] == tuple(contexto), axis=1)]\n","      if len(busqueda) == 0: #No se encontr√≥ el contexto en los conteos de nGramas, todas las palabras siguientes tienen probabilidad 1/|V|\n","        return self.vocabulario.palabras.sample(1).index[0],1/len(self.vocabulario),tuple(contexto)\n","      busqueda = busqueda.sample(1,weights=\"probabilidad\")\n","      return  busqueda.index[0][self.n-1], busqueda[\"probabilidad\"][0],tuple(contexto)\n","    elif self.suavizado == \"Backoff\":\n","      busqueda = self.probabilidades[self.probabilidades.apply(lambda x: x.name[0:self.n-1] == tuple(contexto), axis=1)]\n","      if len(busqueda) == 0: #No se encontr√≥ el contexto en los conteos de nGramas\n","        # Se usa un contexto menor para obtener la siguiente palabra\n","        return self.Nmenos1Gramas.siguiente_palabra(contexto[1:])\n","      busqueda = busqueda.sample(1,weights=\"probabilidad\")\n","      return  busqueda.index[0][self.n-1], busqueda[\"probabilidad\"][0],tuple(contexto)\n","\n","    print(\"ERROR: (\"+self.suavizado+\") No es un tipo de suavizado valido\")\n","    return 0,0,None\n","\n","  def generar_oracion(self,longitud_maxima,reemplazar_unk=False):\n","    # Genera una oraci√≥n al azar dependiendo las probabilidades de los nGramas calculados\n","    contexto = [\"<s>\"]*(self.n-1)\n","\n","    oracion = []\n","    extras = []\n","    for i in range(longitud_maxima):\n","      palabra,probabilidad,contexto_usado = self.siguiente_palabra(contexto)\n","      extras.append((probabilidad,contexto_usado))\n","      if palabra == \"<UNK>\" and reemplazar_unk:\n","        palabra = \"(UNK)\"+self.vocabulario.palabras_unk.sample(1,weights=\"Frecuencia\").index[0]\n","      oracion.append(palabra)\n","      if self.n > 1:\n","        contexto = contexto[1:]+[oracion[-1]]\n","      if oracion[-1] == \"<\\s>\":\n","        break\n","\n","    return oracion,extras\n","\n","  def probabilidad_ngrama(self,ngrama_original):\n","    # Regresa la probabilidad de un nGrama\n","    if(len(ngrama_original) != self.n):\n","      print(\"Error: Ngrama muy grande\")\n","      return 0,0,None\n","\n","    ngrama = self.transformar_oracion(ngrama_original)\n","\n","    if self.n == 1:\n","      busqueda = self.probabilidades[self.probabilidades.apply(lambda x: x.name == tuple(ngrama), axis=1)]\n","      return busqueda[\"probabilidad\"][0],tuple(ngrama)\n","\n","    # self.n >= 2\n","    if self.suavizado == \"Laplace\":\n","      busqueda = self.probabilidades[self.probabilidades.apply(lambda x: x.name == tuple(ngrama), axis=1)]\n","      if len(busqueda) == 0: #No se encontr√≥ el nGrama C(w_n|contexto) = 0, ahora se busca C(w|contexto)\n","        conteos_contexto = self.Nmenos1Gramas.conteos.setdefault(tuple(ngrama[:-1]), 0)\n","        return laplace(0,conteos_contexto,len(self.vocabulario)),tuple(ngrama)\n","      return  busqueda[\"probabilidad\"][0],tuple(ngrama)\n","    elif self.suavizado == \"Backoff\":\n","      busqueda = self.probabilidades[self.probabilidades.apply(lambda x: x.name == tuple(ngrama), axis=1)]\n","      if len(busqueda) == 0: #No se encontr√≥ el nGrama C(w_n|contexto) = 0, ahora se busca C(w|contexto[:-1])\n","        # Se usa un contexto menor para obtener la siguiente palabra\n","        return self.Nmenos1Gramas.probabilidad_ngrama(ngrama[1:])\n","      return  busqueda[\"probabilidad\"][0],tuple(ngrama)\n","\n","    print(\"ERROR: (\"+self.suavizado+\") No es un tipo de suavizado valido\")\n","    return 0,0,None\n","\n","  def probabilidad_oracion(self,oracion_original,log = False):\n","    # Regresa la probabilidad de una oraci√≥n\n","    oracion = [\"<s>\"]*(self.n-1) + oracion_original + [\"<\\s>\"]\n","\n","    probabilidad = 0 if log else 1\n","    extras = []\n","    for i in range(len(oracion)-self.n+1):\n","        ngrama = []\n","        for j in range(self.n):\n","          ngrama.append(oracion[i+j])\n","        prob_ngrama,ngrama_usado = self.probabilidad_ngrama(ngrama)\n","        probabilidad = probabilidad+np.log(prob_ngrama) if log else probabilidad*prob_ngrama\n","        extras.append([prob_ngrama,ngrama_usado])\n","    return probabilidad,extras,len(oracion)-self.n+1\n"],"metadata":{"id":"xTZrisAOTx0U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def limpiar_oracion(oracion,stopwords,delimeters):\n","  #Limpia una oracion transformando palabras en minusculas, quitando stopwords y delimeters (puntos, comas, parentesis)\n","  nueva_oracion = []\n","  for palabra in oracion:\n","    palabra_nueva = palabra.lower()\n","    if not(palabra_nueva in stopwords):\n","      for delimeter in delimeters:\n","        palabra_nueva = palabra_nueva.replace(delimeter,\"\")\n","      if palabra_nueva != \"\":\n","        nueva_oracion.append(palabra_nueva)\n","  return nueva_oracion"],"metadata":{"id":"CH4Ua2boDug7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def imprimir_prob(palabra,prob,contexto):\n","  print(\"P(\"+palabra,end=\"\",sep=\"\")\n","  if(contexto != None and len(contexto) != 0): # Osea se utiliz√≥ algo de contexto\n","    print(\"|\",end=\"\")\n","    for j in range(len(contexto)-1):\n","      print(contexto[j]+\",\",end=\"\")\n","    print(contexto[-1],end=\"\")\n","  print(\") = \",prob,sep=\"\")"],"metadata":{"id":"CzOPlMjYEdn7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["stopwords_esp = [\"el\",\"no\",\"ellos\",\"si\"]\n","delimeters = [\",\",\".\",\"(\",\")\",\";\",\":\",\"[\",\"]\",\" \",\"\\t\",\"\\n\",\"'\",'\"']"],"metadata":{"id":"kjYAGaS8Fj83"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Selecci√≥n del corpus"],"metadata":{"id":"xi9xTbGRzJhI"}},{"cell_type":"code","source":["#@title ### Obtenci√≥n del corpus\n","\n","#@markdown Especifica el nombre del archivo desde el cual se leera el corpus\n","archivo_corpus = \"Canciones zo\\xE9 por fila.csv\" #@param {type:\"string\"}\n","columna_texto = \"Letra\" #@param {type:\"string\"}\n","\n","df_corpus = pd.read_csv(archivo_corpus)\n","corpus = []\n","for i in range(len(df_corpus)):\n","  oracion = df_corpus.iloc[i][columna_texto].split(\" \")\n","  corpus.append(limpiar_oracion(oracion,[],delimeters))\n","\n","\n","display(Markdown(f'**10 primeras palabras de 3 documentos del corpus:**'))\n","print(corpus[0][:10])\n","print(corpus[5][:10])\n","print(corpus[-1][:10])"],"metadata":{"id":"2VbpJ5wd_rLa","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Creaci√≥n del vocabulario"],"metadata":{"id":"7Q0V27bif3V6"}},{"cell_type":"code","source":["#@title ### Obtener el vocabulario del corpus\n","\n","#@markdown Calcula las frecuencias de cada palabra que ocurra en el corpus incluyendo todos los documentos\n","\n","# Adem√°s se calcula el n√∫mero de palabras (tokens) totales en el corpus\n","# Y la frecuencia maxima en el vocabulario\n","vocabulario_completo = Vocabulario(corpus)\n","vocabulario_completo.imprimir_datos_vocabulario()"],"metadata":{"id":"HRLHnqdmpOWi","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Reducir el vocabulario"],"metadata":{"id":"JwvDs49h8Lxd"}},{"cell_type":"code","source":["#@title ### Selecci√≥n de criterio\n","\n","#@markdown Dos formas de reducir el vocabulario son mantener en el vocabulario las N palabras con mayor frecuencia o mantener la palabras con una frecuencia mayor o igual a una frecuencia m√≠nima. Las palabras que no cumplan el criterio se cambian por el token \"\\<UNK\\>\"\n","criterio = \"N palabras\" #@param [\"N palabras\", \"Frecuencia minima\"]\n","\n","\n","lenVocabulario = len(vocabulario_completo.palabras)\n","frecMaxima = vocabulario_completo.palabras.iloc[0][\"Frecuencia\"]\n","nSlider = widgets.IntSlider(value=int(lenVocabulario/2), max=lenVocabulario)\n","minFrecSlider = widgets.IntSlider(value=int(frecMaxima/2), max=frecMaxima)\n","if criterio == \"N palabras\":\n","  display(Markdown(f'**Selecciona el n√∫mero N de palabras m√°s frecuentes a conservar:**'))\n","  display(nSlider)\n","else:\n","  display(Markdown(f'**Selecciona la frecuencia m√≠nima de las palabras a conservar:**'))\n","  display(minFrecSlider)"],"metadata":{"id":"nkjca8p2uYLA","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Reducci√≥n del vocabulario\n","#@markdown Se reduce el vocabulario dependiendo el criterio y parametro elegido\n","if criterio == \"N palabras\":\n","  vocabularioEsqN = Vocabulario(corpus)\n","  vocabularioEsqN.reducir_vocabulario(criterio,nSlider.value)\n","  vocabularioEsqN.imprimir_datos_vocabulario()\n","\n","else:\n","  vocabularioEsqMin = Vocabulario(corpus)\n","  vocabularioEsqMin.reducir_vocabulario(criterio,minFrecSlider.value)\n","  vocabularioEsqMin.imprimir_datos_vocabulario()\n"],"metadata":{"id":"1noOkezL1DdM","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Entrenamiento de modelos de N-gramas"],"metadata":{"id":"Ujh51m_48QTL"}},{"cell_type":"markdown","source":["En un modelo de N-gramas hay varias partes que se pueden realizar de formas diferentes, las formas en que se realizan estas partes constituyen los parametros del modelo.\n","\n","**Tama√±o de los N-gramas**: Define el n√∫mero de palabras que se tomaran en cuenta para el calculo de probabilidades. Por ejemplo para la oraci√≥n:\n","\n","$$veo \\;\\; una \\;\\; pelota \\;\\; roja$$\n","\n","En un modelo con tama√±o de N-gramas = 1, se calculan las probabilidades:\n","\n","$$P(veo),\\;\\;P(una),\\;\\;P(pelota),\\;\\;P(roja)$$\n","\n","Para un modelo con tama√±o de N-gramas = 3:\n","\n","$$P(veo|<s>,<s>),\\;\\;P(una|<s>,veo),\\;\\;P(pelota|veo,una),\\;\\;P(roja|una,pelota)$$\n","\n","**Vocabulario del modelo**: Define las palabras que reconoce el modelo, las palabras que no esten dentro del vocabulario (incluso si no aparecian en el corpus originalmente) seran transformadas al token \"\\<UNK\\>\" que representa a las palabras desconocidas (Unknow). Este vocabulario proviene de alguno de los 2 tipos de reducci√≥n hechas anteriormente.\n","\n","![](https://raw.github.com/labsemco/EVIA-UAEM/main/Modelos%20de%20Lenguaje/Notebooks/Imagenes/Palabras%20UNK.png)\n","\n","**Suavizado**: Define como se trataran los casos en los que la probabilidad de un N-grama sea 0, es decir, cuando un N-grama no aparece dentro del corpus que fue suministrado al modelo para su entrenamiento.\n","\n","* Laplace: Este suavizado cambia la forma en que se calculan las probabilidades de los N-gramas, agregando un conteo a todos los N-gramas, incluso si no estan en el corpus de entrenamiento, de esta forma evita que las probabilidades se hagan 0 para N-gramas que no esten en el corpus.\n","\n"," $$p(w_n|w_1^{n-1}) \\approx \\frac{C(w_n|w_1^{n-1})+1}{\\sum_{w\\in V} C(w|w_1^{n-1})+|V|}$$\n","\n","* Backoff: Aproxima la probabilidad de un N-grama no encontrado como la probabilidad de un (N-1)-grama similar, por ejemplo si se busca la probabilidad del N-grama:\n","\n","$$el\\;\\;perro\\;\\;corre\\;\\;$$\n","&emsp;&emsp;&ensp;primero intenta buscar dentro del modelo la probabilidad $P(corre|el,perro)$ pero si es cero entonces aproxima la probabilidad como:\n","\n","$$P(corre|el,perro)\\approx P(corre|perro)$$\n","\n","&emsp;&emsp;&ensp;De forma general se expresa como:\n","\n","$$P(w_n|w_1^{n-1})\\approx P(w_n|w_2^{n-1})\\approx \\;... \\approx P(w_n|w_i^{n-1}) \\approx \\; ...\\approx P(w_n)$$\n","\n","&emsp;&emsp;&ensp;Mientras $P(w_n|w_i^{n-1})=0$\n","\n"],"metadata":{"id":"6wzYUfPRgYk-"}},{"cell_type":"code","source":["#@title Creaci√≥n de un modelo de N-gramas\n","#@markdown Selecciona los parametros del modelo de Ngramas a crear\n","\n","tam_ngrama = 10 #@param {type:\"integer\"}\n","reduccion = \"Frecuencia minima\" #@param [\"N palabras\", \"Frecuencia minima\"]\n","suavizado = \"Backoff\" #@param [\"Laplace\", \"Backoff\"]\n","\n","if reduccion == \"N palabras\":\n","  vocabularioParaModelo = vocabularioEsqN\n","elif reduccion == \"Frecuencia minima\":\n","  vocabularioParaModelo = vocabularioEsqMin\n","else:\n","  vocabularioParaModelo = vocabulario_completo\n","\n","# Diccionario que almacena los nGramas construidos\n","modelos_entrenados = modelos_entrenados if \"modelos_entrenados\" in globals() else {}\n","\n","# Entrena los modelos necesarios de nGramas\n","modelo_nuevo = Ngrama(tam_ngrama,vocabularioParaModelo,suavizado)\n","modelo_nuevo.entrenar(corpus)\n","modelos_entrenados[tuple([tam_ngrama,reduccion,suavizado])] = modelo_nuevo\n","\n","display(Markdown(f'**10 conteos de {tam_ngrama}-gramas al azar**'))\n","for i in range(10):\n","  res = key, val = random.choice(list(modelo_nuevo.conteos.items()))\n","  print(res[0], \"aparece\",val, \"veces\")\n","\n","display(Markdown(f'**10 probabilidades de {tam_ngrama}-gramas al azar**'))\n","print(modelo_nuevo.probabilidades.sample(10))"],"metadata":{"id":"vwu2o3DYAB8U","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Busqueda de N-gramas dentro del modelo creado\n","N_grama = \"'hay', 'nada', 'que', 'pueda', 'perder', 'que', 'no', 'pueda', 'ser', 'que'\" #@param {type:\"string\"}\n","\n","if N_grama != \"\":\n","  N_grama_lista = N_grama.split(\" \")\n","else:\n","  N_grama_lista = []\n","\n","if \"modelo_nuevo\" in globals():\n","  if len(N_grama_lista) != modelo_nuevo.n:\n","    display(Markdown(f'**ERROR: El N-grama debe ser de tama√±o {modelo_nuevo.n} no de {len(N_grama_lista)}**'))\n","  else:\n","    display(Markdown(f'**N-grama transformado:**'))\n","    print(modelo_nuevo.transformar_oracion(N_grama_lista))\n","\n","    prob,extras = modelo_nuevo.probabilidad_ngrama(N_grama_lista)\n","\n","    display(Markdown(f'**Probabilidad asignada al N-grama:**'))\n","    print(prob)\n","\n","    display(Markdown(f'**N-grama calculado:**'))\n","    imprimir_prob(extras[-1],prob,extras[:-1])\n","else:\n","  display(Markdown(f'**ERROR: No hay ning√∫n modelo de N-gramas creado**'))"],"metadata":{"cellView":"form","id":"D4wjO63g4XT0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Uso de los modelos de N-gramas"],"metadata":{"id":"zcR7GepVYCWm"}},{"cell_type":"code","source":["#@title Selecci√≥n del modelo a usar\n","\n","lista_modelos = [ key for key in modelos_entrenados]\n","lista_modelos_str = [ str(key) for key in modelos_entrenados]\n","\n","seleccion_modelo = widgets.Dropdown(\n","    options=lista_modelos_str,\n","    value=lista_modelos_str[0],\n","    disabled=False,\n",")\n","\n","display(Markdown(f'**Selecciona el modelo a usar:**'))\n","display(seleccion_modelo)"],"metadata":{"cellView":"form","id":"ZyR195Unvq6d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Generaci√≥n de oraciones aleatorias\n","#@markdown Genera una oraci√≥n al azar de longitud menor o igual a M usando las probabilidades calculadas en el modelo de N-gramas\n","\n","long_maxima = 10 #@param {type:\"integer\"}\n","reemplazar_UNK = False #@param {type:\"boolean\"}\n","mostrar_probabilidades = False #@param {type:\"boolean\"}\n","\n","modelo = modelos_entrenados[lista_modelos[seleccion_modelo.index]]\n","\n","oracion,extras = modelo.generar_oracion(long_maxima,reemplazar_UNK)\n","\n","display(Markdown(f'**Oraci√≥n generada al azar usando el modelo de N-gramas entrenado:**'))\n","for elemento in oracion:\n","  print(elemento,end=\" \")\n","print()\n","\n","if(mostrar_probabilidades):\n","  for i in range(len(extras)):\n","    imprimir_prob(oracion[i],extras[i][0],extras[i][1])"],"metadata":{"id":"I5R9cFR0YFi6","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Obtenci√≥n de palabras siguientes dado un contexto\n","#@markdown Da una palabra al azar que puede aparecer dado un contexto\n","contexto = \"casi nunca dice que yo estoy enamorado de \" #@param {type:\"string\"}\n","\n","if contexto != \"\":\n","  oracion_contexto = [\"<s>\"]*(modelo.n-1) + contexto.split(\" \")\n","else:\n","  oracion_contexto = [\"<s>\"]*(modelo.n-1)\n","\n","modelo = modelos_entrenados[lista_modelos[seleccion_modelo.index]]\n","\n","display(Markdown(f'**Oraci√≥n transformada:**'))\n","print(modelo.transformar_oracion(oracion_contexto[modelo.n-1:]))\n","\n","sig_palabra,prob,contexto_usado = modelo.siguiente_palabra(oracion_contexto[len(oracion_contexto)-modelo.n+1:])\n","\n","display(Markdown(f'**Siguiente palabra escogida al azar con su respectiva probabilidad:**'))\n","print(sig_palabra)\n","imprimir_prob(sig_palabra,prob,contexto_usado)"],"metadata":{"cellView":"form","id":"l5vI476cgIv5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Probabilidad asignada a una oraci√≥n\n","#@markdown Da la probabilidad que el modelo le asigna a la oraci√≥n dada\n","oracion = \"casi nunca nadie dice que yo estoy enamorado\" #@param {type:\"string\"}\n","mostrar_ngramas = False #@param {type:\"boolean\"}\n","\n","if oracion != \"\":\n","  oracion_lista = oracion.split(\" \")\n","else:\n","  oracion_lista = []\n","\n","modelo = modelos_entrenados[lista_modelos[seleccion_modelo.index]]\n","\n","display(Markdown(f'**Oraci√≥n transformada:**'))\n","print(modelo.transformar_oracion(oracion_lista))\n","\n","prob,extras,_ = modelo.probabilidad_oracion(oracion_lista)\n","\n","display(Markdown(f'**Probabilidad asignada a la oraci√≥n:**'))\n","print(prob)\n","\n","if(mostrar_ngramas):\n","  display(Markdown(f'**N-gramas calculados:**'))\n","  for i in range(len(extras)):\n","    imprimir_prob(extras[i][1][-1],extras[i][0],extras[i][1][:-1])"],"metadata":{"cellView":"form","id":"sTDTpm1QgimA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Comparaci√≥n de los modelos de N-gramas"],"metadata":{"id":"bUNLWwZ_w29F"}},{"cell_type":"code","source":["#@title Probabilidad asignada a una oraci√≥n\n","#@markdown Compara la probabilidad que le asignan los modelos entrenados sobre un mismo vocabulario a la oraci√≥n dada\n","oracion = \"casi nunca nadie dice que yo estoy enamorado\" #@param {type:\"string\"}\n","reduccion = \"Frecuencia minima\" #@param [\"N palabras\", \"Frecuencia minima\"]\n","log_probabilidad = False #@param {type:\"boolean\"}\n","mostrar_ngramas = False #@param {type:\"boolean\"}\n","\n","espacio = \"\\t\\t\\t\" if reduccion == \"Frecuencia minima\" else \"\\t\\t\"\n","\n","if oracion != \"\":\n","  oracion_lista = oracion.split(\" \")\n","else:\n","  oracion_lista = []\n","\n","modelos = [(key,modelos_entrenados[key]) for key in modelos_entrenados if key[1] == reduccion]\n","modelos.sort(key=lambda x: x[0][2])\n","if len(modelos) == 0:\n","  display(Markdown(f'**ERROR:** No hay modelos creados bajo ese vocabulario'))\n","else:\n","  probabilidades = []\n","  list_extras = []\n","  for key,modelo in modelos:\n","    prob,extras,_ = modelo.probabilidad_oracion(oracion_lista,log_probabilidad)\n","    probabilidades.append(prob)\n","    list_extras.append(extras)\n","\n","  display(Markdown(f'**Oraci√≥n transformada:**'))\n","  print(modelo.transformar_oracion(oracion_lista))\n","\n","  display(Markdown(f'**Probabilidades por cada modelo**'))\n","  print(\"Modelo:        \",end=\"\")\n","  for key,modelo in modelos:\n","    print(key,end=\"\\t\")\n","  if log_probabilidad:\n","    print(\"\\nLog(Probabilidad):      \",end=\"\")\n","  else:\n","    print(\"\\nProbabilidad:      \",end=\"\")\n","  for i in range(len(probabilidades)):\n","    print(probabilidades[i],end=espacio)\n","\n","  text_prob = \"Log(Probabilidad)\" if log_probabilidad else \"Probabilidad\"\n","\n","  display(Markdown(f'**Mapa de calor de las probabilidades por cada modelo**'))\n","  df = pd.DataFrame({\"Modelo\": [(key[0],key[2]) for key,modelo in modelos],\n","                   text_prob: probabilidades})\n","  df.set_index(\"Modelo\", inplace=True)\n","  ax = sns.heatmap(df, annot=True, fmt=\"g\", cmap='Reds',vmin=min(probabilidades), vmax=max(probabilidades))\n","  plt.show()\n","\n","  if mostrar_ngramas:\n","    for i in range(len(modelos)):\n","      display(Markdown(f'**N-gramas calculados del modelo {modelos[i][0]}**'))\n","      extras = list_extras[i]\n","      for i in range(len(extras)):\n","        imprimir_prob(extras[i][1][-1],extras[i][0],extras[i][1][:-1])\n",""],"metadata":{"cellView":"form","id":"OoeLj58J-OxD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Bibliograf√≠a\n","\n","* Jurafsky, D. (2018). *Speech and Language Processing: An Introduction to Natural Language Processing,\n","Computational Linguistics, and Speech Recognition*.\n","* Manning, C. (2000). *Foundations of Statistical Natural Language Processing*. London, England: The MIT Press."],"metadata":{"id":"mKA2H4BCUw09"}}],"metadata":{"colab":{"provenance":[],"collapsed_sections":["4MPsdzv-zCGk","xi9xTbGRzJhI","7Q0V27bif3V6","zcR7GepVYCWm","bUNLWwZ_w29F"]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"}},"nbformat":4,"nbformat_minor":0}