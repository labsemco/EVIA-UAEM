{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/jhermosillo/Escuela_CD_IMATE_2019/blob/master/Wiki_PCA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><center>\n",
    "    \n",
    "## **Modelado de texto usando técnicas de reducción de dimensionalidad.**\n",
    "### Aplicación en WikiPedia para medir semejanza entre documentos.\n",
    "    \n",
    "</center></h3>\n",
    "<h5><center>\n",
    "    Dr. Jorge Hermosillo Valadez<br>\n",
    "    Centro de Investigación en Ciencias<br>\n",
    "    Universidad Autónoma del Estado de Morelos<br>\n",
    "</center></h5>\n",
    "<h1><center>\n",
    "<img src=\"img/CINC_TRANSP.png\" width=\"100\"/>\n",
    "<img src=\"img/UAEM_COLOR_2.png\" width=\"100\"/>\n",
    "</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descubrir temas es útil para diversos fines, como agrupar documentos, organizar contenido disponible en línea para recuperar información y hacer recomendaciones. El modelado de temas es una técnica de minería de texto que proporciona métodos para identificar palabras clave concurrentes, con el fin de resumir grandes colecciones de información textual. Ayuda a descubrir temas ocultos en el documento, anotar los documentos con estos temas y organizar una gran cantidad de datos no estructurados. Numerosos proveedores de contenido y agencias de noticias están utilizando modelos de temas para recomendar artículos a los lectores. \n",
    "\n",
    "Utilizaremos dos técnicas de reducción de dimensionalidad, PCA (Principal Component Analysis) y LSA (Latent Semantic Analysis), con el propósito de modelar documentos y establecer semejanzas entre ellos. \n",
    "\n",
    "Ambas técnicas utilizan el modelo de bolsa de palabras (BoW -- Bag of words), que da como resultado una matriz documento-término que representa documentos en función del conteo de términos. Como veremos, PCA y LSA guardan una estrecha relación. La diferencia básica es que el primero hace un pre-procesamiento de los datos mediante el centrado de las observaciones.\n",
    "\n",
    "En este curso veremos cómo:\n",
    "* Leer documentos de la wikipedia (raw)\n",
    "* Construir una matriz BoW sobre de ellos\n",
    "* Aplicar PCA y LSA sobre esta matriz\n",
    "* Comparar el desempeño de ambos métodos analizando la semejanza entre documentos\n",
    "\n",
    "En términos generales, el proceso que vamos a seguir es lo siguiente:\n",
    "\n",
    "![Proceso](img/BOW.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Módulos necesarios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Sólo para COLAB**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"\"\"\n",
    "!apt-get install subversion\n",
    "!svn checkout \"https://github.com/jhermosillo/Escuela_CD_IMATE_2019/trunk/datos/\"\n",
    "!svn checkout \"https://github.com/jhermosillo/Escuela_CD_IMATE_2019/trunk/modelos/\"\n",
    "#\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para traer archivos al entorno de COLAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"\"\"\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials\n",
    "\n",
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "file_drive = GoogleDrive(gauth)\n",
    "#\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La siguiente instrucción requiere el vínculo al archivo desde DRIVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wi = file_drive.CreateFile({'id':'1sV6vK0CLUXpH1VInmtBUBnVACMm5fPFB'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wi.GetContentFile('wiki.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Jorge\n",
      "[nltk_data]     Hermosillo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import wiki as wi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Jorge\n",
      "[nltk_data]     Hermosillo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para ahorrar tiempo..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wi.mas_RAM_porfavor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Cargado de archivo Wikipedia**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dato informativo: para este curso primero descargamos los archivos raw de wikipedia (https://www.cs.upc.edu/~nlp/wikicorpus/). Estos archivos son tipo texto.\n",
    "\n",
    "**Para propósitos de este curso, solo usamos un par de archivos cada uno con varios miles de documentos.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./datos/textosWiki_1']\n"
     ]
    }
   ],
   "source": [
    "archivos = glob.glob('./datos/textosWiki_1')\n",
    "print(archivos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leemos los archivos descargados y sus nombres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "leyendo...\n",
      "./datos/textosWiki_1\n",
      "tamaño del contenido de archivos cargados:             12 MB\n"
     ]
    }
   ],
   "source": [
    "file,nombres = wi.carga_datos(archivos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verificamos algunos datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<doc', 'id=\"1871762\"', 'title=\"jud', 'buechler\"', 'nonfiltered=\"1\"', 'processed=\"1\"', 'dbindex=\"435001\">', 'judson', 'donald', 'buechler', '(nacido', 'el', '19', 'de', 'junio', 'de', '1968', 'en', 'san', 'diego'] ['sostiene', 'que,', 'para', 'todas', 'las', 'personas,', 'ese', 'criterio', 'imparcial', 'se', 'aplica', 'en', 'forma', 'homogénea.', 'véase', 'también', '.', 'objetividad;', 'endofarticle.', '</doc>']\n"
     ]
    }
   ],
   "source": [
    "print(file[0][:20],file[0][-20:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Extracción de documentos**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vamos a identificar cada documento y elaborar una lista de los mismos.\n",
    "Para ello, se debe tener en cuenta la forma en que se indexan listas y arreglos en Python.\n",
    "\n",
    "Como hemos visto, los archivos de Wikipedia traen el identificador de documento ```id=``` (p.ej. _id=\"1842224\"_) y marcadores de inicio y fin de documento, que se reconocen por el caracter ```\">\"``` (p.ej. _dbindex=\"430000\">_ y ```</doc>```). \n",
    "\n",
    "__Lo que vamos a hacer es obtener los indices en donde se encuentran estos datos para extraer el id de documento y su contenido textual__. Esto con el fin de construir una lista de documentos.\n",
    "\n",
    "Los textos se limpian y procesan usando el módulo ```nltk``` (Natural Language ToolKit) (Loper and Bird, 2002). \n",
    "\n",
    "Al final tendremos una lista cuyo contenido será como sigue:\n",
    "\n",
    "```\n",
    "docs->[[id_doc1,texto del documento doc1],[id_doc2,texto del documento doc2], ... ]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Mi unidad\\LABORATORIO DE SEMANTICA COMPUTACIONAL\\CURSOS\\Escuela_CD_IMATE_2019\\wiki.py:69: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  r=r.cadena.str.translate(\\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "archivo ./datos/textosWiki_1 contiene 4753        documentos \n",
      "\n"
     ]
    }
   ],
   "source": [
    "docs = wi.lee_documentos(file,nombres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se leyeron 1 archivos\n",
      "1871762 judson donald buechler nacido junio san diego california california unidos jugador profesional ameri\n"
     ]
    }
   ],
   "source": [
    "print('Se leyeron {} archivos'.format(len(docs)))\n",
    "print(docs[0][0][0],docs[0][0][1][:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data Frame de documentos**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vamos a usar pandas (McKinney & others, 2010) para crear un DataFrame que es una estructura de datos especialmente diseñada para manipular grandes cantidades de datos de manera ágil y eficiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4753 documentos clase 0\n",
      "(4753, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>Texto</th>\n",
       "      <th>clase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1871762</td>\n",
       "      <td>judson donald buechler nacido junio san diego ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1871768</td>\n",
       "      <td>lost highway the concert dvd recoge concierto ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1871769</td>\n",
       "      <td>eburones tribu descendencia germánica habitaro...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1871771</td>\n",
       "      <td>aguada baixo portuguesa águeda km² área habita...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1871772</td>\n",
       "      <td>selge griego importante ciudad pisidia ladera ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    doc_id                                              Texto  clase\n",
       "0  1871762  judson donald buechler nacido junio san diego ...      0\n",
       "1  1871768  lost highway the concert dvd recoge concierto ...      0\n",
       "2  1871769  eburones tribu descendencia germánica habitaro...      0\n",
       "3  1871771  aguada baixo portuguesa águeda km² área habita...      0\n",
       "4  1871772  selge griego importante ciudad pisidia ladera ...      0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_0 = pd.DataFrame(docs[0],columns = ['doc_id','Texto','clase'])\n",
    "#df_1 = pd.DataFrame(docs[1],columns = ['doc_id','Texto','clase'])\n",
    "print(len(df_0.index),'documentos clase 0')\n",
    "#print(len(df_1.index),'documentos clase 1')\n",
    "df = df_0\n",
    "#df = pd.concat([df_0, df_1], ignore_index=True, sort=False)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Extracción de características**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tratemos de visualizar algunas propiedades de los documentos.\n",
    "\n",
    "Para ello vamos a utilizar un contador ([Counter](https://docs.python.org/2/library/collections.html)). Un contador es un contenedor que almacena elementos como claves de diccionario, y sus recuentos se almacenan como valores de diccionario.\n",
    "\n",
    "Construiremos una columna con el conteo de palabras por documento y otra con la palabra más frecuente en el documento.\n",
    "\n",
    "Para visualizar estos datos utilizaremos [matplotlib](https://matplotlib.org/) (Hunter, 2007).\n",
    "\n",
    "John D. Hunter. Matplotlib: A 2D Graphics Environment, _Computing in Science & Engineering, 9, 90-95 (2007)_, DOI:10.1109/MCSE.2007.55 (publisher link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>Texto</th>\n",
       "      <th>clase</th>\n",
       "      <th>Palabras</th>\n",
       "      <th>Total</th>\n",
       "      <th>Conteos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1889137</td>\n",
       "      <td>invasiones japonesas corea conflicto bélico de...</td>\n",
       "      <td>0</td>\n",
       "      <td>[invasiones, japonesas, corea, conflicto, béli...</td>\n",
       "      <td>7549</td>\n",
       "      <td>{'invasiones': 6, 'japonesas': 31, 'corea': 69...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1889938</td>\n",
       "      <td>cartapuebla oviedo documento concedido ciudad ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[cartapuebla, oviedo, documento, concedido, ci...</td>\n",
       "      <td>5280</td>\n",
       "      <td>{'cartapuebla': 1, 'oviedo': 26, 'documento': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1881683</td>\n",
       "      <td>historia sal trata uso comercio dado siglos ún...</td>\n",
       "      <td>0</td>\n",
       "      <td>[historia, sal, trata, uso, comercio, dado, si...</td>\n",
       "      <td>4047</td>\n",
       "      <td>{'historia': 7, 'sal': 228, 'trata': 1, 'uso':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1891268</td>\n",
       "      <td>movimiento homófilo segundo movimiento homosex...</td>\n",
       "      <td>0</td>\n",
       "      <td>[movimiento, homófilo, segundo, movimiento, ho...</td>\n",
       "      <td>3738</td>\n",
       "      <td>{'movimiento': 26, 'homófilo': 14, 'segundo': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1892060</td>\n",
       "      <td>sarah trimmer enero diciembre escritora crític...</td>\n",
       "      <td>0</td>\n",
       "      <td>[sarah, trimmer, enero, diciembre, escritora, ...</td>\n",
       "      <td>3694</td>\n",
       "      <td>{'sarah': 16, 'trimmer': 115, 'enero': 2, 'dic...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    doc_id                                              Texto  clase  \\\n",
       "0  1889137  invasiones japonesas corea conflicto bélico de...      0   \n",
       "1  1889938  cartapuebla oviedo documento concedido ciudad ...      0   \n",
       "2  1881683  historia sal trata uso comercio dado siglos ún...      0   \n",
       "3  1891268  movimiento homófilo segundo movimiento homosex...      0   \n",
       "4  1892060  sarah trimmer enero diciembre escritora crític...      0   \n",
       "\n",
       "                                            Palabras  Total  \\\n",
       "0  [invasiones, japonesas, corea, conflicto, béli...   7549   \n",
       "1  [cartapuebla, oviedo, documento, concedido, ci...   5280   \n",
       "2  [historia, sal, trata, uso, comercio, dado, si...   4047   \n",
       "3  [movimiento, homófilo, segundo, movimiento, ho...   3738   \n",
       "4  [sarah, trimmer, enero, diciembre, escritora, ...   3694   \n",
       "\n",
       "                                             Conteos  \n",
       "0  {'invasiones': 6, 'japonesas': 31, 'corea': 69...  \n",
       "1  {'cartapuebla': 1, 'oviedo': 26, 'documento': ...  \n",
       "2  {'historia': 7, 'sal': 228, 'trata': 1, 'uso':...  \n",
       "3  {'movimiento': 26, 'homófilo': 14, 'segundo': ...  \n",
       "4  {'sarah': 16, 'trimmer': 115, 'enero': 2, 'dic...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter  #regresa un diccionario con conteos\n",
    "\n",
    "df['Palabras']=df['Texto'].apply(lambda x: x.split())\n",
    "df['Total']=df['Palabras'].apply(lambda x: len(x))\n",
    "df['Conteos']=df['Palabras'].apply(lambda x: Counter(x))\n",
    "\n",
    "df=df.sort_values(by=\"Total\",ascending=False)\n",
    "\n",
    "df.index = range(len(df.index))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Reducción del tamaño de las matrices**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para reducir la complejidad espacial de nuestro ejercicio, podemos hacer dos cosas:\n",
    "* 1.- Un muestreo aleatorio de documentos, lo que nos ayudaría a reducir el vocabulario. \n",
    "* 2.- Un recorte en el número de documentos por la cantidad de palabras.\n",
    "Usaremos el segundo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtenemos el vocabulario.\n",
    "Para ello vamos a usar el método de tokenización de NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103133 palabras únicas (tipos)\n"
     ]
    }
   ],
   "source": [
    "textos = df['Texto'].values\n",
    "textos = \" \".join(textos)\n",
    "vocabulario = set(word_tokenize(textos))\n",
    "print(len(vocabulario),'palabras únicas (tipos)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtramos algunos documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2117\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>Texto</th>\n",
       "      <th>clase</th>\n",
       "      <th>Palabras</th>\n",
       "      <th>Total</th>\n",
       "      <th>Conteos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1886236</td>\n",
       "      <td>taifa valencia taifa balansiya reinos taifas c...</td>\n",
       "      <td>0</td>\n",
       "      <td>[taifa, valencia, taifa, balansiya, reinos, ta...</td>\n",
       "      <td>1949</td>\n",
       "      <td>{'taifa': 25, 'valencia': 61, 'balansiya': 4, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1878783</td>\n",
       "      <td>deva victrix simplemente deva ciudadfortaleza ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[deva, victrix, simplemente, deva, ciudadforta...</td>\n",
       "      <td>1893</td>\n",
       "      <td>{'deva': 19, 'victrix': 16, 'simplemente': 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1882951</td>\n",
       "      <td>animax latinoamérica complejo tres canales cab...</td>\n",
       "      <td>0</td>\n",
       "      <td>[animax, latinoamérica, complejo, tres, canale...</td>\n",
       "      <td>1847</td>\n",
       "      <td>{'animax': 60, 'latinoamérica': 10, 'complejo'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1888749</td>\n",
       "      <td>serie fílmica superman lista largometrajes per...</td>\n",
       "      <td>0</td>\n",
       "      <td>[serie, fílmica, superman, lista, largometraje...</td>\n",
       "      <td>1831</td>\n",
       "      <td>{'serie': 11, 'fílmica': 2, 'superman': 75, 'l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1880333</td>\n",
       "      <td>rock rolinga llamado rock chabón rock stone ro...</td>\n",
       "      <td>0</td>\n",
       "      <td>[rock, rolinga, llamado, rock, chabón, rock, s...</td>\n",
       "      <td>1829</td>\n",
       "      <td>{'rock': 67, 'rolinga': 19, 'llamado': 2, 'cha...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    doc_id                                              Texto  clase  \\\n",
       "0  1886236  taifa valencia taifa balansiya reinos taifas c...      0   \n",
       "1  1878783  deva victrix simplemente deva ciudadfortaleza ...      0   \n",
       "2  1882951  animax latinoamérica complejo tres canales cab...      0   \n",
       "3  1888749  serie fílmica superman lista largometrajes per...      0   \n",
       "4  1880333  rock rolinga llamado rock chabón rock stone ro...      0   \n",
       "\n",
       "                                            Palabras  Total  \\\n",
       "0  [taifa, valencia, taifa, balansiya, reinos, ta...   1949   \n",
       "1  [deva, victrix, simplemente, deva, ciudadforta...   1893   \n",
       "2  [animax, latinoamérica, complejo, tres, canale...   1847   \n",
       "3  [serie, fílmica, superman, lista, largometraje...   1831   \n",
       "4  [rock, rolinga, llamado, rock, chabón, rock, s...   1829   \n",
       "\n",
       "                                             Conteos  \n",
       "0  {'taifa': 25, 'valencia': 61, 'balansiya': 4, ...  \n",
       "1  {'deva': 19, 'victrix': 16, 'simplemente': 1, ...  \n",
       "2  {'animax': 60, 'latinoamérica': 10, 'complejo'...  \n",
       "3  {'serie': 11, 'fílmica': 2, 'superman': 75, 'l...  \n",
       "4  {'rock': 67, 'rolinga': 19, 'llamado': 2, 'cha...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=df[(df.Total < 2000) & (df.Total > 100)]\n",
    "print(len(df))\n",
    "df.index = range(len(df.index))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guardamos el data frame\n",
    "\n",
    "Para ello, vamos a usar [pickle](https://docs.python.org/2/library/pickle.html), que forma parte de las [funcionalidades de I/O](https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html) de Pandas.\n",
    "\n",
    "Pickle permite serializar y deserializar una estructura de datos Python. \"_Pickling_\" es el proceso mediante el cual una jerarquía de objetos de Python se convierte en una secuencia de bytes, y \"_Depickling_\" es la operación inversa, mediante la cual una secuencia de bytes se convierte nuevamente en una jerarquía de objetos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'animax latinoamérica complejo tres canales cable lanzado julio señales latinoamérica brasil reemplazando canal locomotion igual filiales asia inició proyecto canal transmite únicamente series anime aunque recientemente confirmó emitira películas distribuidas sony pictures mayo estudios animax latinoamérica estan ubicados caracas venezuela igual estudios canales hermanos sony entertainment television axn pertenece sony pictures television international estructura señales complejo cuenta feeds lugares transmisión masivos cuales transmite programación señal venezuela señal brasil señal señal genérica disponible resto países latinoamérica historia siendo primer intento sony brindar canal exclusivo anime horas latinoamérica pensó poner series formatos series contuvieran capítulos transmitidas días mientras aquellas cantidad menor emitirían ciertos días semana mismo modo transmitidas japón total puede encontrar solo día capítulo estreno cualquier serie mínimo repeticiones final cada serie presenta sección llamada animedia presentan videoclips artistas estadounidenses latinos datos curiosos nerdcore resúmenes eventos relacionados presentaciones futuras series canal anteriormente animedia transmitia videos artistas japoneses inicios cuentan segmento llamado animax nius niusnews presenta noticias relacionadas anime temas éste segmento desapareció aire luego cambio imagen agosto agosto animax estrenó nueva página web nuevo diseño contenido motivo cumplimiento dos años existencia latinoamérica además septiembre renovó vez programación estrenos trinity blood basilisk speed grapher último destinado nueva franja jueves bajo nombre lollipop incluyen series fanservice mayo estrenó primer episodio bleach gracias negociaciones viz mediapero doblaje mexicano animax emitió latinoamérica episodio serie excel saga censurado japón continua violencia obscenidad embargo animax pasó episodio canal ningún tipo censura animax comunicado explícitamente compromiso nocensura emisión todas producciones forma totalmente íntegra ningún tipo cortes ediciones marzo estrenó primer animé coreano legend blue además animax confirmó mayo haría reestreno neon genesis evangelion basado versión renewal evangelion totalmente remasterizada escenas inéditas lanzada japón serie animax doblaje mexicano confirmó estreno bleach jigoku shoujo conocida aquí hell girl primera segunda temporada además nuevo bloque llamado reciclo emitirán películas estadounidenses distribuidas columbia pictures filial sony novedades distraction nuevo programa concursos origen británico además uso subtítulos programación mayo adelante sido confirmados siguientes estrenos señal latinoamericana baby baachan black cat bleach bokurano death note fatestay night hell girl primera segunda temporada humanoid monster bem mushishi renewal evangelion rod the rurouni kenshin samurai solty rei xxxholic cuanto supuesto estreno anime hentai immoral sisters sido totalmente desmentida transmisión propios ejecutivos animax través revista lazer editorial ivrea difundido noticia números pasados animax jamás consiguió derechos serie error empresa doblaje hizo confundir cientos páginas internet dedicadas animé noticias estrenos animax emisión ovas pasó ser sólo falso rumor mayo estrenaron bleach hell girl neon genesis evangelion nuevo canal extraña transmisión películas live action licenciadas sony pictures entertainment distraction ver animé despertado multitud críticas parte televidentes comenzado realizar fuertes campañas eliminar tipo programación grilla brevedad posible similar ocurrido ova immoral sisters ocurrió kiddy grade anime nunca adquirido animax latinoamérica confundido distribuidora doblaje anime solty rei cuál adquirido mismo género partir mayo animax cambió totalmente imagen canal primera vez latinoamérica además estrenos mayo volvieron programación last exile super milk chan show comerciales comenzó pasar avances episodios nuevos verían semana siguiente bleach hell girl evangelion junio estreno humanoid monster bem sustituyendo anime hungry heart julio empezaron transmitirse nuevos episodios prince tennis capítulo último une emisión nuevos episodios crayon shinchan julio anunciado estreno segundo semestre últimas ovas hunter hunter greed island greed island final estrenándose éstos agosto finales septiembre comienzos octubre estrenaron siguientes series rurouni kenshin samurai así rod the estrenaron día septiembre solty rei octubre dia octubre blog oficial animax confirmo transmisión serie losta partir lunes octubre remplazando así series heat guy saikano llevaban dos años aire initerrumpidamente dar horario rod the conjunto solty rei conlleva reacomodo horarios siendo the twelve kingdoms getbackers pasaran horario diurno mientras fullmetal alchemist blood sufrirán hora retraso emisión embargo ninguna mencionadas eliminara parrilla programación ultima noticia explica dada inminente llegada mes noviembre series ×××holic dos lollipop mushishi octubre animax estrenó serie live action axn lost generó nuevas quejas canal proyección animax países latinoamericanos partir julio animax inició transmisión remplazando locomotion llegada países latinoamérica inició simultáneamente llegando cada forma diferente argentina animax inició transmisión dia julio reemplazando locomotion transmite canal gigared canal directv canal telecentro digital canal cablevisión multicanal digital canal antina canal telered digital perú señal animax inició transmisión perú operador cable llamado starglobalcom arequipa tacna transmisión animax produjo mismo dia estreno señal mes julio mes septiembre dos meses después primera transmisión julio mismo año señal transmitida cánal empresa privada cable cable mágico forma parte grupo telefónica además distribuidora nacional difusora satelital directv transmite través canal compañía telmex transmite television satelital colombia animax apareciera presentación series anime disminuyendo televisión colombia tipo programas algún momento alcanzado gran acogida varias series nunca visto anterioridad buen número programas anime vistos canales peruanos extranjeros comunidades colombianas podían ver animax presente operadores cable aunque ninguno pedido exclusividad cablecentro operador parrilla canales debido existencia contenido violento tener cuenta canal dirigido público adolescente adulto jóven operadores colombianos animax parrilla cableunion operador grande cubre varias regiones colombia lado ciudades barranquilla empresa cable teledinámica ofrece parrilla medellín animax siendo trasmitido conformación terminación locomotion interrumpir transmición siendo emitido une epm television diciembre animax dejó transmitirse servicio normal cable operador afiliado superview ambos controlados empresa mexicana telmex permaneciendo dentro servicio smart primero canal transmite supercable directv telefonica telecom debido decisión fans animax colombia mostrado inconformes molestos éstas mismas personas enviado cartas correos empresa telmex volver incluir animax grilla programación normal bogotá ciudades país incluso television digital bogotá animax disponible costa rica inicia transmisión julio reemplazando locomotion actualmente solo sky super cable cablevisión transmite señal costa rica espera empresas televisión paga adquieran derechos momento grandes empresas amnet cabletica negado incluirlo parte programación canal dirigido audiencia adolescente adulta aunque parece incursión animax gran avance sociedad otaku país aún sido aceptada gran parte fanáticos doblaje español casos consideran baja calidad dentro grandes espectativas generado canal encuentra transmisión anime talla fullmetal alchemist blood initial hellsing etc actualmente generado mucha espectativa adquisición derechos anime bleach estrenarse canal venezuela grupos operadores cable regionales nacionales implementaron canal animax ampliar proyección país operadores netuno directv sólo embargo todavía venezuela existen compañías cable animax parrilla canales premium ejemplo intercable últimos meses mostrado canal parrilla normal canal sólo unas semanas parte promoción atraer clientes contratar servicio caso decodificador caso debilitado relación animax venezuela país mercado televisión cable divide casi mitad entera directv cuarto supercable incluido falta directv completar dicha mitad cuarto intercable contando empresas pequeñas televisión cable ejemplo sanare totalcable compañía intercable enviado cartas diferentes comunidades animé pedir inclusión animax parrilla normal debido gran comunidad fans anime existente país chile país proyección animax hizo manera paulatina comenzando transmisión julio directv canal luego junio animax integró programación telefónica digital canal obtenerlo contratar pack canales temáticos llamado cine ofrece empresa junto canales cinemax cinecanal etc después diciembre mismo año canal llega servicio televisión cable empresa gtd manquehue canal cobertura solo región metropolitana santiago chile vtr mayor compañía cableoperadora país incoporó animax noviembre servicio digital dbox canal satisfaciendo manera insistentes pedidos clientes año compañía telefónica sur integró canal servicio televisión digital witv canal punta arenas parte intentado compañía cable tvred integre animax parrilla programática obtenido resultados negativos todas ocasiones cable local ciudad melipilla melivisión encuentra parrilla programática inicios animax méxico méxico comienza transmitirse julio mismo día inicia transmisiones toda latinoamerica reemplazando locomotion méxico encuentra disponible cablevisión cablecom canal cablemas zona centro republica resto país encuentra sistemas cable afiliados pctv sky jóvenes toda república pedido operadores televisión cable incluyan animax barra canales sólo podido recursos poder transmitirlo abril sony pictures televisión internacional lanzó oficialmente animax mobile exclusivo méxico traves red iusacell servicio además incluir tonos imágenes podrá ver señal especial canal teléfonos celulares cuarto animax mobile importante después lanzar servicio televisión móvil japón partir abril australia junio canada junio ecuador ecuador julio comienza transmisión animax mismo día inicia transmisiones toda latinoamerica reemplazando locomotion medio directv país empresa grande televisión pagada país cable incluyó animax paquete canales digitales bolivia bolivia animax comenzó transmitir junio través sistema televisión cable cotas cabletv canal paz transmitía sistema cable supercanal señal locomotion así principio parte sistema cable cotel introdujo año actualmente trasmiten canal junio sistema cotel trasmitiendo canal ciudad oruro empezó transmitirse coteorcable partir actualmente encuentra canal guatemala país canal transmitido través muchas empresas cable telecom claro mayoría empresas departamento guatemala embargo encuentra franja infantil mayoría considerar incluye fundamentalmente series jóvenes adultos transmitido través directv claro animax encontraba canal pedido padres contenido pasaban animax siempre publico sección lollipop series mostrar violencia escenas involucran sexo canal cambiado actualmente encuentra transmisión canal nicaragua animax comenzó transmitirse nicaragua enero señal cable estesa canal reemplazando canal film arts luego cambió ubicación canal panamá animax inició transmisión día julio reemplazando locomotion señal directv embrago diciembre directv dejó extrañamente transmitir señal animax pais causando enojo masivo clientes listado series críticas mayoría series transmitidas animax américa latina dobladas venezuela desempeño actores rebeca aponte excel excel saga josé manuel vieira edward elric full metal alchemist alucard hellsing maite guedes obtuvo críticas positivas país negativas resto latinoamerica parte fans doblaje general américa latina criticado animax debido comparaciones antecesor locomotion cuanto estilo contenido programación animax retransmitido mayoría series ovas películas anime transmitían existía canal locomotion varias series películas producidas compañías afiliadas sony casos locomotion perdido derechos transmisión material inclusión programa distraction falta ova películas relacionadas anime dando prioridad segmento llamado reciclo errores horarios mostrados señal animax venezuela difunden hora hora establecida canal bloque lollipop transmite según horarios animax transmite hora caracasahora cambio horario media hora despues series super milk chan show mostradas horarios transmisión programación semanal web misma animax animedia artistas aparecido espacio respectivos videos artista video musical seconds mars from yesterday afra digital breath acidman world symphony otsuka frienger angela aki kiss good bye anna tsuchiya rose asian kungfu generation blue train ayumi hamasaki born free biyuden issai gassai bonnie pink wonderful chemistry almost love chirinuruwowaka kasugai coco lee magic words crystal kay koi ochitara daft punk robot rock depapepe summer parade uppercut the attack ninja video experimental infinity under the moon cuarteto hacer conmigo cuarteto yendo casa damián fall out boy the take over the breaks over gackt redemption gorillaz fell good inc gorillaz dirty harry globe anytime smokin cigarette gwen stefani damian marley now that you got hajime chitose kataritsugu koto hifana akero hifana fat bros hifana fresh push breathing high and mighty color ichirin hana tema bleach opening high and mighty color run run run hitomi takahashi aozorano namida tema inicial blood home made kazoku joy ride jolin tsai warriors peace jayz feat linkin park numbencore ken hirai pop star koda kumi candy feat blistah arcenciel ready steady tema full metal alchemist arcenciel new world arcenciel link arcenciel jyoujyoushi arcenciel killing maaya sakamoto honey come maaya sakamoto loop tema final tsubasa reservoir chronicle miranda perfecta miranda prisionero motel dos tres nana kitade kiss kiss oreskaband pinocchio ending naruto porno graffiti bailo jyobairo puffy amiyumi sowelu get over sowelu lets farway stereophonics means nothing mika relax taiyozoku himawari takagi masakatsu girls takagi masakatsu private dreaming video experimental tenjochiki boomerang tommy february lonely gorgeous tema inicial paradise kiss utada hikaru for you utada hikaru passion william got from momma ximena sariñana vidas paralelas younha houki hoshiending bleach yui tokio fuentes consultadas página lamac referencias enlaces externos animax latinoamericaen español animax brasil portugues blog oficial animax animax latinoamerica anime news network'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[2].Texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle('datos/data_frame_4K.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leemos un Data Frame previamente almacenado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4041, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>Texto</th>\n",
       "      <th>clase</th>\n",
       "      <th>Palabras</th>\n",
       "      <th>Total</th>\n",
       "      <th>Conteos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1039186</td>\n",
       "      <td>práctica movimiento scout argentina adopta dif...</td>\n",
       "      <td>1</td>\n",
       "      <td>[práctica, movimiento, scout, argentina, adopt...</td>\n",
       "      <td>1988</td>\n",
       "      <td>{'práctica': 1, 'movimiento': 14, 'scout': 38,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1039418</td>\n",
       "      <td>coalición norte alianza provincias norte confe...</td>\n",
       "      <td>1</td>\n",
       "      <td>[coalición, norte, alianza, provincias, norte,...</td>\n",
       "      <td>1987</td>\n",
       "      <td>{'coalición': 10, 'norte': 15, 'alianza': 1, '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1886236</td>\n",
       "      <td>taifa valencia taifa balansiya reinos taifas c...</td>\n",
       "      <td>0</td>\n",
       "      <td>[taifa, valencia, taifa, balansiya, reinos, ta...</td>\n",
       "      <td>1949</td>\n",
       "      <td>{'taifa': 25, 'valencia': 61, 'balansiya': 4, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1039204</td>\n",
       "      <td>elecciones municipales chile realizaron octubr...</td>\n",
       "      <td>1</td>\n",
       "      <td>[elecciones, municipales, chile, realizaron, o...</td>\n",
       "      <td>1942</td>\n",
       "      <td>{'elecciones': 8, 'municipales': 2, 'chile': 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1038384</td>\n",
       "      <td>vanilla ninja banda originaria estonia compues...</td>\n",
       "      <td>1</td>\n",
       "      <td>[vanilla, ninja, banda, originaria, estonia, c...</td>\n",
       "      <td>1911</td>\n",
       "      <td>{'vanilla': 58, 'ninja': 58, 'banda': 26, 'ori...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    doc_id                                              Texto  clase  \\\n",
       "0  1039186  práctica movimiento scout argentina adopta dif...      1   \n",
       "1  1039418  coalición norte alianza provincias norte confe...      1   \n",
       "2  1886236  taifa valencia taifa balansiya reinos taifas c...      0   \n",
       "3  1039204  elecciones municipales chile realizaron octubr...      1   \n",
       "4  1038384  vanilla ninja banda originaria estonia compues...      1   \n",
       "\n",
       "                                            Palabras  Total  \\\n",
       "0  [práctica, movimiento, scout, argentina, adopt...   1988   \n",
       "1  [coalición, norte, alianza, provincias, norte,...   1987   \n",
       "2  [taifa, valencia, taifa, balansiya, reinos, ta...   1949   \n",
       "3  [elecciones, municipales, chile, realizaron, o...   1942   \n",
       "4  [vanilla, ninja, banda, originaria, estonia, c...   1911   \n",
       "\n",
       "                                             Conteos  \n",
       "0  {'práctica': 1, 'movimiento': 14, 'scout': 38,...  \n",
       "1  {'coalición': 10, 'norte': 15, 'alianza': 1, '...  \n",
       "2  {'taifa': 25, 'valencia': 61, 'balansiya': 4, ...  \n",
       "3  {'elecciones': 8, 'municipales': 2, 'chile': 2...  \n",
       "4  {'vanilla': 58, 'ninja': 58, 'banda': 26, 'ori...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_pickle('datos/data_frame_4K.pkl')\n",
    "df.index = range(len(df.index))\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **BoW**\n",
    "\n",
    "Para obtener la Bolsa de Palabras, vamos a utilizar la columna de Conteos con los conteos de palabras respectivos por cada documento.\n",
    "\n",
    "Esta columna consta de diccionarios que usaremos como entrada para el módulo [DictVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.DictVectorizer.html), que transforma diccionarios en arreglos Numpy.\n",
    "\n",
    "Antes, vamos a definir nuestra función BOW. Esta función recibe una lista de diccionarios y regresa una matriz $X$ de documentos y un diccionario $v$ con el vocabulario asociado a un entero. \n",
    "\n",
    "**Cada fila de $X$ es un documento y cada columna representa una palabra del vocabulario.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/BoW_M.png\" width=\"300\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from collections import Counter, OrderedDict\n",
    "\n",
    "def bow_(docs):\n",
    "    v = DictVectorizer(sparse=False)\n",
    "    X = v.fit_transform(docs)\n",
    "    return X,v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2117, 87310)\n"
     ]
    }
   ],
   "source": [
    "docs = df.Conteos.tolist()\n",
    "X,vocab_ = bow_(docs)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('X:')\n",
    "print(X[:2])\n",
    "print('La primer fila de la matriz X suma {} conteos que coincide con el primer documento.'.format(int(X[0].sum())))\n",
    "print('La segunda fila de la matriz X suma {} conteos que coincide con el segundo documento.'.format(int(X[1].sum())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Métodos de reducción de dimensionalidad y codificación latente**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El análisis de componentes principales (PCA) y la descomposición de valores singulares (SVD) son enfoques de reducción de dimensionalidad comúnmente utilizados en el análisis de datos exploratorios y el aprendizaje automático. \n",
    "\n",
    "Ambos son métodos clásicos de reducción de dimensionalidad lineal que intentan encontrar combinaciones lineales de características en la matriz de datos de alta dimensión original para construir una representación significativa del conjunto de datos.\n",
    "\n",
    "PCA tiene como objetivo encontrar ejes ortogonales linealmente no correlacionados, que también se conocen como componentes principales (PC) en el espacio dimensional m (espacio de las características &mdash;_features_) para proyectar los puntos de datos en esas PC. La primera PC captura la mayor variación en los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/PCA.gif\" width=\"375\" height=\"375\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las representaciones resultantes de PCA y SVD son similares para algunos datos, ya que PCA y SVD están estrechamente relacionados. Si tenemos:\n",
    "\n",
    "<img src=\"img/PCA_SVD-1.png\" width=\"75\"/> \n",
    "\n",
    "donde, $\\mathbf{x}_i=[x_1,x_2,\\cdots,x_d]$ son muestras observadas de datos $d-$dimensionales, $d\\in \\mathbb{N}$.\n",
    "\n",
    "Los $k$ _ejes principales_ $\\mathbf{w}_j$, $j \\in \\{1,\\cdots,k\\}$, son aquellos ejes ortonormales sobre los cuales la varianza de la proyección es máxima. \n",
    "\n",
    "**¿Por qué la varianza?**:\n",
    "Se puede considerar cualquier conjunto de observaciones como una señal contaminada con ruido. Lo deseable es maximizar la razón señal-a-ruido (SNR):\n",
    "<center>\n",
    "$SNR=\\frac{\\sigma^2_{senal}}{\\sigma^2_{ruido}}$\n",
    "</center>\n",
    "\n",
    "Esto es equivalente a encontrar una transformación que permita proyectar las observaciones sobre ejes que maximicen la varianza de los datos y minimicen la varianza del ruido.\n",
    "\n",
    "Visto de otra forma, se desea encontrar ejes de proyección de los datos en donde haya la **menor** redundancia (i.e. correlación) posible; es decir, ejes en donde la varianza de la señal sea máxima y su correlación con respecto a otros ejes (\"ruido\") sea mínima:\n",
    "\n",
    "<img src=\"img/COV.png\" width=\"350\"/>\n",
    "\n",
    "Se puede demostrar que estos ejes $\\mathbf{w}_j$ están dados por los $k$ vectores propios dominantes (i.e. los asociados con los valores propios más grandes) de la matriz de covarianza de las muestras\n",
    "<center>\n",
    "$\\mathbf{S}=\\sum_{i}(\\mathbf{x}_i-\\bar{\\mathbf{x}})(\\mathbf{x}_i-\\bar{\\mathbf{x}})^{^\\textrm{T}}/N$ \n",
    "</center>\n",
    "donde $\\bar{\\mathbf{x}}$ es la media de las muestras, de tal forma que\n",
    "<center>\n",
    "$\\mathbf{S}\\mathbf{w}_j=\\lambda_j\\mathbf{w}_j$. \n",
    "</center>\n",
    "\n",
    "Escribiendo $\\mathbf{P}=\\mathbf{W}^{^\\textrm{T}}$, las $k$ componentes principales del vector observado $\\mathbf{x}_n$ están dadas por el vector: \n",
    "<center>\n",
    "$\\mathbf{p}_n=\\mathbf{P}\\,(\\mathbf{x}_n-\\bar{\\mathbf{x}})$. \n",
    "</center>\n",
    "\n",
    "De esta forma, las filas de $\\mathbf{P}$ son _las componentes principales_ de $\\mathbf{X}$ y constituyen una nueva base sobre la que se pueden proyectar los vectores $\\mathbf{x}_n$:\n",
    "<center>\n",
    "$\\mathbf{Y}=\\mathbf{P}\\,\\mathbf{X}$. \n",
    "</center>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/PCA_3.png\" width=\"400\"/>\n",
    "<em><center>Proyección de la matriz original en el espacio de componentes principales (Tharwat, 2016)</em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **PCA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(svd_solver='auto')\n",
    "\n",
    "Y_pca = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('datos/Y_pca.npy', Y_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pca = np.load('datos/Y_pca.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypca=wi.get_dataFrame(Y_pca,df)\n",
    "print(ypca.shape)\n",
    "ypca.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_vr=pca.explained_variance_ratio_\n",
    "print(pca_vr[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wi.distribucion_vr(pca_vr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **LSA**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LSA** (Latent Semantic Analysis) utiliza una ponderación de palabras llamada $\\textit{tf}$-$\\textit{idf}$.\n",
    "\n",
    "Tf-idf (del inglés Term frequency – Inverse document frequency), frecuencia de término – frecuencia inversa de documento (o sea, la frecuencia de ocurrencia del término en la colección de documentos), es una medida numérica que expresa cuán relevante es una palabra para un documento en una colección. \n",
    "\n",
    "Esta medida se utiliza a menudo como un factor de ponderación en la recuperación de información y la minería de texto. El valor tf-idf aumenta proporcionalmente al número de veces que una palabra aparece en el documento, pero es compensada por la frecuencia de la palabra en la colección de documentos, lo que permite manejar el hecho de que algunas palabras son generalmente más comunes que otras. \n",
    "\n",
    "$\\textit{tf}\\,(t,d) = \\frac{f(t,d)}{\\max\\{f(t,d):t\\in d\\}}$\n",
    "\n",
    "$\\textit{idf}\\,(t,D) = \\log\\frac{|D|}{1+|\\{d\\in D:t \\in d\\}|}$\n",
    "\n",
    "TF-IDF=$\\textit{tf}\\times \\textit{idf}$\n",
    "\n",
    "LSI utiliza la Descomposición en Valores Singulares (Singular Value Decomposition) o SVD para calcular tres matrices como sigue (Baker, 2005):\n",
    "<center>\n",
    "$\\mathbf{X}_{nd}=\\mathbf{U}_{nn}\\,\\mathbf{D}_{nd}\\,\\mathbf{W}^{^\\textrm{T}}_{dd}$\n",
    "</center>\n",
    "\n",
    "donde $\\mathbf{U}\\,\\mathbf{U}^{^\\textrm{T}}=\\mathbf{I}$, $\\mathbf{V}\\,\\mathbf{V}^{^\\textrm{T}}=\\mathbf{I}$; las columnas de $\\mathbf{U}$ son los vectores propios ortonormales de $\\mathbf{X}\\,\\mathbf{X}^{^\\textrm{T}}$, las columnas de $\\mathbf{W}$ son los vectores propios ortonormales de $\\mathbf{X}^{^\\textrm{T}}\\,\\mathbf{X}$, y $\\mathbf{D}$ es una matriz diagonal que contiene las raíces cuadradas de los vectores propios de $\\mathbf{U}$ o $\\mathbf{W}$ en orden descendiente.\n",
    "\n",
    "**Nota** que PCA puede obtenerse a partir de SVD. Una diferencia importante es que SVD no necesita centrar los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "q=300  #Elegimos usar q componentes\n",
    "svd = TruncatedSVD(n_components=q, n_iter=7, random_state=42)\n",
    "\n",
    "vectorizer = TfidfVectorizer(vocabulary=vocab_.vocabulary_)\n",
    "\n",
    "corpus = df.Texto.tolist()\n",
    "D_tfidf = vectorizer.fit_transform(corpus)\n",
    "print(D_tfidf[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dlsa=svd.fit_transform(D_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dlsa = np.load('datos/dlsa.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4041, 301)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1039186</td>\n",
       "      <td>0.121646</td>\n",
       "      <td>-0.024406</td>\n",
       "      <td>-0.039368</td>\n",
       "      <td>-0.035705</td>\n",
       "      <td>-0.041790</td>\n",
       "      <td>-0.010379</td>\n",
       "      <td>-0.004593</td>\n",
       "      <td>-0.042547</td>\n",
       "      <td>-0.021562</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020695</td>\n",
       "      <td>0.008753</td>\n",
       "      <td>-0.013189</td>\n",
       "      <td>0.022894</td>\n",
       "      <td>-0.008763</td>\n",
       "      <td>-0.013317</td>\n",
       "      <td>-0.004110</td>\n",
       "      <td>-0.006685</td>\n",
       "      <td>-0.016296</td>\n",
       "      <td>0.020286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1039418</td>\n",
       "      <td>0.196638</td>\n",
       "      <td>-0.058735</td>\n",
       "      <td>-0.067956</td>\n",
       "      <td>-0.072488</td>\n",
       "      <td>-0.109628</td>\n",
       "      <td>0.022077</td>\n",
       "      <td>-0.010523</td>\n",
       "      <td>0.085696</td>\n",
       "      <td>0.116796</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.027488</td>\n",
       "      <td>0.009622</td>\n",
       "      <td>0.027392</td>\n",
       "      <td>0.002523</td>\n",
       "      <td>-0.010163</td>\n",
       "      <td>-0.005532</td>\n",
       "      <td>-0.022401</td>\n",
       "      <td>-0.004529</td>\n",
       "      <td>-0.028139</td>\n",
       "      <td>-0.008621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1886236</td>\n",
       "      <td>0.119631</td>\n",
       "      <td>-0.034328</td>\n",
       "      <td>-0.042508</td>\n",
       "      <td>-0.039718</td>\n",
       "      <td>-0.048568</td>\n",
       "      <td>-0.021897</td>\n",
       "      <td>-0.001945</td>\n",
       "      <td>-0.007390</td>\n",
       "      <td>0.024269</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004913</td>\n",
       "      <td>-0.023344</td>\n",
       "      <td>-0.002691</td>\n",
       "      <td>-0.002397</td>\n",
       "      <td>0.011871</td>\n",
       "      <td>0.015983</td>\n",
       "      <td>0.003193</td>\n",
       "      <td>-0.012030</td>\n",
       "      <td>0.007192</td>\n",
       "      <td>0.004401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1039204</td>\n",
       "      <td>0.150461</td>\n",
       "      <td>-0.054271</td>\n",
       "      <td>-0.025108</td>\n",
       "      <td>-0.037852</td>\n",
       "      <td>-0.058971</td>\n",
       "      <td>0.035331</td>\n",
       "      <td>-0.031666</td>\n",
       "      <td>0.034916</td>\n",
       "      <td>-0.018692</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003030</td>\n",
       "      <td>0.038374</td>\n",
       "      <td>-0.008059</td>\n",
       "      <td>0.007696</td>\n",
       "      <td>-0.008421</td>\n",
       "      <td>-0.000248</td>\n",
       "      <td>0.010082</td>\n",
       "      <td>0.011281</td>\n",
       "      <td>-0.005485</td>\n",
       "      <td>-0.015180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1038384</td>\n",
       "      <td>0.118576</td>\n",
       "      <td>-0.060460</td>\n",
       "      <td>-0.039442</td>\n",
       "      <td>0.000945</td>\n",
       "      <td>0.146879</td>\n",
       "      <td>0.024474</td>\n",
       "      <td>0.013319</td>\n",
       "      <td>-0.022460</td>\n",
       "      <td>0.021937</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.025737</td>\n",
       "      <td>-0.023569</td>\n",
       "      <td>-0.020597</td>\n",
       "      <td>-0.069216</td>\n",
       "      <td>0.064265</td>\n",
       "      <td>0.029761</td>\n",
       "      <td>-0.016103</td>\n",
       "      <td>0.037081</td>\n",
       "      <td>0.018603</td>\n",
       "      <td>0.046333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 301 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    doc_id         0         1         2         3         4         5  \\\n",
       "0  1039186  0.121646 -0.024406 -0.039368 -0.035705 -0.041790 -0.010379   \n",
       "1  1039418  0.196638 -0.058735 -0.067956 -0.072488 -0.109628  0.022077   \n",
       "2  1886236  0.119631 -0.034328 -0.042508 -0.039718 -0.048568 -0.021897   \n",
       "3  1039204  0.150461 -0.054271 -0.025108 -0.037852 -0.058971  0.035331   \n",
       "4  1038384  0.118576 -0.060460 -0.039442  0.000945  0.146879  0.024474   \n",
       "\n",
       "          6         7         8  ...       290       291       292       293  \\\n",
       "0 -0.004593 -0.042547 -0.021562  ...  0.020695  0.008753 -0.013189  0.022894   \n",
       "1 -0.010523  0.085696  0.116796  ... -0.027488  0.009622  0.027392  0.002523   \n",
       "2 -0.001945 -0.007390  0.024269  ...  0.004913 -0.023344 -0.002691 -0.002397   \n",
       "3 -0.031666  0.034916 -0.018692  ...  0.003030  0.038374 -0.008059  0.007696   \n",
       "4  0.013319 -0.022460  0.021937  ... -0.025737 -0.023569 -0.020597 -0.069216   \n",
       "\n",
       "        294       295       296       297       298       299  \n",
       "0 -0.008763 -0.013317 -0.004110 -0.006685 -0.016296  0.020286  \n",
       "1 -0.010163 -0.005532 -0.022401 -0.004529 -0.028139 -0.008621  \n",
       "2  0.011871  0.015983  0.003193 -0.012030  0.007192  0.004401  \n",
       "3 -0.008421 -0.000248  0.010082  0.011281 -0.005485 -0.015180  \n",
       "4  0.064265  0.029761 -0.016103  0.037081  0.018603  0.046333  \n",
       "\n",
       "[5 rows x 301 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dlsa=wi.get_dataFrame(dlsa,df)\n",
    "print(dlsa.shape)\n",
    "dlsa.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'svd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-1fcaaca9ea2e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msvd_vr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msvd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplained_variance_ratio_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msvd_vr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'svd' is not defined"
     ]
    }
   ],
   "source": [
    "svd_vr=svd.explained_variance_ratio_\n",
    "print(svd_vr[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wi.distribucion_vr(svd_vr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Ejercicio**\n",
    "\n",
    "Queremos saber que tan bien podemos modelar documentos utilizando estas técnicas de reducción de dimensionalidad.\n",
    "\n",
    "Observa la distribución de los componentes principales de la matriz BoW, calculados por PCA y por LSI. \n",
    "\n",
    "Elige un número $q$ de componentes principales (que llamaremos _representativos_) para PCA y compara la calidad de los documentos más cercanos (semejantes) a los documentos de análisis mostrados aquí abajo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Documentos de análisis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>Texto</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1891029</td>\n",
       "      <td>accidente trenes chatsworth ángeles accidente ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1023628</td>\n",
       "      <td>revanchismo francés revanche revancha término ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1024447</td>\n",
       "      <td>templo siglo xix situado centro villa pola sie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1035967</td>\n",
       "      <td>ilyushin avión pasajeros largo alcance diseñad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1894599</td>\n",
       "      <td>tomás teresa atleta español nacido santona pro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    doc_id                                              Texto\n",
       "0  1891029  accidente trenes chatsworth ángeles accidente ...\n",
       "1  1023628  revanchismo francés revanche revancha término ...\n",
       "2  1024447  templo siglo xix situado centro villa pola sie...\n",
       "3  1035967  ilyushin avión pasajeros largo alcance diseñad...\n",
       "4  1894599  tomás teresa atleta español nacido santona pro..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docus = df[(df['doc_id']=='1023628') |\\\n",
    "           (df['doc_id']=='1024447') |\\\n",
    "           (df['doc_id']=='1035967') |\\\n",
    "           (df['doc_id']=='1891029') |\\\n",
    "           (df['doc_id']=='1894599') ].\\\n",
    "            drop(columns=['Total','Conteos','Palabras','clase'])  \n",
    "docus.index=range(len(docus.index))\n",
    "\n",
    "docus.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análisis usando PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q=300  #elige un numero q de componentes principales\n",
    "pca_rep=wi.get_representativos(ypca,q)\n",
    "print(pca_rep.shape)\n",
    "pca_rep.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelamos los documentos utilizando las componentes principales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edf_pca=wi.modela_documentos_rep(pca_rep)\n",
    "print(edf_pca.shape)\n",
    "edf_pca.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculamos el vecino más cercano a cada uno de los documentos de análisis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=1\n",
    "vecinos_pca=wi.k_vecinos_mas_cercanos(docus,edf_pca,k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por cada documento de análisis, podemos ver qué documento es el más semejante (el vecino más cercano)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vecinos_pca['1891029'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traemos los textos correspondientes y comparamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1=df[df['doc_id']=='1891029'].Texto.values[0][:400]\n",
    "test2=df[df['doc_id']=='1887192'].Texto.values[0][:400]\n",
    "print(test1)\n",
    "print(test2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos repetir el proceso con los demás documentos de análisis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vecinos_pca['1023628'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1=df[df['doc_id']=='1023628'].Texto.values[0][:400]\n",
    "test2=df[df['doc_id']=='1875751'].Texto.values[0][:400]\n",
    "print(test1)\n",
    "print(test2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análisis usando LSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4041, 301)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1039186</td>\n",
       "      <td>0.121646</td>\n",
       "      <td>-0.024406</td>\n",
       "      <td>-0.039368</td>\n",
       "      <td>-0.035705</td>\n",
       "      <td>-0.041790</td>\n",
       "      <td>-0.010379</td>\n",
       "      <td>-0.004593</td>\n",
       "      <td>-0.042547</td>\n",
       "      <td>-0.021562</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020695</td>\n",
       "      <td>0.008753</td>\n",
       "      <td>-0.013189</td>\n",
       "      <td>0.022894</td>\n",
       "      <td>-0.008763</td>\n",
       "      <td>-0.013317</td>\n",
       "      <td>-0.004110</td>\n",
       "      <td>-0.006685</td>\n",
       "      <td>-0.016296</td>\n",
       "      <td>0.020286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1039418</td>\n",
       "      <td>0.196638</td>\n",
       "      <td>-0.058735</td>\n",
       "      <td>-0.067956</td>\n",
       "      <td>-0.072488</td>\n",
       "      <td>-0.109628</td>\n",
       "      <td>0.022077</td>\n",
       "      <td>-0.010523</td>\n",
       "      <td>0.085696</td>\n",
       "      <td>0.116796</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.027488</td>\n",
       "      <td>0.009622</td>\n",
       "      <td>0.027392</td>\n",
       "      <td>0.002523</td>\n",
       "      <td>-0.010163</td>\n",
       "      <td>-0.005532</td>\n",
       "      <td>-0.022401</td>\n",
       "      <td>-0.004529</td>\n",
       "      <td>-0.028139</td>\n",
       "      <td>-0.008621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1886236</td>\n",
       "      <td>0.119631</td>\n",
       "      <td>-0.034328</td>\n",
       "      <td>-0.042508</td>\n",
       "      <td>-0.039718</td>\n",
       "      <td>-0.048568</td>\n",
       "      <td>-0.021897</td>\n",
       "      <td>-0.001945</td>\n",
       "      <td>-0.007390</td>\n",
       "      <td>0.024269</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004913</td>\n",
       "      <td>-0.023344</td>\n",
       "      <td>-0.002691</td>\n",
       "      <td>-0.002397</td>\n",
       "      <td>0.011871</td>\n",
       "      <td>0.015983</td>\n",
       "      <td>0.003193</td>\n",
       "      <td>-0.012030</td>\n",
       "      <td>0.007192</td>\n",
       "      <td>0.004401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1039204</td>\n",
       "      <td>0.150461</td>\n",
       "      <td>-0.054271</td>\n",
       "      <td>-0.025108</td>\n",
       "      <td>-0.037852</td>\n",
       "      <td>-0.058971</td>\n",
       "      <td>0.035331</td>\n",
       "      <td>-0.031666</td>\n",
       "      <td>0.034916</td>\n",
       "      <td>-0.018692</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003030</td>\n",
       "      <td>0.038374</td>\n",
       "      <td>-0.008059</td>\n",
       "      <td>0.007696</td>\n",
       "      <td>-0.008421</td>\n",
       "      <td>-0.000248</td>\n",
       "      <td>0.010082</td>\n",
       "      <td>0.011281</td>\n",
       "      <td>-0.005485</td>\n",
       "      <td>-0.015180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1038384</td>\n",
       "      <td>0.118576</td>\n",
       "      <td>-0.060460</td>\n",
       "      <td>-0.039442</td>\n",
       "      <td>0.000945</td>\n",
       "      <td>0.146879</td>\n",
       "      <td>0.024474</td>\n",
       "      <td>0.013319</td>\n",
       "      <td>-0.022460</td>\n",
       "      <td>0.021937</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.025737</td>\n",
       "      <td>-0.023569</td>\n",
       "      <td>-0.020597</td>\n",
       "      <td>-0.069216</td>\n",
       "      <td>0.064265</td>\n",
       "      <td>0.029761</td>\n",
       "      <td>-0.016103</td>\n",
       "      <td>0.037081</td>\n",
       "      <td>0.018603</td>\n",
       "      <td>0.046333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 301 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    doc_id         0         1         2         3         4         5  \\\n",
       "0  1039186  0.121646 -0.024406 -0.039368 -0.035705 -0.041790 -0.010379   \n",
       "1  1039418  0.196638 -0.058735 -0.067956 -0.072488 -0.109628  0.022077   \n",
       "2  1886236  0.119631 -0.034328 -0.042508 -0.039718 -0.048568 -0.021897   \n",
       "3  1039204  0.150461 -0.054271 -0.025108 -0.037852 -0.058971  0.035331   \n",
       "4  1038384  0.118576 -0.060460 -0.039442  0.000945  0.146879  0.024474   \n",
       "\n",
       "          6         7         8  ...       290       291       292       293  \\\n",
       "0 -0.004593 -0.042547 -0.021562  ...  0.020695  0.008753 -0.013189  0.022894   \n",
       "1 -0.010523  0.085696  0.116796  ... -0.027488  0.009622  0.027392  0.002523   \n",
       "2 -0.001945 -0.007390  0.024269  ...  0.004913 -0.023344 -0.002691 -0.002397   \n",
       "3 -0.031666  0.034916 -0.018692  ...  0.003030  0.038374 -0.008059  0.007696   \n",
       "4  0.013319 -0.022460  0.021937  ... -0.025737 -0.023569 -0.020597 -0.069216   \n",
       "\n",
       "        294       295       296       297       298       299  \n",
       "0 -0.008763 -0.013317 -0.004110 -0.006685 -0.016296  0.020286  \n",
       "1 -0.010163 -0.005532 -0.022401 -0.004529 -0.028139 -0.008621  \n",
       "2  0.011871  0.015983  0.003193 -0.012030  0.007192  0.004401  \n",
       "3 -0.008421 -0.000248  0.010082  0.011281 -0.005485 -0.015180  \n",
       "4  0.064265  0.029761 -0.016103  0.037081  0.018603  0.046333  \n",
       "\n",
       "[5 rows x 301 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q=300  #debe ser <= 300 o debes correr de nuevo el algoritmo más arriba\n",
    "lsa_rep=wi.get_representativos(dlsa,q)\n",
    "print(lsa_rep.shape)\n",
    "lsa_rep.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4041, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>Vectores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1039186</td>\n",
       "      <td>[0.12164561687616307, -0.024406265620847013, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1039418</td>\n",
       "      <td>[0.19663802423140955, -0.05873545922712045, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1886236</td>\n",
       "      <td>[0.11963116468228008, -0.03432836785615718, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1039204</td>\n",
       "      <td>[0.15046060314843715, -0.05427054969379254, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1038384</td>\n",
       "      <td>[0.11857619280475228, -0.060459988904037684, -...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    doc_id                                           Vectores\n",
       "0  1039186  [0.12164561687616307, -0.024406265620847013, -...\n",
       "1  1039418  [0.19663802423140955, -0.05873545922712045, -0...\n",
       "2  1886236  [0.11963116468228008, -0.03432836785615718, -0...\n",
       "3  1039204  [0.15046060314843715, -0.05427054969379254, -0...\n",
       "4  1038384  [0.11857619280475228, -0.060459988904037684, -..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edf_lsa=wi.modela_documentos_rep(lsa_rep)\n",
    "print(edf_lsa.shape)\n",
    "edf_lsa.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=1\n",
    "vecinos_lsa=wi.k_vecinos_mas_cercanos(docus,edf_lsa,k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vecinos_lsa['1891029'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1=df[df['doc_id']=='1891029'].Texto.values[0][:400]\n",
    "test2=df[df['doc_id']=='1035967'].Texto.values[0][:400]\n",
    "print(test1)\n",
    "print(test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vecinos_lsa['1023628'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1=df[df['doc_id']=='1023628'].Texto.values[0][:400]\n",
    "test2=df[df['doc_id']=='1885462'].Texto.values[0][:400]\n",
    "print(test1)\n",
    "print(test2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Matriz palabra-documento**\n",
    "\n",
    "Queremos ver ahora si podemos representar documentos utilizando representaciones latentes de palabras\n",
    "\n",
    "Una forma de representar palabras como vectores de co-ocurrencia en documentos es usando LSI sobre la matriz Palabra-Documento:\n",
    "\n",
    "<img src=\"img/V.png\" width=\"300\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **LSI de Matriz Palabra-Documento**\n",
    "\n",
    "**Nota que el algoritmo SVD va a representar cada palabra en un vector de dimensión 300.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V_TFIDF = D_tfidf.T \n",
    "\n",
    "vlsa=svd.fit_transform(V_TFIDF)\n",
    "\n",
    "vlsa = pd.DataFrame(data = vlsa)\n",
    "\n",
    "print(vlsa.shape)\n",
    "vlsa.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Índices de las palabras del vocabulario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulario=vocab_.vocabulary_\n",
    "vocabulario = OrderedDict(sorted(vocabulario.items(), key=lambda v: v[1]))\n",
    "print('palabra {:2} índice'.format(''))\n",
    "for x in list(vocabulario)[:5]:\n",
    "    print (\"{:11}:{:2} \".format(x,vocabulario[x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edf_vlsa=wi.modela_documentos_w(vlsa,df,vocabulario)\n",
    "print(edf_vlsa.shape)\n",
    "edf_vlsa.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test con documentos de análisis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=1\n",
    "vecinos_vlsa=wi.k_vecinos_mas_cercanos(docus,edf_vlsa,k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vecinos_vlsa['1891029'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1=df[df['doc_id']=='1891029'].Texto.values[0][:200]\n",
    "test2=df[df['doc_id']=='1894767'].Texto.values[0][:200]\n",
    "print(test1)\n",
    "print(test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vecinos_vlsa['1023628'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1=df[df['doc_id']=='1023628'].Texto.values[0][:200]\n",
    "test2=df[df['doc_id']=='1894767'].Texto.values[0][:200]\n",
    "print(test1)\n",
    "print(test2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Referencias**<br>\n",
    "\n",
    "Baker, K.. 2005. Singular value decomposition tutorial\n",
    "The Ohio State University Vol.24\n",
    "\n",
    "Loper, E. and Bird, S. 2002. NLTK: the Natural Language Toolkit. In _Proceedings of the ACL-02 Workshop on Effective tools and methodologies for teaching natural language processing and computational linguistics - Volume 1 (ETMTNLP '02), Vol. 1_. Association for Computational Linguistics, Stroudsburg, PA, USA, 63-70. DOI: https://doi.org/10.3115/1118108.1118117 \n",
    "\n",
    "McKinney, W., & others. (2010). Data structures for statistical computing in python. In _Proceedings of the 9th Python in Science Conference_ (Vol. 445, pp. 51–56).\n",
    "\n",
    "Tharwat, A. (2016). Principal component analysis-a tutorial. IJAPR, 3(3), 197-240."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
