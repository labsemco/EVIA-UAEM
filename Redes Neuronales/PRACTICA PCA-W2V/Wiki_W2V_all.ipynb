{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/jhermosillo/Escuela_CD_IMATE_2019/blob/master/Wiki_W2V_all.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><center>\n",
    "    \n",
    "### **Modelado de texto usando redes neuronales: algoritmo Word2Vec.**\n",
    "#### Aplicación en WikiPedia para medir semejanza entre documentos.\n",
    "    \n",
    "</center></h3>\n",
    "<h5><center>\n",
    "    Dr. Jorge Hermosillo Valadez<br>\n",
    "    Centro de Investigación en Ciencias<br>\n",
    "    Universidad Autónoma del Estado de Morelos<br>\n",
    "</center></h5>\n",
    "</center>\n",
    "<img src=\"img/logoCInC.jpg\" width=\"100\"/>\n",
    "<img src=\"img/uaem.jpg\" width=\"100\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este curso veremos cómo:\n",
    "* Los principios básicos de W2V\n",
    "* Cómo construir una matriz de vectores de palabras usando W2V\n",
    "* Cómo modelar documentos\n",
    "* Calcular la semejanza entre dos documentos usando W2V y comparar contra PCA y LSI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Módulos necesarios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Sólo para COLAB**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"\"\"\n",
    "!apt-get install subversion\n",
    "!svn checkout \"https://github.com/jhermosillo/Escuela_CD_IMATE_2019/trunk/datos/\"\n",
    "!svn checkout \"https://github.com/jhermosillo/Escuela_CD_IMATE_2019/trunk/modelos/\"\n",
    "#\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"\"\"\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials\n",
    "\n",
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "file_drive = GoogleDrive(gauth)\n",
    "#\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La siguiente instrucción requiere el vínculo al archivo desde DRIVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wi = file_drive.CreateFile({'id':'1sV6vK0CLUXpH1VInmtBUBnVACMm5fPFB'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wi.GetContentFile('wiki.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Jorge\n",
      "[nltk_data]     Hermosillo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import wiki as wi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Jorge\n",
      "[nltk_data]     Hermosillo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./datos/textosWiki_1']\n"
     ]
    }
   ],
   "source": [
    "archivos = glob.glob('./datos/textosWiki_1')\n",
    "print(archivos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leemos los archivos descargados y sus nombres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "leyendo...\n",
      "./datos/textosWiki_1\n",
      "tamaño del contenido de archivos cargados:             12 MB\n"
     ]
    }
   ],
   "source": [
    "file,nombres = wi.carga_datos(archivos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Extracción de documentos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Mi unidad\\LABORATORIO DE SEMANTICA COMPUTACIONAL\\CURSOS\\Escuela_CD_IMATE_2019\\wiki.py:69: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  r=r.cadena.str.translate(\\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "archivo ./datos/textosWiki_1 contiene 4753        documentos \n",
      "\n",
      "Se leyeron 1 archivos\n",
      "1871762 judson donald buechler nacido junio san diego california california unidos jugador profesional ameri\n"
     ]
    }
   ],
   "source": [
    "docs = wi.lee_documentos(file,nombres)\n",
    "print('Se leyeron {} archivos'.format(len(docs)))\n",
    "print(docs[0][0][0],docs[0][0][1][:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data Frame de documentos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4753 documentos clase 0\n",
      "(4753, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>Texto</th>\n",
       "      <th>clase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1871762</td>\n",
       "      <td>judson donald buechler nacido junio san diego ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1871768</td>\n",
       "      <td>lost highway the concert dvd recoge concierto ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1871769</td>\n",
       "      <td>eburones tribu descendencia germánica habitaro...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1871771</td>\n",
       "      <td>aguada baixo portuguesa águeda km² área habita...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1871772</td>\n",
       "      <td>selge griego importante ciudad pisidia ladera ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    doc_id                                              Texto  clase\n",
       "0  1871762  judson donald buechler nacido junio san diego ...      0\n",
       "1  1871768  lost highway the concert dvd recoge concierto ...      0\n",
       "2  1871769  eburones tribu descendencia germánica habitaro...      0\n",
       "3  1871771  aguada baixo portuguesa águeda km² área habita...      0\n",
       "4  1871772  selge griego importante ciudad pisidia ladera ...      0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df,documentos=wi.get_dataFrame_WiDocs(docs)\n",
    "# print(df.shape)\n",
    "# df.head()\n",
    "\n",
    "df_0 = pd.DataFrame(docs[0],columns = ['doc_id','Texto','clase'])\n",
    "#df_1 = pd.DataFrame(docs[1],columns = ['doc_id','Texto','clase'])\n",
    "print(len(df_0.index),'documentos clase 0')\n",
    "#print(len(df_1.index),'documentos clase 1')\n",
    "df = df_0\n",
    "#df = pd.concat([df_0, df_1], ignore_index=True, sort=False)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Modelo Word2Vec**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Introducción** <br>\n",
    "\n",
    "El modelo Word2Vec (Mikolov et al., 2013) es un algoritmo de representación latente (o embebida) de palabras, que se calcula utilizando una red neuronal.\n",
    "\n",
    "Su origen epistémico está en los modelos estadísticos del lenguaje.\n",
    "\n",
    "$P(w_1,w_2,\\cdots,w_n)=P(w_1)P(w_2|w_1)P(w_3|w_1^2)\\cdots P(w_n|w_1^{n-1})$.\n",
    "\n",
    "Estos modelos, buscan calcular la probabilidad de _n-gramas_: $P(w_1)$ unigramas, $P(w_2|w_1)$ bigramas, $P(w_3|w_1^2)$ trigramas, etc.\n",
    "\n",
    "Los unigramas, son modelos tipo Bolsa-de-Palabras, ya que todas las palabras se consideran _independientes_; los bigramas son modelos donde se busca la probabilidad de una palabra, dado un _contexto_ de una palabra; en los trigramas el contexto es de dos palabras, y así sucesivamente.\n",
    "\n",
    "El uso pionero de redes neuronales para calcular estas probabilidades se debe a Bengio y colegas (Bengio et al., 2003). La hipótesis es que _**los términos que co-ocurren en contextos similares tendrán representaciones similares**_, ya que la red neuronal busca maximizar el valor de probabilidad de co-ocurrencia y ajusta los pesos (representación embebida) de la red para este fin. \n",
    "\n",
    "Sin embargo, la red de Bengio era profunda y muy ineficiente. La aportación de Mikolov y colegas fue optimizar la arquitectura, haciéndola superficial y utilizando trucos de aceleración del cómputo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modelos en word2vec** <br>\n",
    "\n",
    "Word2vec implementa dos tipos de modelos: CBOW (Continuous Bag-of-Words: predicción de una palabra dado un contexto de n palabras) y SKIP-gram (predicción de un contexto de n palabras, dada una palabra).\n",
    "\n",
    "</center>\n",
    "<img src=\"img/CBOW.png\" width=\"300\"/>\n",
    "</center><em><center>Modelo CBOW de un bigrama</em></center>\n",
    "\n",
    "</center>\n",
    "<img src=\"img/SKIP-gram.png\" width=\"300\"/>\n",
    "</center><em><center>Modelo SKIP-gram para un contexto de 3 palabras</em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Paso hacia adelante (forward propagation) en word2vec** <br>\n",
    "\n",
    "Las palabras del vocabulario se modelan como un vector _one-hot_, donde sola hay un $1$ en la unidad correspondiente a la palabra de entrada.\n",
    "\n",
    "Si el vocabulario es de tamaño $V$, y si la capa oculta ($\\mathbf{h}$) tiene $N$ neuronas, entonces la matriz de pesos que une la entrada a $\\mathbf{h}$, $\\mathbf{W}$, es de tamaño $V\\times N$. \n",
    "\n",
    "Cada fila de $\\mathbf{W}$ es la representación vectorial $\\mathbf{v}_w$ de dimensión $N$ de la palabra $w$. Formalmente, la fila $i$ de $\\mathbf{W}$ es $\\mathbf{v}^{^\\textrm{T}}_w$.\n",
    "\n",
    "Dado un contexto (de una palabra para el modelo CBOW del bigrama), suponiendo $x_k=1$ y $x_k'=0$ para $k'\\neq k$ tenemos:\n",
    "<center>\n",
    "$\\mathbf{h}=\\mathbf{W}^{^\\textrm{T}}\\mathbf{x}=\\mathbf{W}^{^\\textrm{T}}_{(k,\\cdot)}:=\\mathbf{v}^{^\\textrm{T}}_{w_I}$\n",
    "</center>\n",
    "\n",
    "Hacia la salida, hay otra matriz $\\mathbf{W}'$ de tamaño $N\\times V$, por lo que una unidad de salida $j$ (palabra del vocabulario) tendrá un puntaje (score):\n",
    "<center>\n",
    "$u_j=\\mathbf{v}_{w_j}'^{^\\textrm{T}}\\mathbf{h}$\n",
    "</center>\n",
    "donde $\\mathbf{v}_{w_j}'$ es la columna $j$ de la matriz $\\mathbf{W}'$.\n",
    "\n",
    "Para obtener la distribución a posteriori de las palabras del vocabulario, que es una distribución multinomial, podemos usar un modelo de clasificación multiclase log-lineal llamado _softmax_ \n",
    "<center>\n",
    "$p(w_j|w_I)=y_j=\\frac{\\exp(u_j)}{\\sum_{j'=1}^{V}\\exp(u_{j'})}$\n",
    "</center>\n",
    "\n",
    "El objetivo es entonces optimizar la expresión anterior mediante el algoritmo de descenso de gradiente, utilizando una función de costo (loss function) de entropía cruzada. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Entropía cruzada y cálculo de pesos en word2vec** <br>\n",
    "\n",
    "**Entropía:**<br>\n",
    "\n",
    "Recordemos que podemos medir la cantidad de información de un evento estocástico dada su probabilidad:\n",
    "<center>\n",
    "$I(E)=-\\log[Pr(E)]=−\\log(p)$\n",
    "</center>\n",
    "\n",
    "La entropía es el valor esperado o promedio de la información de un conjunto de eventos estocásticos.\n",
    "\n",
    "El valor esperado de una variable aleatoria se escribe:\n",
    "<center>\n",
    "$E[X]=\\sum_i^nx_ip_i$\n",
    "</center>\n",
    "Por lo que la entropía es:\n",
    "<center>\n",
    "$E[I(X)]=E[-\\log[Pr(I(X))]]=-\\sum_i^np(x_i)\\log p(x_i)$\n",
    "</center>\n",
    "\n",
    "\n",
    "**Entropía cruzada:**<br>\n",
    "\n",
    "Una forma de interpretar la entropía cruzada es verla como (menos) una función de verosimilitud log (log-likelyhood) para datos $y_i'$, bajo un modelo $y_i$.\n",
    "\n",
    "Es decir, supongamos que tenemos algún modelo fijo (también conocido como \"hipótesis\"), que predice para $n$ clases $\\{1,2, \\cdots,n\\}$ sus probabilidades de ocurrencia hipotéticas $y_1, y_2, \\cdots, y_n$. Supongamos que ahora observamos (en realidad) $k_1$ instancias de la clase 1, $k_2$ instancias de la clase 2, $k_n$ instancias de la clase $n$, etc. \n",
    "\n",
    "Según el modelo, la probabilidad de que esto ocurra es (distribución multinomial):\n",
    "<center>\n",
    "$P[datos|modelo]:= y_1^{k_1}\\,y_2^{k_2}\\,\\cdots,y_n^{k_n}$.\n",
    "</center>\n",
    "\n",
    "Tomando el logaritmo y cambiando el signo:\n",
    "<center>\n",
    "$-\\log\\,P[datos|modelo]= -k_1\\log\\,y_1-k_2\\log\\,y_2\\,\\cdots,-k_n\\log\\,y_n=-\\sum_i k_i\\log\\,y_i$.\n",
    "</center>\n",
    "\n",
    "Si ahora dividimos por el número de observaciones $N=k_1+k_2+\\cdots+k_n$, y escribimos las probabilidades empíricas $y_i'=k_i/N$, tenemos la _entropía cruzada_:\n",
    "<center>\n",
    "$-\\frac{1}{N}\\log\\,P[datos|modelo]= -\\frac{1}{N}\\sum_i k_i\\log\\,y_i = \\sum_i y_i'\\log\\,y_i $.\n",
    "</center>\n",
    "\n",
    "En el caso de las redes neuronales, $y_i'$ corresponde con el valor _verdadero_ de la instancia ($\\{0,1\\}$) y $y_i$ es el valor que predice el modelo. \n",
    "\n",
    "\n",
    "</center>\n",
    "<img src=\"img/softmax.png\" width=\"450\"/>\n",
    "</center><em><center>Resumen del modelo CBOW - Forward pass</em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Funciones de costo en word2vec**(Rong, 2014)<br>\n",
    "\n",
    "En word2vec queremos maximizar:\n",
    "<center>\n",
    "$p(w_O|w_I)=y_{j*}=\\frac{\\exp(u_{j*})}{\\sum_{j'=1}^{V}\\exp(u_{j'})}$\n",
    "</center>\n",
    "donde $j*$ es el índice de la palabra que debe estar a la salida. Este es el índice que se compara contra la salida de la red cuando se entrena.\n",
    "\n",
    "**CBOW:**<br>\n",
    "<center>\n",
    "$E=-\\log p(w_O|w_I)=-u_{j*}+\\log\\sum_{j'=1}^{V}\\exp(u_{j'})$\n",
    "</center>\n",
    "<center>\n",
    "$E=-\\mathbf{v}_{w_O}'^{^\\textrm{T}}\\cdot\\mathbf{h}+\\log\\sum_{j'=1}^{V}\\exp(\\mathbf{v}_{w_j}'^{^\\textrm{T}}\\cdot\\mathbf{h})$\n",
    "</center>\n",
    "\n",
    "\n",
    "En el caso en que haya $C$ palabras de entrada (e.g. trigramas o más) la expresión de arriba es la misma, solo cambia $\\mathbf{h}$ que en este caso es:\n",
    "<center>\n",
    "$\\mathbf{h}=\\frac{1}{C}\\mathbf{W}^{^\\textrm{T}}(\\mathbf{x}_1+\\mathbf{x}_2+\\cdots+\\mathbf{x}_C)$\n",
    "</center>\n",
    "\n",
    "**SKIP-gram**<br>\n",
    "Para este caso, en lugar de tener una sola distribución multinomial, tenemos $C$ distribuciones, donde $C$ es el número de palabras del contexto.\n",
    "<center>\n",
    "$E=-\\log p(w_{O,1},w_{O,2},\\cdots,w_{O,C}|w_I)=-\\log\\,\\prod_{c=1}^C\\frac{\\exp(u_{c,j*})}{\\sum_{j'=1}^{V}\\exp(u_{j'})}=\n",
    "-\\sum_{c=1}^{C}u_{j*_c}+C\\cdot\\log\\sum_{j'=1}^{V}\\exp(u_{j'})$\n",
    "</center>\n",
    "\n",
    "\n",
    "De esta forma, se puede aplicar el algoritmo de Back-Propagation, donde se calcula el gradiente de las funciones de costo con respecto a las entradas, según el caso y según la capa correspondiente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Gensim**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a utilizar las librerías y módulos de [gensim](https://radimrehurek.com/gensim/) para [word2vec](https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec), que tienen una amplia gama de soluciones en Python para procesamiento de la [Wikipedia](https://radimrehurek.com/gensim/scripts/segment_wiki.html), y el Procesamiento de Lenguaje Natural en general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hay 4753 documentos y 103132 palabras únicas\n"
     ]
    }
   ],
   "source": [
    "types=df['Texto'].str.split(' ',expand=True).stack().unique()\n",
    "Textos=df.Texto.values\n",
    "\n",
    "#Creamos las oraciones, este será la entrada del modelo W2V\n",
    "frases = [s.split() for s in Textos]\n",
    "\n",
    "documentos= []\n",
    "\n",
    "#Concatenamos todas las oraciones en una sola lista\n",
    "for f in frases:\n",
    "    documentos.append(f)\n",
    "\n",
    "print('Hay {} documentos y {} palabras únicas'.\\\n",
    "      format(len(documentos),len(types)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "vec_dim= 300\n",
    "W2V = Word2Vec(documentos, min_count=1, workers=4, window=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.95365494  1.1842033   0.95986784 -0.20210184  0.06581045 -1.5050063\n",
      "  0.3009835   3.0103698  -0.39941457 -0.67336667  0.25975367 -1.7849637\n",
      " -0.6012992   1.0028936   0.78561294  0.07730709  0.4696674  -0.48145908\n",
      " -0.00316865 -1.9760407  -0.08393261  0.11784887  1.2656853  -0.5249868\n",
      "  0.3182674   0.45680985 -0.4472887  -0.32462436 -0.92281055  0.28334874\n",
      "  1.1180803   0.06162243 -0.08688541 -0.9917644  -0.26961812  0.40924034\n",
      "  0.4008505  -1.2847699  -0.56720024 -1.4302909   0.28427872 -0.7403414\n",
      " -0.71390074  0.02349045  0.98195964 -0.06941931 -1.1503823   0.03580956\n",
      " -0.20438397  0.8060666   0.34001172 -1.420007   -1.3811796  -0.25946775\n",
      " -0.6504957   0.26506352  1.0957309  -0.03950319 -0.9075906   0.29268694\n",
      " -0.4622718   0.53714085 -0.68410915  0.21584918 -0.528052    1.075222\n",
      "  1.117179    0.7701692  -0.75786674  1.1643965  -0.40643275  0.71804243\n",
      "  1.0749515  -0.47401437  0.57457703  0.57578593  0.10439728 -0.39164957\n",
      " -1.1636435  -0.05190044 -0.24959731 -0.837277   -0.6171033   1.1416694\n",
      " -0.17987466 -0.03038325  0.05810107  0.32290325  1.3564587   0.20147242\n",
      "  0.41625667  0.13705021  0.32561877 -0.33966637  2.1703248   1.0634131\n",
      "  0.6928267  -1.3955834  -0.05833076  0.405967  ]\n"
     ]
    }
   ],
   "source": [
    "print(W2V.wv.get_vector(\"comenzó\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos guardar el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2V.save('datos/word2vec.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "o leer el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2V= Word2Vec.load('datos/word2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.95365494  1.1842033   0.95986784 -0.20210184  0.06581045 -1.5050063\n",
      "  0.3009835   3.0103698  -0.39941457 -0.67336667]\n"
     ]
    }
   ],
   "source": [
    "print(W2V.wv.get_vector(\"comenzó\")[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelación de documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, OrderedDict\n",
    "\n",
    "def modela_documentos_rep(df):\n",
    "    id_=df.doc_id.values\n",
    "    datos=df.drop(columns=['doc_id'])\n",
    "    datos=datos.values\n",
    "    dx=[]\n",
    "    for i,doc_id in enumerate(id_):\n",
    "        dx.append((doc_id,datos[i]))\n",
    "    do=pd.DataFrame(dx,columns=['doc_id','Vectores'])\n",
    "    return do\n",
    "\n",
    "def modela_documentos(df,w2v):\n",
    "    docs=df.doc_id.values\n",
    "    textos=df.Texto.str.split(' ').values.tolist()\n",
    "    d=[]\n",
    "    Dx=[]\n",
    "    for i,texto in enumerate(textos):\n",
    "        for w in texto:\n",
    "            e=w2v.wv.get_vector(w)\n",
    "            d.append(e)\n",
    "        d=np.array(d)\n",
    "        dx=np.sum(d,axis=0)/len(d)\n",
    "        Dx.append([docs[i],dx])\n",
    "        d=[]\n",
    "    do=pd.DataFrame(Dx,columns=['doc_id','W2V'])\n",
    "    return do\n",
    "\n",
    "def k_vecinos_mas_cercanos(docus,df,k=1):\n",
    "    l=docus.doc_id.values\n",
    "    vec=OrderedDict()\n",
    "    for id_ in l:\n",
    "        d=dist_vecinos(id_,df)\n",
    "        for i in range(k):\n",
    "            if i==0:\n",
    "                vec[id_]=[[d[i][1],d[i][2]]]\n",
    "            else:\n",
    "                vec[id_].append([d[i][1],d[i][2]])\n",
    "    return vec\n",
    "\n",
    "def vecinos_mas_cercanos(df,distancias):\n",
    "    l=df.doc_id.values\n",
    "    vec=OrderedDict()\n",
    "    for id_ in l:\n",
    "        for i,d in enumerate(distancias):\n",
    "            if id_ == d[0]:\n",
    "                vecino=d[1]\n",
    "                if id_ not in vec.keys():\n",
    "                    vec[id_]=[(vecino,d[2])]\n",
    "                else:\n",
    "                    vec[id_].append((vecino,d[2]))\n",
    "            elif id_== d[1]:\n",
    "                vecino=d[0]\n",
    "                if id_ not in vec.keys():\n",
    "                    vec[id_]=[(vecino,d[2])]\n",
    "                else:\n",
    "                    vec[id_].append((vecino,d[2]))\n",
    "    return vec\n",
    "\n",
    "def dist_vecinos(id_docu,df):\n",
    "    dist=[]\n",
    "    candidato = df[df['doc_id']==id_docu]\n",
    "    candidato = candidato.iloc[:,1].values[0]\n",
    "    fila=df.index[df['doc_id'] == id_docu].tolist()\n",
    "    pts=df.drop(df.index[fila])\n",
    "    id_=pts.doc_id.values\n",
    "    pts=pts.iloc[:,1].values\n",
    "\n",
    "    for i in range(len(pts)):\n",
    "        d = np.sqrt(np.sum(np.square(candidato-pts[i])))\n",
    "        dist.append((id_docu,id_[i],d))\n",
    "    dist=sorted(dist,key=lambda x: x[2])\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "edf=modela_documentos(df,W2V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Ejercicio 1**\n",
    "\n",
    "Queremos saber que tan bien podemos modelar documentos utilizando Word2Vec.\n",
    "\n",
    "Compara estos resultados con los obtenidos por les métodos PCA y LSA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Documentos de análisis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>Texto</th>\n",
       "      <th>clase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1885635</td>\n",
       "      <td>ornithopus perpusillus planta familia fabáceas...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1886681</td>\n",
       "      <td>frivolité variedad curiosa encaje pasamanería ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1890585</td>\n",
       "      <td>dekker apellido puede referirse siguientes per...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1877195</td>\n",
       "      <td>vida suficiente segundo álbum banda mexicali i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1874126</td>\n",
       "      <td>guyencourtsurnoye población comuna francesa re...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    doc_id                                              Texto  clase\n",
       "0  1885635  ornithopus perpusillus planta familia fabáceas...      0\n",
       "1  1886681  frivolité variedad curiosa encaje pasamanería ...      0\n",
       "2  1890585  dekker apellido puede referirse siguientes per...      0\n",
       "3  1877195  vida suficiente segundo álbum banda mexicali i...      0\n",
       "4  1874126  guyencourtsurnoye población comuna francesa re...      0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# docus = df[(df['doc_id']=='1023628') |\\\n",
    "#            (df['doc_id']=='1024447') |\\\n",
    "#            (df['doc_id']=='1035967') |\\\n",
    "#            (df['doc_id']=='1891029') |\\\n",
    "#            (df['doc_id']=='1894599') ]  \n",
    "docus = df.sample(n=5)\n",
    "docus.index=range(len(docus.index))\n",
    "\n",
    "docus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=1\n",
    "vecinos=k_vecinos_mas_cercanos(docus,edf,k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('1885635', [['1885583', 0.23433523]]), ('1886681', [['1889858', 0.11623342]]), ('1890585', [['1894945', 0.2674673]]), ('1877195', [['1885993', 0.20900859]]), ('1874126', [['1874168', 0.006189191]])])\n"
     ]
    }
   ],
   "source": [
    "print(vecinos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ornithopus perpusillus planta familia fabáceas descripción pelosa anual tallos extendidos hojas pares folíolos pequeños elípticos oblongos flores blancas rosas cabezuelas flores brácteas bajo cabezuela largas flores lóbulos dientes cáliz mitad largo tubo vaina constreñida segmentos pico recto ganchudo florece primavera otoño hábitat habita lugares arenisca arena distribución bélgica gran bretaña dinamarca francia alemania irlanda holanda suecia suiza españa italia portugal polonia rumanía enlaces externos']\n",
      "   ['cuernecillo grande lotus uliginosus especie botánica erecta ascendente pelosa perenne tallos huecos hojas folíolos obovados verdiazules debajo flores amarillas menudo teñidas rojo cabezuelas largo cabillo normalmente flores pétalos dientes cáliz extendidos brote aproximadamente largo tuco vaina florece mayo agosto habita pantanos prados húmedos gran parte europa introducida noruega finlandia hungría centro españa habita comunidades quercofraxinetum enlaces externos']\n",
      "\n",
      "['frivolité variedad curiosa encaje pasamanería llama encaje lanzadera españa tatting gran bretaña occhi italia incluso makuk oriente consiste montar sucesión nudos baguillas único hilo ayuda dos lanzaderas materiales lanzaderas lanzadera llamada naveta países hispanoamericanos útil realiza preciada labor consiste especie aguja alargada guarda suelta hilo gracias extremos apuntados mencionan tres grandes tipos lanzaderas sencillas lanzaderas frecuentes encuentran mercado actualmente fabriacadas pasta plástica madera hace económicas clover deben nombre casa comercial creó deformación anteriores puntas pronunciada permite trabajar baguillas picots mayor precisión victorianas datan inglaterra finales siglo xix ahí nombre éstas constan canilla parecida máquinas coser estrecha cubierta especie ganchillo lanzadera propiamente dicha mencionar variedad atípica frivolité hace aguja coser ahora desuso hilo recomienda vivamente uso hilos resistentes tensión llega soportar hilo lanzadera enorme generalmente suele pasar límite números ganchillo mayoría dibujos deben llevar cabo serie uniones baguillas ello necesario sacar hebra hora llevar cabo operación preferible utilizar ganchillo apropiado alfiler último pica hilo decir quita rizo habitual pierde calidad punto básico doble nudo nudo aparece siempre labores punto doble nudo compone nudo izquierdo nudo derecho nudo doble bien elaborado permite corra hilo lanzadera principal través éste principio fundamental frivolité baguillas tipos baguillas picots características asas dan verdadera belleza tipo pasamanería resultado dejar hilo dos nudos dobles luego jutan distinguen dos tipos según función picots decoración aquellos sirven enbellecer dibujo tamaño varía función artesano canon estilo extendido cuanta mayor gracia quiera dar labor grandes picots unión aquellos sirven enlazar sección generalmente tamaño mínimo embargo dibujos puntillas demás trabajos surgen picots ambas funciones elementos decorativos distinguir dos motivos fundamentales encaje frivolité anillos arcos anillos círculos cerrados nudos dobles trabajan circunferencia traza mano izquierda hilo lanzadera hilo realiza nudos hacen proceden lanzadera arcos líneas curvas ejecutan dos lanzaderas hilo auxiliar casi siempre utiliza ovillo siempre van dar anillo usos patrones frecuentes frivolité suelen mostrar puntillas variados anchos aplicaciones plan central circulares pueden unir formar tapetes paños etc']\n",
      "   ['dracomon personaje ficción criatura digimon anime manga digimon momento única aparición sido juegos nintendo dracomon digimon dragón sangre pura pues primero clase siendo primer dragón historia mundo digital ancestro digimons cuyos nombres acaban dramon nombre significa dragón latin fuerza agilidad dracomon combierten mejores nivel principiante posee gekirin escama invertida dragon algún casual escama tocada dracomon perderá juicio entrando modo rabia furiosa atacara vea bolas fuego amigo enemigo apariencia dracomon digimon pequeño corpulento parece dragón mide pies alto piel verdosa menos parte mandibula cuello estomago base cola tono marrón claro ojos grandes forma disco rojos antebrazos pequeñas resto cuerpo tres dedos dando imprecion típico dragon antiguo tierra cortos brazos delanteros cuerpo grandes piernas cola alas tripa haci pecho desciende cuello base cola semicirculo cola vastante grande tener nivel tan bajo corriente digimons mayor nivel cabeza hocico compensados cuerpo cabeza provista cuernos forma cuernos dragon chino dicho dando imprecion antiguo dragon tierra aparte posee pequeñas alitas color rojo permitiéndole volar cortos periodos tiempo evoluciones digievoluciones nivel cuerpo nombre bebé bebé petitmon bgcolorefefef entrenamiento babydmon principiante infantil dracomon bgcolorefefef campeón maduroadulto coredramon puede ser azul verde megaultimo perfecto groundramon coredramon verde wingdramon coredramon azul bgcolorefefef hipermega supremo breakdramon verde slayerdramon azul adn digievolución fusion examon slayerdramon breakdramon ataques dracomon arsenal ataques francamente demoledores baby breath dispara aliento ardiende lanzallamas tail smash golpea cola shururen tocan gekirin activa técnica dispara bolas fuego indiscriminadamente vea referencias dma digidex wikimon']\n",
      "\n",
      "['dekker apellido puede referirse siguientes personas desmond dekker cantante ska erik dekker ciclista retirado ganador copa mundo ciclismo jeremias dekker poeta thomas dekker ciclista equipo rabobank thomas dekker escritor además puede referirse algoritmo dekker programación concurrente exclusión mutua']\n",
      "   ['término luttenberger puede referirse peter luttenberger ciclista profesional luttenbergerklug dúo musical poprock michelle luttenberger cantante femenina dúo anterior']\n",
      "\n",
      "['vida suficiente segundo álbum banda mexicali insite lanzado temática canciones habla acerca oportunidad vivir suficiente alcanza necesita vida poder expresar quieres sientes explica tema contigo muerte llévame contigo separes minuto hoy sol regrese vez disco cambia profundamente género musical entonces álbum historia primer sencillo disco tema cielos lloran tour vida suficiente tour insite toda latinoamérica comenzó abril comenzado méxico primeramente ciudad natal mexicali luego monterrey siendo sede auditorio cocacola parque fundidora luego guadalajara último ciudad méxico aclamados recibieron buenas críticas fans personas presentaron espectáculo fines junio comenzó gira centroamérica sudámerica último españa participación insite mucha participación año mediados meses enero junio constantes participanes vive latino espera insite participe mtv video music awards sencillos cielos lloran sola siempre dejas lista canciones títuloduración destrózame sola día discúlpame rindo pregunta amé contigo muerte siempre dejas continuación quisiera lejos luces cielos lloran soñar extras títuloduración rifan the making vida suficiente día agradecimientos']\n",
      "   ['carlos enrique estremera colón conocido cano estremera nació septiembre santurce puerto rico excelente estudiante escuela primaria motivado dios padres estudiar música empezó cantando himnos religiosos comenzó mundo profesional percusionista grupo músicos formó barrio obrero santurce sección dentro año vocalista grupo plena grupo folklórico pleneros quinto olivo mayor oportunidad llegó unirse orquesta mulenze contrato exclusivo fania records reunió bobby valentín invitó unirse banda primera canción grabó grupo valentín boda ganó aclamación generalizada mundo música popular grabó seis discos valentín incluyendo dos solista finales decidió abandonar valentín formar propia banda nueva etapa cano conocido dedicó cantar géneros música popular ganó premio paoli vocalista salsa año premios estremera debutó productor propios registros salvaje álbum año tarde dio conocer dueño soneo nueva serie grabaciones cano nació albino conocido industria música mejores soneros cantante afrocaribe música siglo habilidad demostró ahora famoso concierto guánica arena estrofas consecutivos repetir sola frase logro mismo año habilidades improvisación pone prueba vez produjo única soneos concierto fans yabucoa mientras unas pocas semanas tarde espectadores concierto juana díaz número aumentó afilado destreza improvisar par estremera ganado epíteto titular soneo título prácticamente sustituido pibe oro inicialmente conocido utilizó título dos grabaciones álbum hit dueño soneo vol incluyen salsa pistas acaso dueño soneo vol exitos dueño soneo éxitos musicales estremera mejor conocido canciones ingratitudes manuel garcía caso mejoro mujer primavera quedé ganas novia automática awilda compromiso nací así aprovecha viernes social discografía puerto rican maestra historia salsa opera ecuajey vol sonora ponceña años diferente encuentro histórico punto aparte cambio éxitos dueño soneo dueño soneo vol dueño soneo vol salvaje niño oro cano estremera orquesta bobby valentín acción brujería bobby valentín bobby valentín presenta cano estremera siempre forma bobby valentín bobby valentín gato bobby valentín presenta cano estremera boda']\n",
      "\n",
      "['guyencourtsurnoye población comuna francesa región picardía departamento somme distrito amiens cantón boves demografía enlaces externos insee elecciones municipales']\n",
      "   ['grattepanche población comuna francesa región picardía departamento somme distrito amiens cantón boves demografía enlaces externos insee elecciones municipales']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lista = []\n",
    "for item in vecinos:\n",
    "    vecino = vecinos[item][0][0]\n",
    "    lista.append((item,vecino))\n",
    "for docs in lista:\n",
    "    print(df.loc[df['doc_id']==docs[0]].Texto.values)\n",
    "    print('  ',df.loc[df['doc_id']==docs[1]].Texto.values)\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Ejercicio 2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelo SKIP-gram... Ojo! Es tardado..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "vec_dim= 300\n",
    "w2v_sg = Word2Vec(documentos, min_count=1, size=vec_dim, workers=4, window=5, iter=30,sg=1)\n",
    "print(W2V[\"comenzó\"][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_sg=Word2Vec.load('/content/gdrive/My Drive/MisCursos/word2vec_sg.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edf_sg=wi.modela_documentos(df,w2v_sg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=1\n",
    "vecinos_sg=wi.k_vecinos_mas_cercanos(docus,edf_sg,k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vecinos_sg['1891029'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1=df[df['doc_id']=='1891029'].values\n",
    "test2=df[df['doc_id']=='1896707'].values\n",
    "print(test1)\n",
    "print(test2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "</hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lsa=pd.read_pickle('datos/data_frame_4K.pkl')\n",
    "df_lsa.index = range(len(df_lsa.index))\n",
    "print(df_lsa.shape)\n",
    "df_lsa.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from collections import Counter, OrderedDict\n",
    "\n",
    "def bow_(docs):\n",
    "    v = DictVectorizer(sparse=False)\n",
    "    X = v.fit_transform(docs)\n",
    "    return X,v\n",
    "\n",
    "docs = df_lsa.Conteos.tolist()\n",
    "X,vocab_ = bow_(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "q=300  #Elegimos usar q componentes\n",
    "svd = TruncatedSVD(n_components=q, n_iter=7, random_state=42)\n",
    "\n",
    "vectorizer = TfidfVectorizer(vocabulary=vocab_.vocabulary_)\n",
    "\n",
    "corpus = df_lsa.Texto.tolist()\n",
    "D_tfidf = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dlsa=svd.fit_transform(D_tfidf)\n",
    "dlsa=wi.get_dataFrame(dlsa,df_lsa)\n",
    "print(dlsa.shape)\n",
    "dlsa.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_vr=svd.explained_variance_ratio_\n",
    "wi.distribucion_vr(svd_vr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q=300  #debe ser <= 300 o debes correr de nuevo el algoritmo más arriba\n",
    "lsa_rep=wi.get_representativos(dlsa,q)\n",
    "print(lsa_rep.shape)\n",
    "lsa_rep.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edf_lsa=wi.modela_documentos_rep(lsa_rep)\n",
    "print(edf_lsa.shape)\n",
    "edf_lsa.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=1\n",
    "vecinos_lsa=wi.k_vecinos_mas_cercanos(docus,edf_lsa,k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vecinos_lsa['1891029'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1=df[df['doc_id']=='1891029'].Texto.values[0][:400]\n",
    "test2=df[df['doc_id']=='1035967'].Texto.values[0][:400]\n",
    "print(test1)\n",
    "print(test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vecinos_lsa['1023628'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1=df[df['doc_id']=='1023628'].Texto.values[0][:400]\n",
    "test2=df[df['doc_id']=='1885462'].Texto.values[0][:400]\n",
    "print(test1)\n",
    "print(test2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Referencias** <br>\n",
    "\n",
    "Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and Christian Janvin. 2003. A neural probabilistic language model. J. Mach. Learn. Res. 3 (March 2003), 1137-1155. \n",
    "\n",
    "Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Distributed representations of words and phrases and their compositionality. In _Proceedings of the 26th International Conference on Neural Information Processing Systems - Volume 2 (NIPS'13)_, C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger (Eds.), Vol. 2. Curran Associates Inc., USA, 3111-3119. \n",
    "\n",
    "Xin Rong. 2014. Word2vec Parameter Learning Explained.arXiv 1411.2738. disponible en linea {http://arxiv.org/abs/1411.2738}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
